# Architecture Analysis & Optimization Plan

## Current State Assessment

### ‚úÖ What's Already Built (Strong Foundation)

#### 1. **Company Research System** (EXCELLENT)
- **Location**: `src/lib/perplexity-intelligence.ts` ‚Üí `researchCompanyV2()`
- **Features**:
  - Comprehensive company intelligence gathering
  - Hiring contacts scraping
  - Culture, values, tech stack analysis
  - Glassdoor ratings, salary data
  - Recent news & market position
- **Caching**: In-memory cache with TTL
- **Cost**: ~$0.05 per company research
- **Status**: ‚úÖ Working, per-click research already integrated

#### 2. **Resume Keyword Extraction** (WEAK - NEEDS IMPROVEMENT)
- **Location**: `src/lib/utils.ts` ‚Üí `extractKeywords()`
- **Current Implementation**:
  ```typescript
  // PROBLEM: Too simplistic!
  const words = text.toLowerCase().split(/\s+/)
  const stopWords = new Set(['a', 'an', 'and', ...])
  return words
    .filter(word => word.length > 2 && !stopWords.has(word))
    .slice(0, 20) // Only 20 keywords!
  ```
- **Issues**:
  - ‚ùå No skill weighting
  - ‚ùå No multi-word phrases ("machine learning" ‚Üí "machine", "learning")
  - ‚ùå No context awareness (React framework vs react to situation)
  - ‚ùå Only 20 keywords (too few)
  - ‚ùå No primary vs secondary skill distinction
- **Status**: üî¥ **CRITICAL BOTTLENECK**

#### 3. **Perplexity Resume Analysis** (EXISTS BUT UNDERUTILIZED)
- **Location**: `src/lib/perplexity-intelligence.ts` ‚Üí `extractResumeSignals()`
- **Features**:
  - Extracts 50 keywords with priority ordering
  - Extracts location, personal info
  - Uses Perplexity Sonar (cheap)
  - Cached results
- **Cost**: ~$0.01 per resume
- **Status**: ‚ö†Ô∏è **EXISTS BUT NOT USED FOR JOB MATCHING**

#### 4. **Job Search** (SLOW & EXPENSIVE)
- **Current Flow**:
  ```
  User searches ‚Üí Perplexity API call ($0.10)
  ‚Üí Scrapes LinkedIn, Indeed, Glassdoor in real-time
  ‚Üí Returns 25-50 jobs
  ‚Üí Takes 10-30 seconds
  ```
- **Issues**:
  - ‚ùå Expensive ($0.10 per search)
  - ‚ùå Slow (10-30 second wait)
  - ‚ùå Limited results (25-50 jobs)
  - ‚ùå No job database (can't share across users)
  - ‚ùå Rate limits
- **Status**: üî¥ **CRITICAL BOTTLENECK**

---

## üéØ THE REAL PROBLEMS

### Problem #1: Weak Keyword Extraction
**Impact**: Poor job matching, irrelevant results
**Root Cause**: Using basic string splitting instead of Perplexity's AI analysis

**Current**:
```typescript
extractKeywords("Senior React Developer with 5 years Node.js experience")
// Returns: ["senior", "react", "developer", "years", "node", "experience"]
// ‚ùå Lost context, no weighting, no phrases
```

**Should Be**:
```typescript
{
  primarySkills: [
    { skill: "React", weight: 0.95, years: 5 },
    { skill: "Node.js", weight: 0.90, years: 5 },
    { skill: "JavaScript", weight: 0.85, years: 5 }
  ],
  secondarySkills: [
    { skill: "TypeScript", weight: 0.60, years: 2 },
    { skill: "AWS", weight: 0.50, years: 1 }
  ],
  seniorityLevel: "senior"
}
```

### Problem #2: Real-time Job Search
**Impact**: Slow UX, high costs, limited scale
**Root Cause**: Searching external APIs on every user request

**Current Flow**:
```
User 1 searches "Software Engineer Toronto" ‚Üí Perplexity API ($0.10, 15s)
User 2 searches "Software Engineer Toronto" ‚Üí Perplexity API ($0.10, 15s) AGAIN!
User 3 searches "Software Engineer Toronto" ‚Üí Perplexity API ($0.10, 15s) AGAIN!
```

**Should Be**:
```
Tuesday scraper runs ‚Üí 10,000 jobs in MongoDB ($20 total)
User 1 searches ‚Üí MongoDB query (instant, $0.00)
User 2 searches ‚Üí MongoDB query (instant, $0.00)
User 3 searches ‚Üí MongoDB query (instant, $0.00)
```

### Problem #3: Not Using Existing Perplexity Resume Analysis
**Impact**: Missing opportunity for smart matching
**Root Cause**: `extractResumeSignals()` exists but isn't used for job search

---

## üöÄ OPTIMIZATION PLAN

### Phase 1: Fix Keyword Extraction (IMMEDIATE - 1 day)

#### Step 1.1: Use Perplexity for Resume Analysis
**Change**: Replace `extractKeywords()` with `extractResumeSignals()`

```typescript
// OLD (src/lib/utils.ts)
const keywords = extractKeywords(resumeText) // ["react", "node", "aws"]

// NEW (use existing Perplexity function)
const signals = await PerplexityIntelligenceService.extractResumeSignals(resumeText, 50)
// Returns:
{
  keywords: ["React", "Node.js", "TypeScript", "AWS", ...], // 50 skills in priority order
  location: "Toronto, ON",
  personalInfo: { name, email, phone }
}
```

#### Step 1.2: Add Skill Weighting to UserProfile
**Change**: Store weighted skills in MongoDB

```typescript
// Add to UserProfile schema
interface UserProfile {
  // ... existing fields
  skillsWeighted: {
    primarySkills: Array<{ skill: string, weight: number, years?: number }>
    secondarySkills: Array<{ skill: string, weight: number, years?: number }>
    extractedAt: Date
  }
}
```

#### Step 1.3: Use Weighted Skills in Job Search
**Change**: Match jobs using skill weights

```typescript
// Job matching with weights
function calculateJobMatch(job, userSkills) {
  let score = 0
  
  // Primary skills: 2x weight
  userSkills.primarySkills.forEach(skill => {
    if (job.description.includes(skill.skill)) {
      score += skill.weight * 2
    }
  })
  
  // Secondary skills: 1x weight
  userSkills.secondarySkills.forEach(skill => {
    if (job.description.includes(skill.skill)) {
      score += skill.weight
    }
  })
  
  return score / (userSkills.primarySkills.length + userSkills.secondarySkills.length)
}
```

**Cost**: $0.01 per resume (one-time)
**Benefit**: 10x better job matching

---

### Phase 2: Scheduled Job Scraping (HIGH IMPACT - 3 days)

#### Step 2.1: Create Job Database Schema
```typescript
interface ScrapedJob {
  _id: ObjectId
  
  // Core fields
  title: string
  company: string
  location: string
  description: string // Full text
  url: string
  
  // Metadata
  source: 'jsearch' | 'adzuna' | 'jobbank'
  salary?: { min: number, max: number, currency: string }
  workType: 'remote' | 'hybrid' | 'onsite'
  experienceLevel: 'entry' | 'mid' | 'senior' | 'executive'
  
  // Extracted intelligence
  requiredSkills: string[]
  preferredSkills: string[]
  
  // Timestamps
  scrapedAt: Date
  expiresAt: Date // Auto-delete after 7 days
  postedDate?: Date
  
  // Indexes
  indexes: {
    location: 1,
    'requiredSkills': 1,
    expiresAt: 1 // TTL index
  }
}
```

#### Step 2.2: Create Scheduled Scraper Service
**File**: `src/services/job-scraper.service.ts`

```typescript
export class JobScraperService {
  
  // Run every Tuesday & Saturday at 2 AM
  static async runScheduledScrape() {
    console.log('[JOB_SCRAPER] Starting scheduled scrape...')
    
    const locations = [
      'Toronto, ON', 'Vancouver, BC', 'Montreal, QC',
      'Calgary, AB', 'Edmonton, AB', 'Ottawa, ON',
      'Seattle, WA', 'San Francisco, CA', 'New York, NY'
    ]
    
    const jobTitles = [
      'Software Engineer', 'Data Scientist', 'Product Manager',
      'DevOps Engineer', 'UX Designer', 'Marketing Manager'
    ]
    
    let totalScraped = 0
    
    for (const location of locations) {
      for (const title of jobTitles) {
        // Scrape from JSearch (LinkedIn, Indeed, Glassdoor, ZipRecruiter)
        const jSearchJobs = await this.scrapeJSearch(title, location)
        
        // Scrape from Adzuna
        const adzunaJobs = await this.scrapeAdzuna(title, location)
        
        // Scrape from Job Bank Canada (if Canadian location)
        const jobBankJobs = location.includes('Canada') 
          ? await this.scrapeJobBank(title, location)
          : []
        
        // Combine and deduplicate
        const allJobs = [...jSearchJobs, ...adzunaJobs, ...jobBankJobs]
        const uniqueJobs = this.deduplicateByUrl(allJobs)
        
        // Save to MongoDB
        await this.saveJobs(uniqueJobs)
        
        totalScraped += uniqueJobs.length
        
        // Rate limiting
        await sleep(2000)
      }
    }
    
    console.log(`[JOB_SCRAPER] ‚úÖ Scraped ${totalScraped} jobs`)
    
    // Clean up old jobs (older than 7 days)
    await this.cleanupOldJobs()
  }
  
  private static async cleanupOldJobs() {
    const sevenDaysAgo = new Date(Date.now() - 7 * 24 * 60 * 60 * 1000)
    const result = await ScrapedJob.deleteMany({ 
      scrapedAt: { $lt: sevenDaysAgo } 
    })
    console.log(`[JOB_SCRAPER] üóëÔ∏è Deleted ${result.deletedCount} old jobs`)
  }
}
```

#### Step 2.3: Set Up Cron Job
**File**: `src/app/api/cron/scrape-jobs/route.ts`

```typescript
// Vercel Cron: runs Tuesday & Saturday at 2 AM
export async function GET(request: NextRequest) {
  // Verify cron secret
  const authHeader = request.headers.get('authorization')
  if (authHeader !== `Bearer ${process.env.CRON_SECRET}`) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
  }
  
  await JobScraperService.runScheduledScrape()
  
  return NextResponse.json({ success: true })
}
```

**vercel.json**:
```json
{
  "crons": [{
    "path": "/api/cron/scrape-jobs",
    "schedule": "0 2 * * 2,6"
  }]
}
```

#### Step 2.4: Update Job Search API
**Change**: Query MongoDB instead of Perplexity

```typescript
// OLD: src/app/api/jobs/search/route.ts
const result = await PerplexityIntelligenceService.jobSearchWithAgent(
  keywords, location, { maxResults: 25 }
) // $0.10, 15 seconds

// NEW: Query MongoDB
const jobs = await ScrapedJob.find({
  location: { $regex: location, $options: 'i' },
  $or: [
    { title: { $regex: keywords, $options: 'i' } },
    { description: { $regex: keywords, $options: 'i' } },
    { requiredSkills: { $in: userSkills.primarySkills.map(s => s.skill) } }
  ],
  expiresAt: { $gt: new Date() }
})
.sort({ scrapedAt: -1 })
.limit(100) // Return 100 jobs instead of 25!
// $0.00, instant
```

**Cost Savings**:
- Before: $0.10 √ó 1000 searches/day = $100/day = $3,000/month
- After: $20/month for scraping = **99% cost reduction**

**Speed Improvement**:
- Before: 10-30 seconds per search
- After: <100ms per search = **100x faster**

---

### Phase 3: Permanent Company Profile Cache (MEDIUM IMPACT - 2 days)

#### Step 3.1: Create CompanyProfile Schema
```typescript
interface CompanyProfile {
  _id: ObjectId
  companyName: string // indexed, unique
  domain: string
  
  // Permanent data (never expires)
  about: string
  industry: string
  size: string
  founded: number
  headquarters: string
  culture: string[]
  values: string[]
  techStack: string[]
  
  // Hiring contacts (append-only)
  hiringContacts: Array<{
    name: string
    title: string
    email: string
    linkedin: string
    verified: boolean
    addedAt: Date
  }>
  
  // Dynamic data (refresh monthly)
  recentNews: Array<{ title: string, date: Date, summary: string }>
  glassdoorRating: number
  salaryRanges: Record<string, { min: number, max: number }>
  
  // Metadata
  firstScrapedAt: Date
  lastUpdatedAt: Date
  researchCount: number
}
```

#### Step 3.2: Update Company Research to Use Cache
```typescript
async function getCompanyIntelligence(companyName: string) {
  // Check cache first
  const cached = await CompanyProfile.findOne({ companyName })
  
  if (cached) {
    const age = Date.now() - cached.lastUpdatedAt.getTime()
    
    if (age < 30 * 24 * 60 * 60 * 1000) {
      // Less than 30 days - use cache
      console.log('[COMPANY] Cache hit:', companyName)
      return cached // $0.00
    }
    
    // Refresh dynamic data only
    const updates = await refreshDynamicData(companyName) // $0.01
    await CompanyProfile.updateOne({ companyName }, { $set: updates })
    return { ...cached, ...updates }
  }
  
  // First time - full research
  const intel = await PerplexityIntelligenceService.researchCompanyV2({ 
    company: companyName 
  }) // $0.05
  
  await CompanyProfile.create({
    companyName,
    ...intel.data,
    firstScrapedAt: new Date(),
    lastUpdatedAt: new Date(),
    researchCount: 1
  })
  
  return intel.data
}
```

**Cost Evolution**:
- Month 1: 5,000 companies √ó $0.05 = $250
- Month 2: 2,000 new √ó $0.05 = $100 (60% cache hit)
- Month 6: 500 new √ó $0.05 = $25 (90% cache hit)
- Month 12: 200 new √ó $0.05 = $10 (98% cache hit)

---

## üìä FINAL COST COMPARISON

### Current Architecture (Real-time)
```
Resume analysis: Basic keyword extraction (free but weak)
Job search: $0.10 √ó 1000/day = $100/day
Company research: $0.05 √ó 500/day = $25/day
TOTAL: $125/day = $3,750/month
```

### Optimized Architecture (Scheduled + Cache)
```
Resume analysis: $0.01 √ó 100 uploads/day = $1/day
Job scraping: $20/month (scheduled)
Job search: MongoDB queries (free)
Company research: $10/month (98% cache hit after 12 months)
TOTAL: $1/day + $30/month = $60/month
```

**Savings: $3,690/month (98% reduction)**

---

## üéØ IMPLEMENTATION PRIORITY

### Week 1: Fix Keyword Extraction
1. Replace `extractKeywords()` with `extractResumeSignals()`
2. Add weighted skills to UserProfile schema
3. Update job matching to use skill weights
**Impact**: 10x better job matching
**Cost**: $0.01 per resume

### Week 2: Build Job Scraper
1. Create ScrapedJob schema
2. Build JobScraperService
3. Set up Vercel cron (Tuesday/Saturday)
4. Test with 100 jobs
**Impact**: Build job database
**Cost**: $20/month

### Week 3: Switch to MongoDB Search
1. Update job search API to query MongoDB
2. Remove Perplexity job search calls
3. Add skill-weighted ranking
**Impact**: Instant search, 99% cost reduction
**Cost**: $0.00 per search

### Week 4: Permanent Company Cache
1. Create CompanyProfile schema
2. Update research to check cache first
3. Add monthly refresh for dynamic data
**Impact**: 98% cache hit rate over time
**Cost**: $10/month after 12 months

---

## ‚úÖ WHAT TO KEEP (DON'T CHANGE)

1. ‚úÖ **Per-click company research** - Already working great
2. ‚úÖ **Resume optimization** - Uses Perplexity effectively
3. ‚úÖ **Cover letter generation** - High value, worth the cost
4. ‚úÖ **Outreach email generation** - Automated and effective
5. ‚úÖ **Company intelligence display** - Your differentiator

---

## üö´ WHAT TO REMOVE

1. ‚ùå Real-time Perplexity job search
2. ‚ùå Basic `extractKeywords()` function
3. ‚ùå In-memory cache (replace with MongoDB)

---

## Questions Before We Proceed?

1. Should I start with Phase 1 (keyword extraction) this week?
2. Do you want to see the job scraper working before switching search?
3. Any concerns about the 7-day job expiration?
4. Should we keep some real-time search as a fallback?
