This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: src/lib/adzuna-api-client.ts, src/lib/rapidapi-client.ts, src/lib/job-aggregator.ts, src/lib/supabase-bulk-download.ts, src/lib/public-job-discovery-service.ts, src/lib/public-job-boards-config.ts, src/lib/perplexity-intelligence.ts, src/lib/supabase.ts, src/lib/scrapers/advanced-scraper.ts, src/lib/contact-scraper.ts, src/lib/enhanced-canadian-scraper.ts, src/lib/job-scraper-service.ts, src/lib/comprehensive-data-sources.ts, src/app/api/jobs/search/route.ts, src/types/supabase.ts, download-incremental.ts, download-edmonton-jobs.ts
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
download-edmonton-jobs.ts
download-incremental.ts
src/app/api/jobs/search/route.ts
src/lib/adzuna-api-client.ts
src/lib/comprehensive-data-sources.ts
src/lib/contact-scraper.ts
src/lib/enhanced-canadian-scraper.ts
src/lib/job-aggregator.ts
src/lib/job-scraper-service.ts
src/lib/perplexity-intelligence.ts
src/lib/public-job-boards-config.ts
src/lib/public-job-discovery-service.ts
src/lib/rapidapi-client.ts
src/lib/scrapers/advanced-scraper.ts
src/lib/supabase-bulk-download.ts
src/lib/supabase.ts
src/types/supabase.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/lib/adzuna-api-client.ts">
/**
 * Adzuna API Client
 * 
 * Official job board aggregator API with FREE tier
 * Coverage: Canada, US, UK, 20+ countries
 */

export interface AdzunaJob {
  id: string
  title: string
  company: {
    display_name: string
  }
  location: {
    display_name: string
    area: string[]
  }
  description: string
  redirect_url: string
  salary_min?: number
  salary_max?: number
  salary_is_predicted?: boolean
  created: string
  contract_time?: string
  contract_type?: string
  category: {
    label: string
    tag: string
  }
}

export interface AdzunaSearchParams {
  what: string // Job title or keywords
  where: string // Location
  country?: 'ca' | 'us' | 'gb' | 'au' // Default: ca (Canada)
  resultsPerPage?: number // Default: 50, Max: 50
  page?: number // Default: 1
  sortBy?: 'relevance' | 'date' | 'salary' // Default: relevance
  maxDaysOld?: number // Only jobs posted in last N days
  fullTime?: boolean
  partTime?: boolean
  contract?: boolean
  permanent?: boolean
}

export interface AdzunaSearchResponse {
  results: AdzunaJob[]
  count: number
  mean: number
  __CLASS__: string
}

export class AdzunaAPIClient {
  private readonly appId: string
  private readonly appKey: string
  private readonly baseUrl = 'https://api.adzuna.com/v1/api/jobs'

  constructor() {
    this.appId = process.env.ADZUNA_APP_ID || 'b0300aa2'
    this.appKey = process.env.ADZUNA_API_KEY || '19f3a3c651c39d4073b1a66516d38432'

    if (!this.appId || !this.appKey) {
      console.warn('[ADZUNA] API credentials not configured')
    }
  }

  /**
   * Search jobs on Adzuna
   */
  async searchJobs(params: AdzunaSearchParams): Promise<AdzunaSearchResponse> {
    const {
      what,
      where,
      country = 'ca',
      resultsPerPage = 50,
      page = 1,
      sortBy = 'relevance',
      maxDaysOld,
      fullTime,
      partTime,
      contract,
      permanent
    } = params

    const url = `${this.baseUrl}/${country}/search/${page}`

    const queryParams: Record<string, string> = {
      app_id: this.appId,
      app_key: this.appKey,
      what,
      where,
      results_per_page: resultsPerPage.toString(),
      sort_by: sortBy,
      'content-type': 'application/json'
    }

    // Optional filters
    if (maxDaysOld) queryParams.max_days_old = maxDaysOld.toString()
    if (fullTime !== undefined) queryParams.full_time = fullTime ? '1' : '0'
    if (partTime !== undefined) queryParams.part_time = partTime ? '1' : '0'
    if (contract !== undefined) queryParams.contract = contract ? '1' : '0'
    if (permanent !== undefined) queryParams.permanent = permanent ? '1' : '0'

    const queryString = new URLSearchParams(queryParams).toString()
    const fullUrl = `${url}?${queryString}`

    console.log('[ADZUNA] Searching:', { what, where, country, resultsPerPage })

    try {
      const response = await fetch(fullUrl, {
        method: 'GET',
        headers: {
          'Accept': 'application/json'
        }
      })

      if (!response.ok) {
        throw new Error(`Adzuna API error: ${response.status} ${response.statusText}`)
      }

      const data: AdzunaSearchResponse = await response.json()
      console.log(`[ADZUNA] Found ${data.results.length} jobs (total: ${data.count})`)

      return data

    } catch (error) {
      console.error('[ADZUNA] Search error:', error)
      throw error
    }
  }

  /**
   * Search multiple pages
   */
  async searchMultiplePages(
    params: AdzunaSearchParams,
    maxPages: number = 2
  ): Promise<AdzunaJob[]> {
    const allJobs: AdzunaJob[] = []

    for (let page = 1; page <= maxPages; page++) {
      try {
        const response = await this.searchJobs({ ...params, page })
        allJobs.push(...response.results)

        // Stop if we've got all results
        if (allJobs.length >= response.count) {
          break
        }

        // Rate limiting - wait 500ms between requests
        if (page < maxPages) {
          await new Promise(resolve => setTimeout(resolve, 500))
        }
      } catch (error) {
        console.error(`[ADZUNA] Error on page ${page}:`, error)
        break
      }
    }

    return allJobs
  }

  /**
   * Convert Adzuna job to our JobListing format
   */
  convertToJobListing(job: AdzunaJob): {
    jobId: string
    title: string
    company: string
    location: string
    description: string
    url: string
    source: string
    salary?: string
    postedDate?: Date
    workType?: 'remote' | 'hybrid' | 'onsite'
    experienceLevel?: 'entry' | 'mid' | 'senior' | 'executive'
  } {
    // Determine work type from description
    const descLower = job.description.toLowerCase()
    let workType: 'remote' | 'hybrid' | 'onsite' = 'onsite'
    if (descLower.includes('remote') || descLower.includes('work from home')) {
      workType = 'remote'
    } else if (descLower.includes('hybrid')) {
      workType = 'hybrid'
    }

    // Format salary
    let salary: string | undefined
    if (job.salary_min && job.salary_max) {
      const currency = job.location.area[0] === 'Canada' ? 'CAD' : 'USD'
      salary = `$${job.salary_min.toLocaleString()} - $${job.salary_max.toLocaleString()} ${currency}`
      if (job.salary_is_predicted) {
        salary += ' (estimated)'
      }
    }

    // Determine experience level from title
    const titleLower = job.title.toLowerCase()
    let experienceLevel: 'entry' | 'mid' | 'senior' | 'executive' = 'mid'
    if (titleLower.includes('junior') || titleLower.includes('entry')) {
      experienceLevel = 'entry'
    } else if (titleLower.includes('senior') || titleLower.includes('lead')) {
      experienceLevel = 'senior'
    } else if (titleLower.includes('principal') || titleLower.includes('director') || titleLower.includes('vp')) {
      experienceLevel = 'executive'
    }

    return {
      jobId: `adzuna_${job.id}`,
      title: job.title,
      company: job.company.display_name,
      location: job.location.display_name,
      description: job.description,
      url: job.redirect_url,
      source: 'adzuna',
      salary,
      postedDate: new Date(job.created),
      workType,
      experienceLevel
    }
  }

  /**
   * Check if API is configured
   */
  isConfigured(): boolean {
    return !!(this.appId && this.appKey)
  }
}

// Singleton instance
let adzunaClient: AdzunaAPIClient | null = null

export function getAdzunaClient(): AdzunaAPIClient {
  if (!adzunaClient) {
    adzunaClient = new AdzunaAPIClient()
  }
  return adzunaClient
}

export default AdzunaAPIClient
</file>

<file path="src/lib/comprehensive-data-sources.ts">
/**
 * COMPREHENSIVE JOB BOARDS + CONTACT SOURCES
 * Production-ready with all Canadian & Global boards
 */

export interface JobBoardSource {
  name: string
  baseUrl: string
  searchUrl: (title: string, location: string) => string
  priority: 1 | 2 | 3 // 1 = highest
  requiresAuth: boolean
  rateLimitPerMin: number
  scrapeSupport: 'easy' | 'medium' | 'hard' // Cheerio difficulty
}

export interface ContactSource {
  name: string
  baseUrl: string
  searchPattern: (company: string) => string
  reliability: number // 0-1
  requiresAuth: boolean
  apiKey?: string
}

// ========================================
// JOB BOARDS (15+ SOURCES)
// ========================================

export const COMPREHENSIVE_JOB_BOARDS: JobBoardSource[] = [
  // TIER 1: Canadian Priority Boards
  {
    name: 'Indeed Canada',
    baseUrl: 'https://ca.indeed.com',
    searchUrl: (title, location) => 
      `https://ca.indeed.com/jobs?q=${encodeURIComponent(title)}&l=${encodeURIComponent(location)}&sort=date&fromage=7`,
    priority: 1,
    requiresAuth: false,
    rateLimitPerMin: 60,
    scrapeSupport: 'easy'
  },
  {
    name: 'LinkedIn Jobs',
    baseUrl: 'https://www.linkedin.com',
    searchUrl: (title, location) => 
      `https://www.linkedin.com/jobs/search?keywords=${encodeURIComponent(title)}&location=${encodeURIComponent(location)}&f_TPR=r86400&sortBy=DD`,
    priority: 1,
    requiresAuth: false,
    rateLimitPerMin: 30,
    scrapeSupport: 'medium'
  },
  {
    name: 'Job Bank Canada',
    baseUrl: 'https://www.jobbank.gc.ca',
    searchUrl: (title, location) => 
      `https://www.jobbank.gc.ca/jobsearch/jobsearch?fjob=${encodeURIComponent(title)}&floc=${encodeURIComponent(location)}&sort=D`,
    priority: 1,
    requiresAuth: false,
    rateLimitPerMin: 60,
    scrapeSupport: 'easy'
  },
  {
    name: 'Workopolis',
    baseUrl: 'https://www.workopolis.com',
    searchUrl: (title, location) => 
      `https://www.workopolis.com/jobsearch/find-jobs?ak=${encodeURIComponent(title)}&l=${encodeURIComponent(location)}`,
    priority: 1,
    requiresAuth: false,
    rateLimitPerMin: 60,
    scrapeSupport: 'easy'
  },
  {
    name: 'Eluta',
    baseUrl: 'https://www.eluta.ca',
    searchUrl: (title, location) => 
      `https://www.eluta.ca/search?q=${encodeURIComponent(title)}&l=${encodeURIComponent(location)}`,
    priority: 2,
    requiresAuth: false,
    rateLimitPerMin: 60,
    scrapeSupport: 'easy'
  },
  {
    name: 'Glassdoor Canada',
    baseUrl: 'https://www.glassdoor.ca',
    searchUrl: (title, location) => 
      `https://www.glassdoor.ca/Job/jobs.htm?sc.keyword=${encodeURIComponent(title)}&locT=C&locKeyword=${encodeURIComponent(location)}`,
    priority: 2,
    requiresAuth: false,
    rateLimitPerMin: 30,
    scrapeSupport: 'medium'
  },
  {
    name: 'Monster Canada',
    baseUrl: 'https://www.monster.ca',
    searchUrl: (title, location) => 
      `https://www.monster.ca/jobs/search?q=${encodeURIComponent(title)}&where=${encodeURIComponent(location)}`,
    priority: 2,
    requiresAuth: false,
    rateLimitPerMin: 60,
    scrapeSupport: 'easy'
  },
  {
    name: 'CareerBeacon',
    baseUrl: 'https://www.careerbeacon.com',
    searchUrl: (title, location) => 
      `https://www.careerbeacon.com/en/search?keywords=${encodeURIComponent(title)}&location=${encodeURIComponent(location)}`,
    priority: 2,
    requiresAuth: false,
    rateLimitPerMin: 60,
    scrapeSupport: 'easy'
  },
  {
    name: 'Jobboom',
    baseUrl: 'https://www.jobboom.com',
    searchUrl: (title, location) => 
      `https://www.jobboom.com/en/job-search?keywords=${encodeURIComponent(title)}&location=${encodeURIComponent(location)}`,
    priority: 2,
    requiresAuth: false,
    rateLimitPerMin: 60,
    scrapeSupport: 'easy'
  },
  {
    name: 'Communitech Work in Tech',
    baseUrl: 'https://www1.communitech.ca',
    searchUrl: (title, location) => 
      `https://www1.communitech.ca/work-in-tech?search=${encodeURIComponent(title)}&location=${encodeURIComponent(location)}`,
    priority: 2,
    requiresAuth: false,
    rateLimitPerMin: 60,
    scrapeSupport: 'easy'
  },

  // TIER 2: Tech-Specific Boards
  {
    name: 'AngelList Talent',
    baseUrl: 'https://wellfound.com',
    searchUrl: (title, location) => 
      `https://wellfound.com/jobs?q=${encodeURIComponent(title)}&l=${encodeURIComponent(location)}`,
    priority: 2,
    requiresAuth: false,
    rateLimitPerMin: 30,
    scrapeSupport: 'medium'
  },
  {
    name: 'Dice',
    baseUrl: 'https://www.dice.com',
    searchUrl: (title, location) => 
      `https://www.dice.com/jobs?q=${encodeURIComponent(title)}&location=${encodeURIComponent(location)}&radius=30&radiusUnit=mi&page=1&pageSize=20&language=en`,
    priority: 3,
    requiresAuth: false,
    rateLimitPerMin: 60,
    scrapeSupport: 'easy'
  },
  {
    name: 'Stack Overflow Jobs',
    baseUrl: 'https://stackoverflow.com/jobs',
    searchUrl: (title, location) => 
      `https://stackoverflow.com/jobs?q=${encodeURIComponent(title)}&l=${encodeURIComponent(location)}`,
    priority: 3,
    requiresAuth: false,
    rateLimitPerMin: 60,
    scrapeSupport: 'medium'
  },

  // TIER 3: ATS Direct Searches
  {
    name: 'Greenhouse Public Boards',
    baseUrl: 'https://boards.greenhouse.io',
    searchUrl: (title, location) => 
      `https://boards.greenhouse.io/embed/job_board?gh_jid=${encodeURIComponent(title)}`,
    priority: 3,
    requiresAuth: false,
    rateLimitPerMin: 120,
    scrapeSupport: 'easy'
  },
  {
    name: 'Lever Jobs',
    baseUrl: 'https://jobs.lever.co',
    searchUrl: (title, location) => 
      `https://jobs.lever.co/search?query=${encodeURIComponent(title)}&location=${encodeURIComponent(location)}`,
    priority: 3,
    requiresAuth: false,
    rateLimitPerMin: 120,
    scrapeSupport: 'easy'
  }
]

// ========================================
// CONTACT SOURCES (10+ SOURCES)
// ========================================

export const COMPREHENSIVE_CONTACT_SOURCES: ContactSource[] = [
  {
    name: 'LinkedIn Company Search',
    baseUrl: 'https://www.linkedin.com',
    searchPattern: (company) => 
      `site:linkedin.com/in/ "${company}" (recruiter OR "talent acquisition" OR "human resources" OR "hiring manager")`,
    reliability: 0.95,
    requiresAuth: false
  },
  {
    name: 'LinkedIn Company Page',
    baseUrl: 'https://www.linkedin.com',
    searchPattern: (company) => 
      `site:linkedin.com/company/${company.toLowerCase().replace(/\s+/g, '-')} people`,
    reliability: 0.90,
    requiresAuth: false
  },
  {
    name: 'Company Careers Page',
    baseUrl: '',
    searchPattern: (company) => 
      `"${company}" (careers OR jobs OR "join our team") contact email`,
    reliability: 0.85,
    requiresAuth: false
  },
  {
    name: 'Hunter.io',
    baseUrl: 'https://hunter.io',
    searchPattern: (company) => 
      `${company.toLowerCase().replace(/\s+/g, '')}.com`,
    reliability: 0.80,
    requiresAuth: true,
    apiKey: process.env.HUNTER_API_KEY
  },
  {
    name: 'RocketReach',
    baseUrl: 'https://rocketreach.co',
    searchPattern: (company) => 
      `${company} recruiter`,
    reliability: 0.85,
    requiresAuth: true,
    apiKey: process.env.ROCKETREACH_API_KEY
  },
  {
    name: 'ContactOut',
    baseUrl: 'https://contactout.com',
    searchPattern: (company) => 
      `${company} hiring manager`,
    reliability: 0.80,
    requiresAuth: true
  },
  {
    name: 'Apollo.io',
    baseUrl: 'https://apollo.io',
    searchPattern: (company) => 
      `${company} talent acquisition`,
    reliability: 0.85,
    requiresAuth: true,
    apiKey: process.env.APOLLO_API_KEY
  },
  {
    name: 'Clearbit',
    baseUrl: 'https://clearbit.com',
    searchPattern: (company) => 
      `${company.toLowerCase().replace(/\s+/g, '')}.com`,
    reliability: 0.75,
    requiresAuth: true,
    apiKey: process.env.CLEARBIT_API_KEY
  },
  {
    name: 'ZoomInfo',
    baseUrl: 'https://www.zoominfo.com',
    searchPattern: (company) => 
      `${company} contacts`,
    reliability: 0.90,
    requiresAuth: true,
    apiKey: process.env.ZOOMINFO_API_KEY
  },
  {
    name: 'Lusha',
    baseUrl: 'https://www.lusha.com',
    searchPattern: (company) => 
      `${company} employees`,
    reliability: 0.75,
    requiresAuth: true
  }
]

// Helper: Get top N job boards by priority
export function getTopJobBoards(count: number = 10): JobBoardSource[] {
  return COMPREHENSIVE_JOB_BOARDS
    .sort((a, b) => a.priority - b.priority)
    .slice(0, count)
}

// Helper: Get all contact sources (free + paid)
export function getAllContactSources(includeAuth: boolean = false): ContactSource[] {
  return includeAuth 
    ? COMPREHENSIVE_CONTACT_SOURCES 
    : COMPREHENSIVE_CONTACT_SOURCES.filter(s => !s.requiresAuth)
}

// Helper: Get contact sources with API keys configured
export function getConfiguredContactSources(): ContactSource[] {
  return COMPREHENSIVE_CONTACT_SOURCES.filter(s => 
    !s.requiresAuth || (s.apiKey && s.apiKey.length > 0)
  )
}
</file>

<file path="src/lib/enhanced-canadian-scraper.ts">
import { WebScraperService } from './web-scraper'
import * as cheerio from 'cheerio'

// ─── Shared Job Result Type ─────────────────────────────────────────────────
export interface SharedJobResult {
  title?:string;
  company?:string;
  location?:string;
  url:string;
  salary?:string;
  date?:string;
  snippet?:string;
  source?:string;
}

// ──────────────────────────────────────────────────────────────────────────────


export class EnhancedCanadianJobScraper {
  private scraper = new WebScraperService()
  
  async scrapeJobBankDirect(keywords:string, location:string): Promise<SharedJobResult[]> {
    const searchUrl = `https://www.jobbank.gc.ca/jobsearch/jobsearch?searchstring=${encodeURIComponent(keywords)}&locationstring=${encodeURIComponent(location)}`
    const response = await fetch(searchUrl)
    const html = await response.text()
    const $ = cheerio.load(html)
    
    const jobs: SharedJobResult[] = []
    $('.resultJobItem').each((i, elem) => {
      const title = $(elem).find('h3 a').text().trim()
      const company = $(elem).find('.business').text().trim()
      const loc = $(elem).find('.location').text().trim()
      const url = $(elem).find('h3 a').attr('href')
      const salary = $(elem).find('.salary').text().trim()
      const date = $(elem).find('.date').text().trim()
      
      if (title) {
        jobs.push({
          title,
          company,
          location: loc || location,
          url: url ? `https://www.jobbank.gc.ca${url}` : searchUrl,
          salary,
          date: new Date(date).toISOString() || new Date().toISOString()
        })
      }
    })
    
    return jobs.slice(0, 15)
  }
  
  async scrapeIndeedCanadaDirect(keywords:string, location:string): Promise<SharedJobResult[]> {
    const searchUrl = `https://ca.indeed.com/jobs?q=${encodeURIComponent(keywords)}&l=${encodeURIComponent(location)}`
    const response = await fetch(searchUrl)
    const html = await response.text()
    const $ = cheerio.load(html)
    
    const jobs: SharedJobResult[] = []
    $('.job_seen_beacon').each((i, elem) => {
      const title = $(elem).find('h2 a span').text().trim()
      const company = $(elem).find('.companyName').text().trim()
      const loc = $(elem).find('.companyLocation').text().trim()
      const url = $(elem).find('h2 a').attr('href')
      const salary = $(elem).find('.salary-snippet').text().trim()
      const date = $(elem).find('.date').text().trim()
      
      if (title) {
        jobs.push({
          title,
          company,
          location: loc || location,
          url: url ? `https://ca.indeed.com${url}` : searchUrl,
          salary,
          date: new Date(date).toISOString() || new Date().toISOString()
        })
      }
    })
    
    return jobs.slice(0, 15)
  }
  
  async combineAllSources(keywords: string, location: string): Promise<SharedJobResult[]> {
    // Scrape bank and indeed (already SharedJobResult[])
    const bankJobs = await this.scrapeJobBankDirect(keywords, location);
    const indeedJobs = await this.scrapeIndeedCanadaDirect(keywords, location);

    // Normalize Google results to SharedJobResult
    const googleResultsRaw = await this.scraper.searchJobsByGoogle({ jobTitle: keywords, location });
    const googleJobs: SharedJobResult[] = googleResultsRaw.map(r => ({
      title: r.title,
      url: r.url,
      snippet: r.snippet,
      source: r.source
    }));

    // Combine all
    const allJobs = [...bankJobs, ...indeedJobs, ...googleJobs];

    // Dedupe by URL
    const uniqueJobs = allJobs.filter((job, index, self) =>
      index === self.findIndex(j => j.url === job.url)
    );

    // Sort by salary safely
    return uniqueJobs.sort((a, b) => {
      const scoreA = a.salary ? parseFloat(a.salary.replace(/[^\d.]/g, '')) : 0;
      const scoreB = b.salary ? parseFloat(b.salary.replace(/[^\d.]/g, '')) : 0;
      return scoreB - scoreA;
    });
  }
}
</file>

<file path="src/lib/job-scraper-service.ts">
/**
 * Bulletproof Job Scraper Service
 * 
 * Multi-source job scraping with:
 * - Puppeteer for dynamic sites (Indeed, LinkedIn)
 * - Cheerio for static sites (faster)
 * - Anti-bot detection bypass
 * - Retry logic with exponential backoff
 * - Rate limiting per source
 * - Proxy support
 */

import puppeteer, { Browser, Page } from 'puppeteer'

// User agents for rotation
const USER_AGENTS = [
  'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
  'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
  'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
  'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
  'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'
]

export interface JobListing {
  jobId: string
  title: string
  company: string
  location: string
  description: string
  url: string
  source: string
  salary?: string
  postedDate?: Date
  workType?: 'remote' | 'hybrid' | 'onsite'
  experienceLevel?: 'entry' | 'mid' | 'senior' | 'executive'
  skills?: string[]
  skillMatchScore?: number
}

export interface ScraperOptions {
  maxResults?: number
  useProxy?: boolean
  timeout?: number
  retries?: number
}

class JobScraperService {
  private browser: Browser | null = null
  private rateLimits: Map<string, number> = new Map()
  private readonly RATE_LIMIT_MS = 2000 // 2 seconds between requests per source

  /**
   * Initialize browser instance (reuse for performance)
   */
  private async getBrowser(): Promise<Browser> {
    if (this.browser && this.browser.isConnected()) {
      return this.browser
    }

    console.log('[SCRAPER] Launching browser...')
    this.browser = await puppeteer.launch({
      headless: true,
      args: [
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        '--disable-accelerated-2d-canvas',
        '--disable-gpu',
        '--window-size=1920x1080',
        '--disable-blink-features=AutomationControlled'
      ]
    })

    return this.browser
  }

  /**
   * Create a new page with anti-detection measures
   */
  private async createStealthPage(): Promise<Page> {
    const browser = await this.getBrowser()
    const page = await browser.newPage()

    // Random user agent
    const userAgent = USER_AGENTS[Math.floor(Math.random() * USER_AGENTS.length)]
    await page.setUserAgent(userAgent)

    // Set viewport
    await page.setViewport({ width: 1920, height: 1080 })

    // Remove webdriver flag
    await page.evaluateOnNewDocument(() => {
      Object.defineProperty(navigator, 'webdriver', {
        get: () => false
      })
    })

    // Add realistic headers
    await page.setExtraHTTPHeaders({
      'Accept-Language': 'en-US,en;q=0.9',
      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
      'Accept-Encoding': 'gzip, deflate, br',
      'Connection': 'keep-alive',
      'Upgrade-Insecure-Requests': '1'
    })

    return page
  }

  /**
   * Rate limiting per source
   */
  private async respectRateLimit(source: string): Promise<void> {
    const lastRequest = this.rateLimits.get(source) || 0
    const now = Date.now()
    const timeSinceLastRequest = now - lastRequest

    if (timeSinceLastRequest < this.RATE_LIMIT_MS) {
      const waitTime = this.RATE_LIMIT_MS - timeSinceLastRequest
      console.log(`[SCRAPER] Rate limiting ${source}, waiting ${waitTime}ms`)
      await new Promise(resolve => setTimeout(resolve, waitTime))
    }

    this.rateLimits.set(source, Date.now())
  }

  /**
   * Retry logic with exponential backoff
   */
  private async retryWithBackoff<T>(
    fn: () => Promise<T>,
    retries: number = 3,
    delay: number = 1000
  ): Promise<T> {
    try {
      return await fn()
    } catch (error) {
      if (retries === 0) throw error

      console.log(`[SCRAPER] Retry failed, ${retries} attempts remaining. Waiting ${delay}ms...`)
      await new Promise(resolve => setTimeout(resolve, delay))
      
      return this.retryWithBackoff(fn, retries - 1, delay * 2)
    }
  }

  /**
   * Scrape Indeed Canada
   */
  async scrapeIndeed(
    keywords: string,
    location: string,
    options: ScraperOptions = {}
  ): Promise<JobListing[]> {
    const { maxResults = 25, retries = 3 } = options
    const source = 'indeed'

    await this.respectRateLimit(source)

    return this.retryWithBackoff(async () => {
      const page = await this.createStealthPage()
      const jobs: JobListing[] = []

      try {
        const searchUrl = `https://ca.indeed.com/jobs?q=${encodeURIComponent(keywords)}&l=${encodeURIComponent(location)}`
        console.log(`[SCRAPER] Indeed: ${searchUrl}`)

        await page.goto(searchUrl, { waitUntil: 'networkidle2', timeout: 30000 })

        // Wait for job cards to load
        await page.waitForSelector('.job_seen_beacon, .jobsearch-ResultsList', { timeout: 10000 })

        // Extract job listings
        const jobElements = await page.$$('.job_seen_beacon')

        for (let i = 0; i < Math.min(jobElements.length, maxResults); i++) {
          try {
            const element = jobElements[i]

            const title = await element.$eval('.jobTitle', el => el.textContent?.trim() || '')
            const company = await element.$eval('[data-testid="company-name"]', el => el.textContent?.trim() || '').catch(() => '')
            const jobLocation = await element.$eval('[data-testid="text-location"]', el => el.textContent?.trim() || '').catch(() => location)
            const salary = await element.$eval('.salary-snippet', el => el.textContent?.trim() || '').catch(() => undefined)
            const url = await element.$eval('.jobTitle a', el => (el as HTMLAnchorElement).href).catch(() => '')
            const snippet = await element.$eval('.job-snippet', el => el.textContent?.trim() || '').catch(() => '')

            if (title && company && url) {
              jobs.push({
                jobId: `indeed_${Buffer.from(url).toString('base64').slice(0, 16)}`,
                title,
                company,
                location: jobLocation,
                description: snippet,
                url: url.startsWith('http') ? url : `https://ca.indeed.com${url}`,
                source: 'indeed',
                salary,
                workType: snippet.toLowerCase().includes('remote') ? 'remote' : 
                         snippet.toLowerCase().includes('hybrid') ? 'hybrid' : 'onsite'
              })
            }
          } catch (err) {
            console.error('[SCRAPER] Error extracting Indeed job:', err)
          }
        }

        console.log(`[SCRAPER] Indeed: Found ${jobs.length} jobs`)
        return jobs

      } finally {
        await page.close()
      }
    }, retries)
  }

  /**
   * Scrape Job Bank Canada (Government site - no API key needed!)
   */
  async scrapeJobBank(
    keywords: string,
    location: string,
    options: ScraperOptions = {}
  ): Promise<JobListing[]> {
    const { maxResults = 25, retries = 3 } = options
    const source = 'jobbank'

    await this.respectRateLimit(source)

    return this.retryWithBackoff(async () => {
      const page = await this.createStealthPage()
      const jobs: JobListing[] = []

      try {
        const searchUrl = `https://www.jobbank.gc.ca/jobsearch/jobsearch?searchstring=${encodeURIComponent(keywords)}&locationstring=${encodeURIComponent(location)}`
        console.log(`[SCRAPER] Job Bank: ${searchUrl}`)

        await page.goto(searchUrl, { waitUntil: 'networkidle2', timeout: 30000 })

        // Wait for results
        await page.waitForSelector('article.resultJobItem, .noresult', { timeout: 10000 })

        // Check if no results
        const noResults = await page.$('.noresult')
        if (noResults) {
          console.log('[SCRAPER] Job Bank: No results found')
          return []
        }

        // Extract job listings
        const jobElements = await page.$$('article.resultJobItem')

        for (let i = 0; i < Math.min(jobElements.length, maxResults); i++) {
          try {
            const element = jobElements[i]

            const title = await element.$eval('.resultJobItemTitle', el => el.textContent?.trim() || '')
            const company = await element.$eval('.resultJobItemEmployer', el => el.textContent?.trim() || '').catch(() => '')
            const jobLocation = await element.$eval('.resultJobItemLocation', el => el.textContent?.trim() || '').catch(() => location)
            const salary = await element.$eval('.resultJobItemWage', el => el.textContent?.trim() || '').catch(() => undefined)
            const url = await element.$eval('.resultJobItemTitle a', el => (el as HTMLAnchorElement).href).catch(() => '')

            if (title && url) {
              jobs.push({
                jobId: `jobbank_${Buffer.from(url).toString('base64').slice(0, 16)}`,
                title,
                company: company || 'Government of Canada',
                location: jobLocation,
                description: `${title} at ${company || 'Government of Canada'}`,
                url: url.startsWith('http') ? url : `https://www.jobbank.gc.ca${url}`,
                source: 'jobbank',
                salary
              })
            }
          } catch (err) {
            console.error('[SCRAPER] Error extracting Job Bank job:', err)
          }
        }

        console.log(`[SCRAPER] Job Bank: Found ${jobs.length} jobs`)
        return jobs

      } finally {
        await page.close()
      }
    }, retries)
  }

  /**
   * Scrape LinkedIn Jobs (using your existing API)
   */
  async scrapeLinkedIn(
    keywords: string,
    location: string,
    options: ScraperOptions = {}
  ): Promise<JobListing[]> {
    const { maxResults = 25, retries = 3 } = options
    const source = 'linkedin'

    await this.respectRateLimit(source)

    return this.retryWithBackoff(async () => {
      const page = await this.createStealthPage()
      const jobs: JobListing[] = []

      try {
        const searchUrl = `https://www.linkedin.com/jobs/search/?keywords=${encodeURIComponent(keywords)}&location=${encodeURIComponent(location)}`
        console.log(`[SCRAPER] LinkedIn: ${searchUrl}`)

        await page.goto(searchUrl, { waitUntil: 'networkidle2', timeout: 30000 })

        // Wait for job cards
        await page.waitForSelector('.jobs-search__results-list li, .base-search-card', { timeout: 10000 })

        // Extract job listings
        const jobElements = await page.$$('.base-search-card')

        for (let i = 0; i < Math.min(jobElements.length, maxResults); i++) {
          try {
            const element = jobElements[i]

            const title = await element.$eval('.base-search-card__title', el => el.textContent?.trim() || '')
            const company = await element.$eval('.base-search-card__subtitle', el => el.textContent?.trim() || '').catch(() => '')
            const jobLocation = await element.$eval('.job-search-card__location', el => el.textContent?.trim() || '').catch(() => location)
            const url = await element.$eval('a.base-card__full-link', el => (el as HTMLAnchorElement).href).catch(() => '')

            if (title && company && url) {
              jobs.push({
                jobId: `linkedin_${Buffer.from(url).toString('base64').slice(0, 16)}`,
                title,
                company,
                location: jobLocation,
                description: `${title} at ${company}`,
                url,
                source: 'linkedin'
              })
            }
          } catch (err) {
            console.error('[SCRAPER] Error extracting LinkedIn job:', err)
          }
        }

        console.log(`[SCRAPER] LinkedIn: Found ${jobs.length} jobs`)
        return jobs

      } finally {
        await page.close()
      }
    }, retries)
  }

  /**
   * Scrape Glassdoor Canada
   */
  async scrapeGlassdoor(
    keywords: string,
    location: string,
    options: ScraperOptions = {}
  ): Promise<JobListing[]> {
    const { maxResults = 25, retries = 3 } = options
    const source = 'glassdoor'

    await this.respectRateLimit(source)

    return this.retryWithBackoff(async () => {
      const page = await this.createStealthPage()
      const jobs: JobListing[] = []

      try {
        const searchUrl = `https://www.glassdoor.ca/Job/jobs.htm?sc.keyword=${encodeURIComponent(keywords)}&locT=C&locId=2&locKeyword=${encodeURIComponent(location)}`
        console.log(`[SCRAPER] Glassdoor: ${searchUrl}`)

        await page.goto(searchUrl, { waitUntil: 'networkidle2', timeout: 30000 })

        // Wait for job cards
        await page.waitForSelector('li[data-test="jobListing"], .JobsList_jobListItem__wjTHv', { timeout: 10000 })

        // Extract job listings
        const jobElements = await page.$$('li[data-test="jobListing"]')

        for (let i = 0; i < Math.min(jobElements.length, maxResults); i++) {
          try {
            const element = jobElements[i]

            const title = await element.$eval('[data-test="job-title"]', el => el.textContent?.trim() || '')
            const company = await element.$eval('[data-test="employer-name"]', el => el.textContent?.trim() || '').catch(() => '')
            const jobLocation = await element.$eval('[data-test="emp-location"]', el => el.textContent?.trim() || '').catch(() => location)
            const salary = await element.$eval('[data-test="detailSalary"]', el => el.textContent?.trim() || '').catch(() => undefined)
            const url = await element.$eval('a[data-test="job-link"]', el => (el as HTMLAnchorElement).href).catch(() => '')

            if (title && company && url) {
              jobs.push({
                jobId: `glassdoor_${Buffer.from(url).toString('base64').slice(0, 16)}`,
                title,
                company,
                location: jobLocation,
                description: `${title} at ${company}`,
                url: url.startsWith('http') ? url : `https://www.glassdoor.ca${url}`,
                source: 'glassdoor',
                salary
              })
            }
          } catch (err) {
            console.error('[SCRAPER] Error extracting Glassdoor job:', err)
          }
        }

        console.log(`[SCRAPER] Glassdoor: Found ${jobs.length} jobs`)
        return jobs

      } finally {
        await page.close()
      }
    }, retries)
  }

  /**
   * Aggregate jobs from multiple sources
   */
  async aggregateJobs(
    keywords: string,
    location: string,
    options: ScraperOptions = {}
  ): Promise<JobListing[]> {
    const { maxResults = 100 } = options

    console.log(`[SCRAPER] Aggregating jobs for "${keywords}" in "${location}"`)

    // Run all scrapers in parallel
    const results = await Promise.allSettled([
      this.scrapeIndeed(keywords, location, { maxResults: 25 }),
      this.scrapeJobBank(keywords, location, { maxResults: 25 }),
      this.scrapeLinkedIn(keywords, location, { maxResults: 25 }),
      this.scrapeGlassdoor(keywords, location, { maxResults: 25 })
    ])

    // Collect all successful results
    const allJobs: JobListing[] = []
    results.forEach((result, index) => {
      const sources = ['Indeed', 'Job Bank', 'LinkedIn', 'Glassdoor']
      if (result.status === 'fulfilled') {
        console.log(`[SCRAPER] ${sources[index]}: ${result.value.length} jobs`)
        allJobs.push(...result.value)
      } else {
        console.error(`[SCRAPER] ${sources[index]} failed:`, result.reason)
      }
    })

    // Deduplicate by URL
    const uniqueJobs = Array.from(
      new Map(allJobs.map(job => [job.url, job])).values()
    )

    console.log(`[SCRAPER] Total unique jobs: ${uniqueJobs.length}`)

    return uniqueJobs.slice(0, maxResults)
  }

  /**
   * Cleanup browser
   */
  async cleanup(): Promise<void> {
    if (this.browser) {
      await this.browser.close()
      this.browser = null
      console.log('[SCRAPER] Browser closed')
    }
  }
}

// Singleton instance
let scraperInstance: JobScraperService | null = null

export function getJobScraper(): JobScraperService {
  if (!scraperInstance) {
    scraperInstance = new JobScraperService()
  }
  return scraperInstance
}

export default JobScraperService
</file>

<file path="src/lib/public-job-boards-config.ts">
/**
 * Public Job Boards Configuration
 * 
 * This configuration includes ONLY job boards with:
 * 1. Public listings that can be scraped via Perplexity
 * 2. Open APIs that don't require partnerships
 * 3. ATS platforms with public job feeds
 * 
 * Based on October 2025 accessibility research
 */

export type JobBoardAccessType = 
  | 'public-api'           // Has open public API
  | 'ats-public'          // ATS with public job feeds
  | 'scraping-allowed'    // Public listings, scraping via Perplexity
  | 'government-open'     // Government job board with open data

export interface PublicJobBoardConfig {
  name: string
  displayName: string
  country: string
  accessType: JobBoardAccessType
  scrapingConfig?: {
    baseUrl: string
    searchUrl: string
    perplexityQuery: string
    canUsePerplexity: boolean
  }
  apiConfig?: {
    baseUrl: string
    requiresAuth: boolean
    authType?: 'api-key' | 'oauth' | 'none'
    documentation?: string
  }
  features: {
    canDiscoverJobs: boolean
    canApplyDirectly: boolean
    estimatedJobCount: string
    updateFrequency: string
  }
}

/**
 * CANADIAN JOB BOARDS - Priority for your target market
 */
export const CANADIAN_JOB_BOARDS: Record<string, PublicJobBoardConfig> = {
  jobbank: {
    name: 'jobbank',
    displayName: 'Job Bank Canada',
    country: 'Canada',
    accessType: 'government-open',
    scrapingConfig: {
      baseUrl: 'https://www.jobbank.gc.ca',
      searchUrl: 'https://www.jobbank.gc.ca/jobsearch/jobsearch',
      perplexityQuery: 'site:jobbank.gc.ca "{keywords}" "{location}" after:2024-01-01',
      canUsePerplexity: true
    },
    apiConfig: {
      baseUrl: 'https://www.jobbank.gc.ca',
      requiresAuth: false,
      authType: 'none',
      documentation: 'https://www.jobbank.gc.ca/content_pieces-eng.do?cid=8524'
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false, // Redirects to employer sites
      estimatedJobCount: '100,000+',
      updateFrequency: 'Daily'
    }
  },

  jobboom: {
    name: 'jobboom',
    displayName: 'Jobboom',
    country: 'Canada',
    accessType: 'scraping-allowed',
    scrapingConfig: {
      baseUrl: 'https://www.jobboom.com',
      searchUrl: 'https://www.jobboom.com/en/job-search',
      perplexityQuery: 'site:jobboom.com "{keywords}" "{location}" after:2024-01-01',
      canUsePerplexity: true
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '50,000+',
      updateFrequency: 'Daily'
    }
  },

  workopolis: {
    name: 'workopolis',
    displayName: 'Workopolis',
    country: 'Canada',
    accessType: 'scraping-allowed',
    scrapingConfig: {
      baseUrl: 'https://www.workopolis.com',
      searchUrl: 'https://www.workopolis.com/jobsearch/jobs',
      perplexityQuery: 'site:workopolis.com "{keywords}" "{location}" after:2024-01-01',
      canUsePerplexity: true
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '30,000+',
      updateFrequency: 'Daily'
    }
  },

  jooble: {
    name: 'jooble',
    displayName: 'Jooble Canada',
    country: 'Canada',
    accessType: 'public-api',
    apiConfig: {
      baseUrl: 'https://ca.jooble.org/api',
      requiresAuth: true,
      authType: 'api-key',
      documentation: 'https://jooble.org/api/about'
    },
    scrapingConfig: {
      baseUrl: 'https://ca.jooble.org',
      searchUrl: 'https://ca.jooble.org/search',
      perplexityQuery: 'site:ca.jooble.org "{keywords}" "{location}" after:2024-01-01',
      canUsePerplexity: true
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '100,000+',
      updateFrequency: 'Daily'
    }
  },

  indeedca: {
    name: 'indeedca',
    displayName: 'Indeed Canada',
    country: 'Canada',
    accessType: 'scraping-allowed',
    scrapingConfig: {
      baseUrl: 'https://ca.indeed.com',
      searchUrl: 'https://ca.indeed.com/jobs',
      perplexityQuery: 'site:ca.indeed.com "{keywords}" "{location}" after:2024-01-01',
      canUsePerplexity: true
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '500,000+',
      updateFrequency: 'Real-time'
    }
  },

  ziprecruiter_ca: {
    name: 'ziprecruiter_ca',
    displayName: 'ZipRecruiter Canada',
    country: 'Canada',
    accessType: 'scraping-allowed',
    scrapingConfig: {
      baseUrl: 'https://www.ziprecruiter.ca',
      searchUrl: 'https://www.ziprecruiter.ca/jobs-search',
      perplexityQuery: 'site:ziprecruiter.ca "{keywords}" "{location}" after:2024-01-01',
      canUsePerplexity: true
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '50,000+',
      updateFrequency: 'Daily'
    }
  },

  monster_ca: {
    name: 'monster_ca',
    displayName: 'Monster Canada',
    country: 'Canada',
    accessType: 'scraping-allowed',
    scrapingConfig: {
      baseUrl: 'https://www.monster.ca',
      searchUrl: 'https://www.monster.ca/jobs/search',
      perplexityQuery: 'site:monster.ca "{keywords}" "{location}" after:2024-01-01',
      canUsePerplexity: true
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '40,000+',
      updateFrequency: 'Daily'
    }
  },

  glassdoor_ca: {
    name: 'glassdoor_ca',
    displayName: 'Glassdoor Canada',
    country: 'Canada',
    accessType: 'scraping-allowed',
    scrapingConfig: {
      baseUrl: 'https://www.glassdoor.ca',
      searchUrl: 'https://www.glassdoor.ca/Job/jobs.htm',
      perplexityQuery: 'site:glassdoor.ca/Job "{keywords}" "{location}" after:2024-01-01',
      canUsePerplexity: true
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '100,000+',
      updateFrequency: 'Daily'
    }
  },

  dice_ca: {
    name: 'dice_ca',
    displayName: 'Dice Canada',
    country: 'Canada',
    accessType: 'scraping-allowed',
    scrapingConfig: {
      baseUrl: 'https://www.dice.com',
      searchUrl: 'https://www.dice.com/jobs',
      perplexityQuery: 'site:dice.com "{keywords}" Canada after:2024-01-01',
      canUsePerplexity: true
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '20,000+',
      updateFrequency: 'Daily'
    }
  }
}

/**
 * MAJOR JOB BOARDS - Public listings only (no direct API access)
 */
export const MAJOR_JOB_BOARDS: Record<string, PublicJobBoardConfig> = {
  linkedin: {
    name: 'linkedin',
    displayName: 'LinkedIn',
    country: 'Global',
    accessType: 'scraping-allowed',
    scrapingConfig: {
      baseUrl: 'https://www.linkedin.com',
      searchUrl: 'https://www.linkedin.com/jobs/search',
      perplexityQuery: 'site:linkedin.com/jobs "{keywords}" "{location}" after:2024-01-01',
      canUsePerplexity: true
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false, // Requires frontend automation
      estimatedJobCount: '20M+',
      updateFrequency: 'Real-time'
    }
  },

  indeed: {
    name: 'indeed',
    displayName: 'Indeed Canada',
    country: 'Canada',
    accessType: 'scraping-allowed',
    scrapingConfig: {
      baseUrl: 'https://ca.indeed.com',
      searchUrl: 'https://ca.indeed.com/jobs',
      perplexityQuery: 'site:ca.indeed.com "{keywords}" "{location}" after:2024-01-01',
      canUsePerplexity: true
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '5M+',
      updateFrequency: 'Real-time'
    }
  },

  glassdoor: {
    name: 'glassdoor',
    displayName: 'Glassdoor',
    country: 'Global',
    accessType: 'scraping-allowed',
    scrapingConfig: {
      baseUrl: 'https://www.glassdoor.ca',
      searchUrl: 'https://www.glassdoor.ca/Job/jobs.htm',
      perplexityQuery: 'site:glassdoor.ca/Job "{keywords}" "{location}" after:2024-01-01',
      canUsePerplexity: true
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '2M+',
      updateFrequency: 'Daily'
    }
  }
}

/**
 * OPEN API JOB BOARDS - Require API keys but are publicly accessible
 */
export const OPEN_API_BOARDS: Record<string, PublicJobBoardConfig> = {
  usajobs: {
    name: 'usajobs',
    displayName: 'USAJobs',
    country: 'United States',
    accessType: 'public-api',
    apiConfig: {
      baseUrl: 'https://data.usajobs.gov/api',
      requiresAuth: true,
      authType: 'api-key',
      documentation: 'https://developer.usajobs.gov'
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '500K+',
      updateFrequency: 'Real-time'
    }
  },

  adzuna: {
    name: 'adzuna',
    displayName: 'Adzuna',
    country: 'Global',
    accessType: 'public-api',
    apiConfig: {
      baseUrl: 'https://api.adzuna.com/v1/api',
      requiresAuth: true,
      authType: 'api-key',
      documentation: 'https://developer.adzuna.com/docs'
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '10M+',
      updateFrequency: 'Real-time'
    }
  },

  careerjet: {
    name: 'careerjet',
    displayName: 'Careerjet',
    country: 'Global',
    accessType: 'public-api',
    apiConfig: {
      baseUrl: 'https://public-api.careerjet.com',
      requiresAuth: true,
      authType: 'api-key',
      documentation: 'https://www.careerjet.com/partners/api/'
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '20M+',
      updateFrequency: 'Real-time'
    }
  }
}

/**
 * ATS PLATFORMS - Public job feeds from company career pages
 */
export const ATS_PLATFORMS: Record<string, PublicJobBoardConfig> = {
  greenhouse: {
    name: 'greenhouse',
    displayName: 'Greenhouse ATS',
    country: 'Global',
    accessType: 'ats-public',
    apiConfig: {
      baseUrl: 'https://api.greenhouse.io/v1/boards',
      requiresAuth: false,
      authType: 'none',
      documentation: 'https://developers.greenhouse.io/job-board.html'
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '200K+',
      updateFrequency: 'Real-time'
    }
  },

  lever: {
    name: 'lever',
    displayName: 'Lever ATS',
    country: 'Global',
    accessType: 'ats-public',
    apiConfig: {
      baseUrl: 'https://api.lever.co/v0/postings',
      requiresAuth: false,
      authType: 'none',
      documentation: 'https://github.com/lever/postings-api'
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '150K+',
      updateFrequency: 'Real-time'
    }
  },

  workable: {
    name: 'workable',
    displayName: 'Workable ATS',
    country: 'Global',
    accessType: 'ats-public',
    apiConfig: {
      baseUrl: 'https://apply.workable.com/api/v1/widget/accounts',
      requiresAuth: false,
      authType: 'none',
      documentation: 'https://workable.readme.io/reference/job-board-api'
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '100K+',
      updateFrequency: 'Real-time'
    }
  },

  ashby: {
    name: 'ashby',
    displayName: 'Ashby ATS',
    country: 'Global',
    accessType: 'ats-public',
    apiConfig: {
      baseUrl: 'https://api.ashbyhq.com/posting-api/job-board',
      requiresAuth: false,
      authType: 'none',
      documentation: 'https://developers.ashbyhq.com/reference/postingapi'
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '50K+',
      updateFrequency: 'Real-time'
    }
  },

  recruitee: {
    name: 'recruitee',
    displayName: 'Recruitee ATS',
    country: 'Global',
    accessType: 'ats-public',
    apiConfig: {
      baseUrl: 'https://{company}.recruitee.com/api/offers',
      requiresAuth: false,
      authType: 'none',
      documentation: 'https://developers.recruitee.com'
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '30K+',
      updateFrequency: 'Real-time'
    }
  },

  jooble_api: {
    name: 'jooble_api',
    displayName: 'Jooble API',
    country: 'Global',
    accessType: 'public-api',
    apiConfig: {
      baseUrl: 'https://jooble.org/api',
      requiresAuth: true,
      authType: 'api-key',
      documentation: 'https://jooble.org/api/about'
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '5M+',
      updateFrequency: 'Real-time'
    }
  },

  careerjet_ca: {
    name: 'careerjet_ca',
    displayName: 'Careerjet Canada',
    country: 'Canada',
    accessType: 'public-api',
    apiConfig: {
      baseUrl: 'https://public-api.careerjet.ca/search',
      requiresAuth: true,
      authType: 'api-key',
      documentation: 'https://www.careerjet.com/partners/api/'
    },
    features: {
      canDiscoverJobs: true,
      canApplyDirectly: false,
      estimatedJobCount: '200K+',
      updateFrequency: 'Real-time'
    }
  }
}

/**
 * ALL PUBLIC JOB BOARDS - Combined configuration
 */
export const ALL_PUBLIC_BOARDS = {
  ...CANADIAN_JOB_BOARDS,
  ...MAJOR_JOB_BOARDS,
  ...OPEN_API_BOARDS,
  ...ATS_PLATFORMS
}

/**
 * Get boards by access type
 */
export function getBoardsByAccessType(accessType: JobBoardAccessType): PublicJobBoardConfig[] {
  return Object.values(ALL_PUBLIC_BOARDS).filter(board => board.accessType === accessType)
}

/**
 * Get Canadian boards only
 */
export function getCanadianBoards(): PublicJobBoardConfig[] {
  return Object.values(ALL_PUBLIC_BOARDS).filter(board => board.country === 'Canada')
}

/**
 * Get boards that can be scraped via Perplexity
 */
export function getPerplexityScrapableBoards(): PublicJobBoardConfig[] {
  return Object.values(ALL_PUBLIC_BOARDS).filter(
    board => board.scrapingConfig?.canUsePerplexity
  )
}

/**
 * Get boards with open APIs
 */
export function getOpenAPIBoards(): PublicJobBoardConfig[] {
  return Object.values(ALL_PUBLIC_BOARDS).filter(
    board => board.apiConfig && !board.apiConfig.requiresAuth
  )
}

/**
 * Priority order for job discovery
 * (Canadian boards first, then major boards, then open APIs)
 */
export const DISCOVERY_PRIORITY_ORDER = [
  'jobbank',         // Canada government (highest priority)
  'jobboom',         // Canadian bilingual
  'workopolis',      // Canadian
  'jooble',          // Canadian job aggregator
  'indeedca',        // Indeed Canada (major board)
  'careerjet_ca',    // Careerjet Canada
  'ziprecruiter_ca', // ZipRecruiter Canada
  'monster_ca',      // Monster Canada
  'glassdoor_ca',    // Glassdoor Canada
  'dice_ca',         // Dice Canada (tech jobs)
  'linkedin',        // LinkedIn (global)
  'indeed',          // Indeed (global)
  'glassdoor',       // Glassdoor (global)
  'greenhouse',      // ATS platform
  'lever',           // ATS platform
  'workable',        // ATS platform
  'recruitee',       // ATS platform
  'ashby',           // ATS platform
  'adzuna',          // Open API aggregator
  'jooble_api',      // Jooble API
  'careerjet',       // Open API aggregator (global)
  'usajobs'          // Government (US)
]

/**
 * Companies using specific ATS platforms (curated list)
 */
export const ATS_COMPANY_DIRECTORY = {
  greenhouse: [
    'airbnb', 'pinterest', 'coinbase', 'robinhood', 'gitlab',
    'doordash', 'figma', 'notion', 'airtable', 'segment',
    'datadog', 'plaid', 'contentful', 'grammarly', 'flexport'
  ],
  lever: [
    'netflix', 'uber', 'spotify', 'postmates', 'box',
    'shopify', 'canva', 'discord', 'coda', 'superhuman',
    'vercel', 'linear', 'mercury', 'ramp', 'brex'
  ],
  workable: [
    'beat', 'workable', 'instacar', 'skroutz', 'persado',
    'epignosis', 'goodvidio', 'scytl', 'quality-unit', 'omnisend'
  ],
  ashby: [
    'ashby', 'descript', 'runway', 'scale', 'ramp',
    'mercury', 'lattice', 'compound', 'rippling', 'scale'
  ],
  recruitee: [
    'recruitee', 'bynder', 'catawiki', 'sendcloud', 'mollie',
    'peak', 'channable', 'trengo', 'effectory', 'piggy'
  ]
}

/**
 * Canadian companies using ATS platforms
 */
export const CANADIAN_ATS_COMPANIES = {
  greenhouse: [
    'shopify', 'hootsuite', 'wealthsimple', 'faire', 'thinkific',
    'lightspeed', 'financeit', 'later', 'clickup', 'copperleaf'
  ],
  lever: [
    'slack', 'wealthsimple', 'hootsuite', 'shopify', 'bench',
    'clio', 'clearco', 'flashfood', 'league', 'properly'
  ],
  workable: [
    'freshbooks', 'visier', 'unbounce', 'axonify', 'crowdriff',
    'soapbox', 'klue', 'samdesk', 'coinsquare', 'tulip'
  ],
  recruitee: [
    'paytm', 'ecobee', 'geotab', 'auvik', 'alida',
    'miovision', 'nulogy', 'ritual', 'wave', 'koho'
  ],
  ashby: [
    'faire', 'clearco', 'notion', 'part', 'properly',
    'district', 'maple', 'borrowell', 'league', 'shakepay'
  ]
}
</file>

<file path="src/lib/public-job-discovery-service.ts">
/**
 * Public Job Discovery Service
 * 
 * Discovers jobs from PUBLIC sources only:
 * 1. Canadian job boards (Job Bank, Jobboom, Workopolis)
 * 2. Major boards via Perplexity scraping (LinkedIn, Indeed, Glassdoor)
 * 3. Open API aggregators (Adzuna, Careerjet, USAJobs)
 * 4. ATS platforms (Greenhouse, Lever, Workable, Ashby)
 */

import { PerplexityService } from './perplexity-service'
import { PerplexityIntelligenceService } from './perplexity-intelligence'
import {
  ALL_PUBLIC_BOARDS,
  CANADIAN_JOB_BOARDS,
  ATS_COMPANY_DIRECTORY,
  DISCOVERY_PRIORITY_ORDER,
  type PublicJobBoardConfig
} from './public-job-boards-config'

export interface JobSearchQuery {
  keywords: string
  location?: string
  boards?: string[]  // Specific boards to search, or all if empty
  limit?: number
  remote?: boolean
  salaryMin?: number
  experienceLevel?: 'entry' | 'mid' | 'senior'
}

export interface DiscoveredJob {
  id: string
  title: string
  company: string
  location: string
  description: string
  url: string
  salary?: string
  datePosted?: string
  source: string
  sourceDisplayName: string
  sourceType: 'scraping' | 'api' | 'ats'
  applyMethod: 'external' | 'direct' | 'manual'
}

export class PublicJobDiscoveryService {
  private perplexity: PerplexityService

  constructor() {
    this.perplexity = new PerplexityService()
  }

  /**
   * Main entry point - discover jobs from all available public sources
   */
  async discoverJobs(query: JobSearchQuery): Promise<DiscoveredJob[]> {
    const { keywords, location = 'Canada', boards, limit = 100 } = query

    // Determine which boards to search
    const boardsToSearch = boards && boards.length > 0
      ? boards
      : DISCOVERY_PRIORITY_ORDER

    console.log(`[JOB_DISCOVERY] Searching ${boardsToSearch.length} job boards for "${keywords}" in ${location}`)

    // Search all boards in parallel
    const searchPromises = boardsToSearch.map(boardName => 
      this.searchSingleBoard(boardName, query).catch(error => {
        console.error(`[JOB_DISCOVERY] Failed to search ${boardName}:`, error)
        return []
      })
    )

    const results = await Promise.all(searchPromises)
    const allJobs = results.flat()

    // Deduplicate and rank
    const uniqueJobs = this.deduplicateJobs(allJobs)
    const rankedJobs = this.rankJobsByRelevance(uniqueJobs, query)

    console.log(`[JOB_DISCOVERY] Found ${rankedJobs.length} unique jobs from ${boardsToSearch.length} sources`)

    return rankedJobs.slice(0, limit)
  }

  /**
   * Search a single job board
   */
  private async searchSingleBoard(boardName: string, query: JobSearchQuery): Promise<DiscoveredJob[]> {
    const config = ALL_PUBLIC_BOARDS[boardName]
    
    if (!config) {
      console.warn(`[JOB_DISCOVERY] Unknown board: ${boardName}`)
      return []
    }

    if (!config.features.canDiscoverJobs) {
      console.warn(`[JOB_DISCOVERY] ${config.displayName} does not support job discovery`)
      return []
    }

    // Route to appropriate search method
    switch (config.accessType) {
      case 'scraping-allowed':
      case 'government-open':
        return this.searchViaPerplexity(config, query)
      
      case 'public-api':
        return this.searchViaPublicAPI(config, query)
      
      case 'ats-public':
        return this.searchViaATS(config, query)
      
      default:
        console.warn(`[JOB_DISCOVERY] Unknown access type for ${config.displayName}`)
        return []
    }
  }

  /**
   * Search via Perplexity web scraping (Canadian boards, LinkedIn, Indeed, Glassdoor)
   */
  private async searchViaPerplexity(
    config: PublicJobBoardConfig,
    query: JobSearchQuery
  ): Promise<DiscoveredJob[]> {
    if (!config.scrapingConfig?.canUsePerplexity) {
      return []
    }

    const { keywords, location = 'Canada', remote, salaryMin } = query

    // Build Perplexity search query
    let searchQuery = config.scrapingConfig.perplexityQuery
      .replace('{keywords}', keywords)
      .replace('{location}', location)

    if (remote) {
      searchQuery += ' remote'
    }

    if (salaryMin) {
      searchQuery += ` salary:>${salaryMin}`
    }

    try {
      const results = await PerplexityIntelligenceService.jobQuickSearch(
        searchQuery,
        [new URL(config.scrapingConfig.baseUrl).hostname],
        20,
        'week'
      )

      return results.map((result: any) => this.normalizeJob(result, config, 'scraping'))
    } catch (error) {
      console.error(`[JOB_DISCOVERY] Perplexity search failed for ${config.displayName}:`, error)
      return []
    }
  }

  /**
   * Search via public APIs (Adzuna, Careerjet, USAJobs)
   */
  private async searchViaPublicAPI(
    config: PublicJobBoardConfig,
    query: JobSearchQuery
  ): Promise<DiscoveredJob[]> {
    if (!config.apiConfig) {
      return []
    }

    const { keywords, location = 'Canada' } = query

    try {
      // Route to specific API implementation
      switch (config.name) {
        case 'adzuna':
          return this.searchAdzuna(keywords, location)
        
        case 'careerjet':
          return this.searchCareerjet(keywords, location)
        
        case 'usajobs':
          return this.searchUSAJobs(keywords, location)
        
        default:
          console.warn(`[JOB_DISCOVERY] No API implementation for ${config.name}`)
          return []
      }
    } catch (error) {
      console.error(`[JOB_DISCOVERY] API search failed for ${config.displayName}:`, error)
      return []
    }
  }

  /**
   * Search via ATS platforms (Greenhouse, Lever, Workable, Ashby)
   */
  private async searchViaATS(
    config: PublicJobBoardConfig,
    query: JobSearchQuery
  ): Promise<DiscoveredJob[]> {
    if (!config.apiConfig) {
      return []
    }

    const { keywords } = query
    const companies = ATS_COMPANY_DIRECTORY[config.name] || []

    if (companies.length === 0) {
      return []
    }

    // Search all companies using this ATS
    const companySearches = companies.map(company =>
      this.searchATSCompany(config, company, keywords).catch(() => [])
    )

    const results = await Promise.all(companySearches)
    return results.flat()
  }

  /**
   * Search jobs from a specific company's ATS
   */
  private async searchATSCompany(
    config: PublicJobBoardConfig,
    company: string,
    keywords: string
  ): Promise<DiscoveredJob[]> {
    try {
      const url = `${config.apiConfig!.baseUrl}/${company}${config.name === 'lever' ? '?mode=json' : config.name === 'greenhouse' ? '/jobs?content=true' : ''}`
      
      const response = await fetch(url, {
        headers: {
          'Accept': 'application/json',
          'User-Agent': 'CareerLeverAI/1.0'
        }
      })

      if (!response.ok) {
        return []
      }

      const data = await response.json()
      const jobs = this.parseATSJobs(data, config.name, company)
      
      // Filter by keywords
      return jobs.filter(job => 
        job.title.toLowerCase().includes(keywords.toLowerCase()) ||
        job.description.toLowerCase().includes(keywords.toLowerCase())
      )
    } catch (error) {
      return []
    }
  }

  /**
   * Parse ATS-specific job formats
   */
  private parseATSJobs(data: any, atsName: string, company: string): DiscoveredJob[] {
    const jobs: any[] = data.jobs || data.postings || data || []

    return jobs.map(job => ({
      id: job.id || job.externalId || `${company}-${job.title}`,
      title: job.title || job.text || '',
      company: company,
      location: job.location?.name || job.location || job.categories?.location || '',
      description: job.description || job.content?.description || '',
      url: job.hostedUrl || job.applyUrl || `https://jobs.${atsName}.com/${company}/${job.id}`,
      salary: this.extractSalary(job),
      datePosted: job.createdAt || job.publishedAt || new Date().toISOString(),
      source: atsName,
      sourceDisplayName: ALL_PUBLIC_BOARDS[atsName].displayName,
      sourceType: 'ats' as const,
      applyMethod: 'external' as const
    }))
  }

  /**
   * Adzuna API search
   */
  private async searchAdzuna(keywords: string, location: string): Promise<DiscoveredJob[]> {
    const appId = process.env.ADZUNA_APP_ID
    const appKey = process.env.ADZUNA_API_KEY

    if (!appId || !appKey) {
      console.warn('[JOB_DISCOVERY] Adzuna API credentials not configured')
      return []
    }

    try {
      const country = location.includes('Canada') ? 'ca' : 'us'
      const url = `https://api.adzuna.com/v1/api/jobs/${country}/search/1?${new URLSearchParams({
        app_id: appId,
        app_key: appKey,
        what: keywords,
        where: location,
        results_per_page: '50'
      })}`

      const response = await fetch(url)
      const data = await response.json()

      return (data.results || []).map((job: any) => ({
        id: job.id,
        title: job.title,
        company: job.company.display_name,
        location: job.location.display_name,
        description: job.description,
        url: job.redirect_url,
        salary: job.salary_min ? `$${job.salary_min} - $${job.salary_max}` : undefined,
        datePosted: job.created,
        source: 'adzuna',
        sourceDisplayName: 'Adzuna',
        sourceType: 'api',
        applyMethod: 'external'
      }))
    } catch (error) {
      console.error('[JOB_DISCOVERY] Adzuna search failed:', error)
      return []
    }
  }

  /**
   * Careerjet API search
   */
  private async searchCareerjet(keywords: string, location: string): Promise<DiscoveredJob[]> {
    const apiKey = process.env.CAREERJET_API_KEY

    if (!apiKey) {
      console.warn('[JOB_DISCOVERY] Careerjet API key not configured')
      return []
    }

    // Careerjet implementation would go here
    // Omitted for brevity - similar pattern to Adzuna
    return []
  }

  /**
   * USAJobs API search
   */
  private async searchUSAJobs(keywords: string, location: string): Promise<DiscoveredJob[]> {
    const apiKey = process.env.USAJOBS_API_KEY
    const email = process.env.USAJOBS_EMAIL

    if (!apiKey || !email) {
      console.warn('[JOB_DISCOVERY] USAJobs API credentials not configured')
      return []
    }

    // USAJobs implementation would go here
    // Omitted for brevity - similar pattern to Adzuna
    return []
  }

  /**
   * Normalize job from different sources into common format
   */
  private normalizeJob(rawJob: any, config: PublicJobBoardConfig, sourceType: 'scraping' | 'api' | 'ats'): DiscoveredJob {
    return {
      id: rawJob.id || rawJob.url || `${config.name}-${Date.now()}`,
      title: rawJob.title || rawJob.jobTitle || '',
      company: rawJob.company || rawJob.companyName || '',
      location: rawJob.location || rawJob.jobLocation || '',
      description: rawJob.description || rawJob.summary || '',
      url: rawJob.url || rawJob.link || '',
      salary: rawJob.salary,
      datePosted: rawJob.postedDate || rawJob.date || new Date().toISOString(),
      source: config.name,
      sourceDisplayName: config.displayName,
      sourceType,
      applyMethod: 'external'
    }
  }

  /**
   * Deduplicate jobs by title + company
   */
  private deduplicateJobs(jobs: DiscoveredJob[]): DiscoveredJob[] {
    const unique = new Map<string, DiscoveredJob>()

    for (const job of jobs) {
      const key = `${job.title.toLowerCase().trim()}-${job.company.toLowerCase().trim()}`
      
      // Prefer API results over scraping if duplicate
      if (!unique.has(key) || (unique.get(key)!.sourceType === 'scraping' && job.sourceType === 'api')) {
        unique.set(key, job)
      }
    }

    return Array.from(unique.values())
  }

  /**
   * Rank jobs by relevance to query
   */
  private rankJobsByRelevance(jobs: DiscoveredJob[], query: JobSearchQuery): DiscoveredJob[] {
    return jobs.sort((a, b) => {
      let scoreA = 0
      let scoreB = 0

      // Prefer Canadian sources
      if (CANADIAN_JOB_BOARDS[a.source]) scoreA += 10
      if (CANADIAN_JOB_BOARDS[b.source]) scoreB += 10

      // Prefer recent posts
      if (a.datePosted && this.isRecent(a.datePosted)) scoreA += 5
      if (b.datePosted && this.isRecent(b.datePosted)) scoreB += 5

      // Prefer jobs with salary info
      if (a.salary) scoreA += 3
      if (b.salary) scoreB += 3

      // Prefer API/ATS over scraping (more reliable data)
      if (a.sourceType !== 'scraping') scoreA += 2
      if (b.sourceType !== 'scraping') scoreB += 2

      return scoreB - scoreA
    })
  }

  /**
   * Check if job was posted recently (within 7 days)
   */
  private isRecent(datePosted: string): boolean {
    const posted = new Date(datePosted)
    const now = new Date()
    const daysDiff = (now.getTime() - posted.getTime()) / (1000 * 60 * 60 * 24)
    return daysDiff <= 7
  }

  /**
   * Extract salary from various formats
   */
  private extractSalary(job: any): string | undefined {
    if (job.salary) return job.salary
    if (job.salaryMin && job.salaryMax) return `$${job.salaryMin} - $${job.salaryMax}`
    if (job.compensation?.min && job.compensation?.max) return `$${job.compensation.min} - $${job.compensation.max}`
    return undefined
  }
}
</file>

<file path="src/lib/scrapers/advanced-scraper.ts">
/**
 * Advanced Web Scraper with 4-Tier Fallback Strategy
 * 
 * Strategy 1: JSON-LD Structured Data (fastest, most reliable)
 * Strategy 2: Cheerio HTML Parsing (fast, reliable for static sites)
 * Strategy 3: Puppeteer Browser (for JavaScript-heavy sites)
 * Strategy 4: Regex Extraction (last resort)
 * 
 * @module AdvancedScraper
 * @description Extracts job posting data from URLs using multiple fallback strategies
 */

import * as cheerio from 'cheerio'
import puppeteer from 'puppeteer-core'
import chromium from '@sparticuz/chromium'

export interface ScrapeResult {
  success: boolean
  data?: {
    title?: string
    description?: string
    requirements?: string[]
    salary?: string
    company?: string
    location?: string
    postedDate?: string
  }
  method?: 'structured' | 'cheerio' | 'puppeteer' | 'regex'
  error?: string
}

export class AdvancedScraper {
  private readonly USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
  ]

  /**
   * Main scraping method with 3-tier fallback
   */
  async scrape(url: string): Promise<ScrapeResult> {
    if (process.env.PPX_DEBUG === 'true') {
      console.log(`[SCRAPER] Processing: ${url}`)
    }

    // Strategy 1: Structured data (JSON-LD) - fastest and most reliable
    try {
      const structured = await this.tryStructuredData(url)
      if (structured.success && structured.data?.description && structured.data.description.length > 100) {
        if (process.env.PPX_DEBUG === 'true') {
          console.log('[SCRAPER] ✓ Structured data found')
        }
        return { ...structured, method: 'structured' }
      }
    } catch (e) {
      if (process.env.PPX_DEBUG === 'true') {
        console.log('[SCRAPER] Structured data failed:', (e as Error).message)
      }
    }

    // Strategy 2: Cheerio HTML parsing - fast and reliable for static sites
    try {
      const cheerioResult = await this.tryCheerioScraping(url)
      if (cheerioResult.success && cheerioResult.data?.description && cheerioResult.data.description.length > 100) {
        if (process.env.PPX_DEBUG === 'true') {
          console.log('[SCRAPER] ✓ Cheerio parsing succeeded')
        }
        return { ...cheerioResult, method: 'cheerio' }
      }
    } catch (e) {
      if (process.env.PPX_DEBUG === 'true') {
        console.log('[SCRAPER] Cheerio failed:', (e as Error).message)
      }
    }

    // Strategy 3: Puppeteer browser - for JavaScript-heavy sites (Indeed, LinkedIn, etc.)
    try {
      const puppeteerResult = await this.tryPuppeteerScraping(url)
      if (puppeteerResult.success && puppeteerResult.data?.description && puppeteerResult.data.description.length > 100) {
        if (process.env.PPX_DEBUG === 'true') {
          console.log('[SCRAPER] ✓ Puppeteer scraping succeeded')
        }
        return { ...puppeteerResult, method: 'puppeteer' }
      }
    } catch (e) {
      if (process.env.PPX_DEBUG === 'true') {
        console.log('[SCRAPER] Puppeteer failed:', (e as Error).message)
      }
    }

    // Strategy 4: Regex extraction - last resort
    try {
      const regex = await this.tryRegexExtraction(url)
      if (regex.success && regex.data?.description && regex.data.description.length > 100) {
        if (process.env.PPX_DEBUG === 'true') {
          console.log('[SCRAPER] ✓ Regex extraction succeeded')
        }
        return { ...regex, method: 'regex' }
      }
    } catch (e) {
      if (process.env.PPX_DEBUG === 'true') {
        console.log('[SCRAPER] Regex failed:', (e as Error).message)
      }
    }

    return {
      success: false,
      error: 'All 4 scraping strategies failed - page may require login or CAPTCHA'
    }
  }

  /**
   * Strategy 1: Extract JSON-LD structured data
   * Many job boards include this for SEO
   */
  private async tryStructuredData(url: string): Promise<ScrapeResult> {
    const html = await this.fetchHTML(url)
    const jsonLdMatches = html.match(/<script type="application\/ld\+json">(.*?)<\/script>/gs)

    if (!jsonLdMatches) {
      return { success: false, error: 'No structured data found' }
    }

    for (const match of jsonLdMatches) {
      try {
        const json = JSON.parse(match.replace(/<\/?script[^>]*>/g, ''))

        // Check for JobPosting schema
        if (json['@type'] === 'JobPosting') {
          return {
            success: true,
            data: {
              title: json.title,
              description: json.description,
              company: json.hiringOrganization?.name,
              location: json.jobLocation?.address?.addressLocality || json.jobLocation?.address?.addressRegion,
              salary: this.extractSalaryFromStructured(json.baseSalary),
              postedDate: json.datePosted
            }
          }
        }
      } catch {
        continue
      }
    }

    return { success: false, error: 'No JobPosting structured data found' }
  }

  /**
   * Strategy 2: Cheerio HTML parsing
   * Works for most standard HTML pages
   */
  private async tryCheerioScraping(url: string): Promise<ScrapeResult> {
    const html = await this.fetchHTML(url)
    const $ = cheerio.load(html)

    // Remove noise elements
    $('script, style, nav, header, footer, aside, .advertisement, .ads').remove()

    // Try multiple selectors for description (ordered by specificity)
    const descriptionSelectors = [
      '.job-description',
      '[class*="job-description"]',
      '[class*="description"]',
      '[id*="description"]',
      '[class*="job-details"]',
      '[class*="jobDetails"]',
      '[data-job-description]',
      'article',
      'main',
      '.content'
    ]

    let description = ''
    for (const selector of descriptionSelectors) {
      const text = $(selector).text().trim()
      if (text.length > description.length && text.length > 100) {
        description = text
      }
    }

    // Extract title
    const title = 
      $('h1.job-title').text() ||
      $('[class*="job-title"]').first().text() ||
      $('[class*="jobTitle"]').first().text() ||
      $('h1').first().text() ||
      ''

    // Extract requirements
    const requirements: string[] = []
    $('.requirements li, .qualifications li, [class*="requirement"] li, [class*="qualification"] li').each((i, el) => {
      const req = $(el).text().trim()
      if (req && req.length > 10 && req.length < 500) {
        requirements.push(req)
      }
    })

    // Extract salary
    const salary = this.extractSalaryFromText(html)

    // Extract company
    const company = 
      $('[class*="company-name"]').first().text() ||
      $('[class*="companyName"]').first().text() ||
      $('[data-company]').text() ||
      ''

    // Extract location
    const location = 
      $('[class*="location"]').first().text() ||
      $('[class*="job-location"]').first().text() ||
      ''

    return {
      success: description.length > 100,
      data: {
        title: this.cleanText(title),
        description: this.cleanText(description),
        requirements,
        salary: this.cleanText(salary),
        company: this.cleanText(company),
        location: this.cleanText(location)
      }
    }
  }

  /**
   * Strategy 3: Puppeteer browser scraping (for JavaScript-heavy sites)
   * Handles Indeed, LinkedIn, Glassdoor, and other dynamic job boards
   */
  private async tryPuppeteerScraping(url: string): Promise<ScrapeResult> {
    let browser: Awaited<ReturnType<typeof puppeteer.launch>> | null = null
    try {
      // Launch headless browser with optimized settings
      const args = [
        ...chromium.args,
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        '--disable-gpu',
        '--no-first-run',
        '--no-zygote',
        '--single-process',
        '--disable-blink-features=AutomationControlled'
      ]

      const executablePath = process.env.CHROMIUM_PATH || await chromium.executablePath()

      browser = await puppeteer.launch({
        args,
        executablePath,
        headless: true,
        timeout: 30000
      })

      const page = await browser.newPage()

      // Set realistic user agent and viewport
      const userAgent = this.USER_AGENTS[Math.floor(Math.random() * this.USER_AGENTS.length)]
      await page.setUserAgent(userAgent)
      await page.setViewport({ width: 1920, height: 1080 })

      // Navigate to page and wait for content
      await page.goto(url, {
        waitUntil: 'networkidle2',
        timeout: 30000
      })

      // Wait for job description to load (common selectors)
      await page.waitForSelector('body', { timeout: 5000 }).catch(() => {})

      // Extract job data using page.evaluate
      const data = await page.evaluate(() => {
        // Helper to clean text
        const cleanText = (text: string) => text.replace(/\s+/g, ' ').trim()

        // Extract title
        const titleSelectors = [
          'h1[class*="title"]',
          'h1[class*="jobTitle"]',
          'h1[class*="job-title"]',
          '[data-testid="jobTitle"]',
          '.job-title',
          'h1'
        ]
        let title = ''
        for (const sel of titleSelectors) {
          const el = document.querySelector(sel)
          if (el?.textContent) {
            title = cleanText(el.textContent)
            break
          }
        }

        // Extract description
        const descSelectors = [
          '[class*="jobDescriptionText"]',
          '[class*="job-description"]',
          '[id*="jobDescriptionText"]',
          '[data-testid="jobDescription"]',
          '.description',
          'article',
          'main'
        ]
        let description = ''
        for (const sel of descSelectors) {
          const el = document.querySelector(sel)
          if (el?.textContent && el.textContent.length > description.length) {
            description = cleanText(el.textContent)
          }
        }

        // Extract company
        const companySelectors = [
          '[class*="companyName"]',
          '[data-testid="companyName"]',
          '[class*="company-name"]',
          '.company'
        ]
        let company = ''
        for (const sel of companySelectors) {
          const el = document.querySelector(sel)
          if (el?.textContent) {
            company = cleanText(el.textContent)
            break
          }
        }

        // Extract location
        const locationSelectors = [
          '[class*="location"]',
          '[data-testid="location"]',
          '[class*="job-location"]'
        ]
        let location = ''
        for (const sel of locationSelectors) {
          const el = document.querySelector(sel)
          if (el?.textContent) {
            location = cleanText(el.textContent)
            break
          }
        }

        // Extract salary
        const salarySelectors = [
          '[class*="salary"]',
          '[data-testid="salary"]',
          '[class*="compensation"]'
        ]
        let salary = ''
        for (const sel of salarySelectors) {
          const el = document.querySelector(sel)
          if (el?.textContent) {
            salary = cleanText(el.textContent)
            break
          }
        }

        return { title, description, company, location, salary }
      })

      await browser.close()

      return {
        success: data.description.length > 100,
        data: {
          title: data.title,
          description: data.description,
          company: data.company,
          location: data.location,
          salary: data.salary || undefined
        }
      }
    } catch (error) {
      if (browser) {
        try { await browser.close() } catch {}
      }
      throw error
    }
  }

  /**
   * Strategy 4: Regex extraction (last resort)
   * Works when HTML structure is non-standard
   */
  private async tryRegexExtraction(url: string): Promise<ScrapeResult> {
    const html = await this.fetchHTML(url)

    // Extract description between common patterns
    const descPatterns = [
      /<div[^>]*class="[^"]*description[^"]*"[^>]*>(.*?)<\/div>/is,
      /<section[^>]*class="[^"]*job[^"]*"[^>]*>(.*?)<\/section>/is,
      /<article[^>]*>(.*?)<\/article>/is,
      /<main[^>]*>(.*?)<\/main>/is
    ]

    let description = ''
    for (const pattern of descPatterns) {
      const match = html.match(pattern)
      if (match && match[1].length > description.length) {
        description = match[1]
      }
    }

    // Extract title
    const titleMatch = html.match(/<h1[^>]*>(.*?)<\/h1>/i)
    const title = titleMatch ? titleMatch[1] : ''

    return {
      success: description.length > 100,
      data: {
        title: this.cleanHTML(title),
        description: this.cleanHTML(description)
      }
    }
  }

  /**
   * Fetch HTML with realistic headers to avoid bot detection
   */
  private async fetchHTML(url: string): Promise<string> {
    const userAgent = this.USER_AGENTS[Math.floor(Math.random() * this.USER_AGENTS.length)]

    const response = await fetch(url, {
      headers: {
        'User-Agent': userAgent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Referer': 'https://www.google.com/'
      },
      signal: AbortSignal.timeout(15000)
    })

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`)
    }

    // Add small delay to be respectful
    await new Promise(resolve => setTimeout(resolve, 500 + Math.random() * 1000))

    return await response.text()
  }

  /**
   * Helper: Extract salary from structured data
   */
  private extractSalaryFromStructured(baseSalary: any): string {
    if (!baseSalary) return ''
    if (typeof baseSalary === 'string') return baseSalary
    
    if (baseSalary.value) {
      const value = baseSalary.value.value || baseSalary.value
      const currency = baseSalary.currency || '$'
      return `${currency}${value}`
    }
    
    if (baseSalary.minValue && baseSalary.maxValue) {
      const currency = baseSalary.currency || '$'
      return `${currency}${baseSalary.minValue} - ${currency}${baseSalary.maxValue}`
    }
    
    return ''
  }

  /**
   * Helper: Extract salary from text using patterns
   */
  private extractSalaryFromText(text: string): string {
    const patterns = [
      /\$\s*[\d,]+\s*-\s*\$\s*[\d,]+/,
      /\$\s*[\d,]+k?\s*-\s*\$?\s*[\d,]+k?/i,
      /salary:\s*\$?[\d,]+\s*-\s*\$?[\d,]+/i,
      /[\d,]+\s*-\s*[\d,]+\s*per\s+(?:year|hour|month)/i,
      /compensation:\s*\$?[\d,]+\s*-\s*\$?[\d,]+/i
    ]

    for (const pattern of patterns) {
      const match = text.match(pattern)
      if (match) return match[0]
    }

    return ''
  }

  /**
   * Helper: Clean HTML tags and entities
   */
  private cleanHTML(html: string): string {
    return html
      .replace(/<script[^>]*>.*?<\/script>/gis, '')
      .replace(/<style[^>]*>.*?<\/style>/gis, '')
      .replace(/<[^>]+>/g, ' ')
      .replace(/&nbsp;/g, ' ')
      .replace(/&amp;/g, '&')
      .replace(/&lt;/g, '<')
      .replace(/&gt;/g, '>')
      .replace(/&quot;/g, '"')
      .replace(/&#39;/g, "'")
      .replace(/\s+/g, ' ')
      .trim()
  }

  /**
   * Helper: Clean text (whitespace only)
   */
  private cleanText(text: string): string {
    return text
      .replace(/\s+/g, ' ')
      .replace(/\n\s*\n/g, '\n')
      .trim()
  }
}
</file>

<file path="download-edmonton-jobs.ts">
/**
 * Mass Download: Edmonton + 160km radius
 * Target: 10,000+ jobs from Adzuna
 */

import { config } from 'dotenv'
config({ path: '.env.local' })

import { bulkDownloadJobs } from './src/lib/supabase-bulk-download'

async function downloadEdmontonJobs() {
  console.log('🚀 MASS DOWNLOAD: Edmonton + 160km Radius')
  console.log('Target: 10,000+ jobs from Adzuna\n')

  // Edmonton + surrounding cities within 160km
  const locations = [
    'Edmonton, AB',           // Main city
    'St. Albert, AB',         // 19km north
    'Sherwood Park, AB',      // 13km east
    'Spruce Grove, AB',       // 11km west
    'Leduc, AB',              // 33km south
    'Fort Saskatchewan, AB',  // 25km northeast
    'Stony Plain, AB',        // 27km west
    'Beaumont, AB',           // 23km south
    'Devon, AB',              // 35km southwest
    'Morinville, AB',         // 34km north
    'Wetaskiwin, AB',         // 70km south
    'Camrose, AB',            // 90km southeast
    'Drayton Valley, AB',     // 130km southwest
    'Lloydminster, AB',       // 250km east (border city, still relevant)
    'Red Deer, AB'            // 145km south (major city)
  ]

  console.log(`📍 Locations to scrape: ${locations.length}`)
  locations.forEach((loc, i) => console.log(`   ${i + 1}. ${loc}`))
  console.log('')

  const startTime = Date.now()

  try {
    await bulkDownloadJobs(locations)
    
    const duration = Math.round((Date.now() - startTime) / 1000 / 60)
    console.log(`\n✅ COMPLETE! Duration: ${duration} minutes`)
    
  } catch (error) {
    console.error('❌ Download failed:', error)
    process.exit(1)
  }
}

downloadEdmontonJobs()
</file>

<file path="src/lib/contact-scraper.ts">
/**
 * Comprehensive Contact Scraper
 * 
 * Scrapes hiring manager contacts from multiple sources:
 * 1. LinkedIn (via RapidAPI LinkedIn Scraper)
 * 2. Company website (Puppeteer + Cheerio)
 * 3. Hunter.io (email finder)
 * 4. Company Contact Scraper API (RapidAPI)
 * 
 * Validates emails and enriches with additional data
 */

import * as cheerio from 'cheerio'
import puppeteer from 'puppeteer-core'
import chromium from '@sparticuz/chromium'

export interface Contact {
  name: string
  title?: string
  email?: string
  linkedinUrl?: string
  phone?: string
  department?: string
  source: 'linkedin' | 'company-website' | 'hunter' | 'rapidapi' | 'manual'
  confidence: number // 0-100
  verified: boolean
}

export interface ContactScraperResult {
  success: boolean
  contacts: Contact[]
  company: string
  totalFound: number
  method: string[]
  error?: string
}

export class ContactScraper {
  private readonly USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15'
  ]

  /**
   * Main method: Scrape contacts from multiple sources
   */
  async scrapeContacts(params: {
    companyName: string
    companyWebsite?: string
    linkedinCompanyUrl?: string
    jobTitle?: string
  }): Promise<ContactScraperResult> {
    console.log('[CONTACT_SCRAPER] Starting scrape for:', params.companyName)
    
    const contacts: Contact[] = []
    const methods: string[] = []
    
    // METHOD 1: RapidAPI Company Contact Scraper
    try {
      const rapidApiContacts = await this.scrapeViaRapidAPI(params.companyName, params.companyWebsite)
      if (rapidApiContacts.length > 0) {
        contacts.push(...rapidApiContacts)
        methods.push('rapidapi')
        console.log(`[CONTACT_SCRAPER] RapidAPI found ${rapidApiContacts.length} contacts`)
      }
    } catch (error) {
      console.error('[CONTACT_SCRAPER] RapidAPI error:', error)
    }
    
    // METHOD 2: LinkedIn Scraper (RapidAPI)
    if (params.linkedinCompanyUrl) {
      try {
        const linkedinContacts = await this.scrapeLinkedIn(params.linkedinCompanyUrl, params.jobTitle)
        if (linkedinContacts.length > 0) {
          contacts.push(...linkedinContacts)
          methods.push('linkedin')
          console.log(`[CONTACT_SCRAPER] LinkedIn found ${linkedinContacts.length} contacts`)
        }
      } catch (error) {
        console.error('[CONTACT_SCRAPER] LinkedIn error:', error)
      }
    }
    
    // METHOD 3: Company Website Scraping
    if (params.companyWebsite) {
      try {
        const websiteContacts = await this.scrapeCompanyWebsite(params.companyWebsite)
        if (websiteContacts.length > 0) {
          contacts.push(...websiteContacts)
          methods.push('website')
          console.log(`[CONTACT_SCRAPER] Website found ${websiteContacts.length} contacts`)
        }
      } catch (error) {
        console.error('[CONTACT_SCRAPER] Website scraping error:', error)
      }
    }
    
    // Deduplicate and rank by confidence
    const uniqueContacts = this.deduplicateContacts(contacts)
    const rankedContacts = uniqueContacts.sort((a, b) => b.confidence - a.confidence)
    
    console.log(`[CONTACT_SCRAPER] ✅ Total: ${rankedContacts.length} unique contacts`)
    
    return {
      success: rankedContacts.length > 0,
      contacts: rankedContacts,
      company: params.companyName,
      totalFound: rankedContacts.length,
      method: methods
    }
  }
  
  /**
   * METHOD 1: RapidAPI Company Contact Scraper
   */
  private async scrapeViaRapidAPI(companyName: string, website?: string): Promise<Contact[]> {
    const apiKey = process.env.RAPIDAPI_KEY
    if (!apiKey) {
      console.warn('[CONTACT_SCRAPER] No RapidAPI key found')
      return []
    }
    
    try {
      // Use Company Contact Scraper API
      const searchQuery = website || companyName
      const response = await fetch(
        `https://company-contact-scraper.p.rapidapi.com/search?query=${encodeURIComponent(searchQuery)}`,
        {
          method: 'GET',
          headers: {
            'X-RapidAPI-Key': apiKey,
            'X-RapidAPI-Host': 'company-contact-scraper.p.rapidapi.com'
          }
        }
      )
      
      if (!response.ok) {
        console.warn('[CONTACT_SCRAPER] RapidAPI returned', response.status)
        return []
      }
      
      const data = await response.json()
      
      // Parse response and extract contacts
      const contacts: Contact[] = []
      
      if (data.contacts && Array.isArray(data.contacts)) {
        for (const contact of data.contacts) {
          contacts.push({
            name: contact.name || 'Unknown',
            title: contact.title || contact.position,
            email: contact.email,
            phone: contact.phone,
            department: contact.department,
            source: 'rapidapi',
            confidence: 85, // High confidence from API
            verified: !!contact.email
          })
        }
      }
      
      return contacts
    } catch (error) {
      console.error('[CONTACT_SCRAPER] RapidAPI error:', error)
      return []
    }
  }
  
  /**
   * METHOD 2: LinkedIn Scraper via RapidAPI
   */
  private async scrapeLinkedIn(companyUrl: string, targetTitle?: string): Promise<Contact[]> {
    const apiKey = process.env.RAPIDAPI_KEY
    if (!apiKey) {
      console.warn('[CONTACT_SCRAPER] No RapidAPI key found')
      return []
    }
    
    try {
      // Use Real-Time LinkedIn Scraper API
      const response = await fetch(
        `https://real-time-linkedin-scraper-api.p.rapidapi.com/company-employees?url=${encodeURIComponent(companyUrl)}`,
        {
          method: 'GET',
          headers: {
            'X-RapidAPI-Key': apiKey,
            'X-RapidAPI-Host': 'real-time-linkedin-scraper-api.p.rapidapi.com'
          }
        }
      )
      
      if (!response.ok) {
        console.warn('[CONTACT_SCRAPER] LinkedIn API returned', response.status)
        return []
      }
      
      const data = await response.json()
      const contacts: Contact[] = []
      
      if (data.employees && Array.isArray(data.employees)) {
        for (const employee of data.employees) {
          // Filter by target title if provided (e.g., "recruiter", "hiring manager", "HR")
          const title = employee.title || employee.headline || ''
          const isRelevant = !targetTitle || 
            title.toLowerCase().includes(targetTitle.toLowerCase()) ||
            title.toLowerCase().includes('recruiter') ||
            title.toLowerCase().includes('talent') ||
            title.toLowerCase().includes('hr') ||
            title.toLowerCase().includes('hiring')
          
          if (isRelevant) {
            contacts.push({
              name: employee.name || 'Unknown',
              title: title,
              linkedinUrl: employee.profileUrl || employee.url,
              source: 'linkedin',
              confidence: 90, // High confidence from LinkedIn
              verified: false // LinkedIn doesn't provide emails directly
            })
          }
        }
      }
      
      return contacts.slice(0, 10) // Limit to top 10
    } catch (error) {
      console.error('[CONTACT_SCRAPER] LinkedIn error:', error)
      return []
    }
  }
  
  /**
   * METHOD 3: Company Website Scraping (Puppeteer + Cheerio)
   */
  private async scrapeCompanyWebsite(websiteUrl: string): Promise<Contact[]> {
    console.log('[CONTACT_SCRAPER] Scraping company website:', websiteUrl)
    
    // Try Cheerio first (faster)
    try {
      const cheerioContacts = await this.scrapeWithCheerio(websiteUrl)
      if (cheerioContacts.length > 0) {
        return cheerioContacts
      }
    } catch (error) {
      console.error('[CONTACT_SCRAPER] Cheerio failed:', error)
    }
    
    // Fallback to Puppeteer (slower but handles JS)
    try {
      const puppeteerContacts = await this.scrapeWithPuppeteer(websiteUrl)
      return puppeteerContacts
    } catch (error) {
      console.error('[CONTACT_SCRAPER] Puppeteer failed:', error)
      return []
    }
  }
  
  /**
   * Scrape with Cheerio (fast, static sites)
   */
  private async scrapeWithCheerio(url: string): Promise<Contact[]> {
    const contacts: Contact[] = []
    
    // Common pages to check
    const pagesToCheck = [
      url,
      `${url}/about`,
      `${url}/team`,
      `${url}/contact`,
      `${url}/about-us`,
      `${url}/leadership`
    ]
    
    for (const pageUrl of pagesToCheck) {
      try {
        const response = await fetch(pageUrl, {
          headers: {
            'User-Agent': this.USER_AGENTS[Math.floor(Math.random() * this.USER_AGENTS.length)]
          }
        })
        
        if (!response.ok) continue
        
        const html = await response.text()
        const $ = cheerio.load(html)
        
        // Extract emails
        const emailRegex = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/g
        const emails = html.match(emailRegex) || []
        
        // Extract names near emails
        for (const email of emails) {
          // Skip generic emails
          if (email.includes('info@') || email.includes('support@') || email.includes('sales@')) {
            continue
          }
          
          // Try to find name near email
          const emailElement = $(`a[href="mailto:${email}"]`)
          let name = emailElement.text().trim()
          
          if (!name) {
            // Try to find name in parent or sibling elements
            name = emailElement.parent().text().trim() || 
                   emailElement.prev().text().trim() ||
                   'Unknown'
          }
          
          contacts.push({
            name: name,
            email: email,
            source: 'company-website',
            confidence: 70,
            verified: false
          })
        }
        
        // Extract from team/about pages
        $('.team-member, .staff-member, .employee').each((_, element) => {
          const $el = $(element)
          const name = $el.find('.name, .member-name, h3, h4').first().text().trim()
          const title = $el.find('.title, .position, .role').first().text().trim()
          const email = $el.find('a[href^="mailto:"]').attr('href')?.replace('mailto:', '')
          
          if (name) {
            contacts.push({
              name,
              title: title || undefined,
              email: email || undefined,
              source: 'company-website',
              confidence: 75,
              verified: !!email
            })
          }
        })
        
      } catch (error) {
        console.error(`[CONTACT_SCRAPER] Error scraping ${pageUrl}:`, error)
      }
    }
    
    return contacts
  }
  
  /**
   * Scrape with Puppeteer (slow, handles JS)
   */
  private async scrapeWithPuppeteer(url: string): Promise<Contact[]> {
    let browser
    const contacts: Contact[] = []
    
    try {
      browser = await puppeteer.launch({
        args: chromium.args,
        defaultViewport: { width: 1920, height: 1080 },
        executablePath: await chromium.executablePath(),
        headless: true
      })
      
      const page = await browser.newPage()
      await page.setUserAgent(this.USER_AGENTS[0])
      
      // Try contact/about pages
      const pagesToCheck = [
        `${url}/contact`,
        `${url}/about`,
        `${url}/team`
      ]
      
      for (const pageUrl of pagesToCheck) {
        try {
          await page.goto(pageUrl, { waitUntil: 'networkidle2', timeout: 10000 })
          
          // Extract all text
          const pageText = await page.evaluate(() => document.body.innerText)
          
          // Extract emails
          const emailRegex = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/g
          const emails = pageText.match(emailRegex) || []
          
          for (const email of emails) {
            if (!email.includes('info@') && !email.includes('support@')) {
              contacts.push({
                name: 'Unknown',
                email,
                source: 'company-website',
                confidence: 60,
                verified: false
              })
            }
          }
        } catch (error) {
          console.error(`[CONTACT_SCRAPER] Puppeteer error on ${pageUrl}:`, error)
        }
      }
      
    } catch (error) {
      console.error('[CONTACT_SCRAPER] Puppeteer launch error:', error)
    } finally {
      if (browser) {
        await browser.close()
      }
    }
    
    return contacts
  }
  
  /**
   * Deduplicate contacts by email/name
   */
  private deduplicateContacts(contacts: Contact[]): Contact[] {
    const seen = new Set<string>()
    const unique: Contact[] = []
    
    for (const contact of contacts) {
      const key = contact.email || contact.name.toLowerCase()
      
      if (!seen.has(key)) {
        seen.add(key)
        unique.push(contact)
      } else {
        // If duplicate, keep the one with higher confidence
        const existingIndex = unique.findIndex(c => 
          (c.email && c.email === contact.email) || 
          c.name.toLowerCase() === contact.name.toLowerCase()
        )
        
        if (existingIndex >= 0 && contact.confidence > unique[existingIndex].confidence) {
          unique[existingIndex] = contact
        }
      }
    }
    
    return unique
  }
  
  /**
   * Validate email address
   */
  validateEmail(email: string): boolean {
    const regex = /^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$/
    return regex.test(email)
  }
  
  /**
   * Enrich contact with additional data (e.g., guess email from name)
   */
  enrichContact(contact: Contact, companyDomain: string): Contact {
    if (!contact.email && contact.name && companyDomain) {
      // Try common email patterns
      const nameParts = contact.name.toLowerCase().split(' ')
      const firstName = nameParts[0]
      const lastName = nameParts[nameParts.length - 1]
      
      const patterns = [
        `${firstName}.${lastName}@${companyDomain}`,
        `${firstName}${lastName}@${companyDomain}`,
        `${firstName[0]}${lastName}@${companyDomain}`,
        `${firstName}@${companyDomain}`
      ]
      
      // Return first valid pattern (not verified)
      for (const pattern of patterns) {
        if (this.validateEmail(pattern)) {
          return {
            ...contact,
            email: pattern,
            confidence: Math.max(30, contact.confidence - 20), // Lower confidence for guessed emails
            verified: false
          }
        }
      }
    }
    
    return contact
  }
}

export const contactScraper = new ContactScraper()
</file>

<file path="src/types/supabase.ts">
/**
 * Supabase Database Types
 * Auto-generated from your Supabase schema
 */

export interface Job {
  id: string
  title: string
  company: string
  location: string
  description?: string
  description_html?: string
  salary_min?: number
  salary_max?: number
  salary_type?: string
  salary_currency?: string
  job_type?: string
  experience_level?: string
  remote_type?: string
  url: string
  external_id?: string
  source: 'active-jobs-db' | 'google-jobs' | 'jsearch' | 'adzuna' | 'indeed' | 'linkedin'
  apply_link?: string
  company_size?: string
  company_industry?: string
  city?: string
  state?: string
  country?: string
  keywords?: string[]
  posted_date?: string
  scraped_at?: string
  expires_at?: string
  raw_data?: any
  created_at?: string
  updated_at?: string
}

export interface Company {
  id: string
  name: string
  normalized_name?: string
  website?: string
  description?: string
  employee_count?: string
  founded_year?: number
  linkedin_url?: string
  linkedin_slug?: string
  linkedin_followers?: number
  linkedin_industry?: string
  linkedin_specialties?: string[]
  glassdoor_url?: string
  glassdoor_rating?: number
  glassdoor_review_count?: number
  headquarters_city?: string
  headquarters_state?: string
  headquarters_country?: string
  raw_data?: any
  created_at?: string
  updated_at?: string
}

export interface SalaryData {
  id: string
  job_title: string
  company?: string
  location?: string
  location_type?: string
  years_of_experience?: string
  min_salary?: number
  max_salary?: number
  median_salary?: number
  min_base_salary?: number
  max_base_salary?: number
  median_base_salary?: number
  min_additional_pay?: number
  max_additional_pay?: number
  median_additional_pay?: number
  salary_period?: string
  salary_currency?: string
  confidence?: string
  salary_count?: number
  created_at?: string
  expires_at?: string
}

export interface DownloadHistory {
  id: string
  source: string
  search_query?: string
  location?: string
  jobs_downloaded: number
  unique_jobs: number
  duplicates_found?: number
  duration_seconds?: number
  success: boolean
  error_message?: string
  started_at?: string
  completed_at?: string
}

// Search parameters
export interface JobSearchParams {
  query?: string
  location?: string
  source?: string[]
  job_type?: string
  remote_type?: string
  salary_min?: number
  limit?: number
  offset?: number
}

// Search results
export interface JobSearchResult {
  jobs: Job[]
  total: number
  page: number
  limit: number
}
</file>

<file path="download-incremental.ts">
/**
 * Incremental Download: One location at a time
 * Safer approach with smaller batches
 */

import { config } from 'dotenv'
config({ path: '.env.local' })

import { AdzunaAPIClient } from './src/lib/adzuna-api-client'
import { createClient } from '@supabase/supabase-js'

const adzuna = new AdzunaAPIClient()

// Create Supabase client AFTER dotenv loads
const supabaseAdmin = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
)

async function sleep(ms: number) {
  return new Promise(resolve => setTimeout(resolve, ms))
}

async function downloadLocationJobs(location: string) {
  console.log(`\n📍 Downloading jobs for ${location}...`)
  
  const jobs: any[] = []
  const seenIds = new Set<string>()
  let totalScraped = 0
  
  // Scrape up to 20 pages (1,000 jobs per location)
  for (let page = 1; page <= 20; page++) {
    try {
      const result = await adzuna.searchJobs({
        what: '',
        where: location,
        country: 'ca',
        resultsPerPage: 50,
        page,
        sortBy: 'date'
      })
      
      // Validate and deduplicate
      const validJobs = result.results.filter((j: any) => {
        if (seenIds.has(j.id)) return false
        
        const hasCompany = j.company?.display_name?.trim()
        const hasDescription = j.description?.trim()
        const hasTitle = j.title?.trim()
        const hasUrl = j.redirect_url?.trim()
        
        if (!hasCompany || !hasDescription || !hasTitle || !hasUrl) {
          return false
        }
        
        seenIds.add(j.id)
        return true
      })
      
      jobs.push(...validJobs.map((j: any) => ({
        title: j.title,
        company: j.company.display_name,
        location: j.location.display_name,
        description: j.description,
        url: j.redirect_url,
        external_id: `adzuna_${j.id}`,
        source: 'adzuna',
        salary_min: j.salary_min || null,
        salary_max: j.salary_max || null,
        salary_type: 'yearly',
        salary_currency: 'CAD',
        job_type: j.contract_time || null,
        remote_type: 'on-site',
        apply_link: j.redirect_url,
        city: location.split(',')[0].trim(),
        state: 'AB',
        country: 'Canada',
        posted_date: j.created,
        scraped_at: new Date().toISOString(),
        expires_at: new Date(Date.now() + 14 * 24 * 60 * 60 * 1000).toISOString(),
        keywords: []
      })))
      
      totalScraped += validJobs.length
      console.log(`  Page ${page}: ${validJobs.length} valid jobs (Total: ${totalScraped})`)
      
      if (result.results.length === 0) {
        console.log(`  No more results, stopping at page ${page}`)
        break
      }
      
      await sleep(500)
      
    } catch (error: any) {
      console.error(`  Page ${page} error:`, error.message)
      break
    }
  }
  
  console.log(`\n✅ Scraped ${jobs.length} jobs for ${location}`)
  
  // Insert in small batches of 50
  if (jobs.length > 0) {
    console.log(`📥 Inserting ${jobs.length} jobs in batches of 50...`)
    
    let inserted = 0
    let errors = 0
    const batchSize = 50
    
    for (let i = 0; i < jobs.length; i += batchSize) {
      const batch = jobs.slice(i, i + batchSize)
      const batchNum = Math.floor(i / batchSize) + 1
      
      try {
        const { data, error} = await supabaseAdmin
          .from('jobs')
          .insert(batch)
          .select('id')
        
        if (error) {
          console.error(`  Batch ${batchNum} error:`, error.message)
          errors += batch.length
        } else {
          inserted += data?.length || 0
          console.log(`  Batch ${batchNum}: ✅ ${data?.length || 0} jobs`)
        }
        
        await sleep(1000) // Wait 1s between batches
        
      } catch (error: any) {
        console.error(`  Batch ${batchNum} exception:`, error.message)
        errors += batch.length
      }
    }
    
    console.log(`\n✅ Inserted ${inserted} jobs, ${errors} errors`)
    return { inserted, errors }
  }
  
  return { inserted: 0, errors: 0 }
}

async function main() {
  console.log('🚀 INCREMENTAL DOWNLOAD: Edmonton Area')
  console.log('Strategy: One location at a time, 50 jobs per batch\n')
  
  const locations = [
    'Edmonton, AB',
    'St. Albert, AB',
    'Sherwood Park, AB',
    'Spruce Grove, AB',
    'Leduc, AB',
    'Fort Saskatchewan, AB',
    'Stony Plain, AB',
    'Beaumont, AB'
  ]
  
  let totalInserted = 0
  let totalErrors = 0
  
  for (const location of locations) {
    const { inserted, errors } = await downloadLocationJobs(location)
    totalInserted += inserted
    totalErrors += errors
    
    console.log(`\n📊 Running Total: ${totalInserted} inserted, ${totalErrors} errors`)
    
    // Wait 2 seconds between locations
    await sleep(2000)
  }
  
  console.log('\n\n🎉 COMPLETE!')
  console.log(`Total Inserted: ${totalInserted}`)
  console.log(`Total Errors: ${totalErrors}`)
}

main().catch(console.error)
</file>

<file path="src/app/api/jobs/search/route.ts">
/**
 * Unified Job Search API - Enhanced with JobAggregator
 * 
 * NOW USES: JobAggregator with Supabase-first search strategy
 * 
 * Search Order:
 * 1. Redis Cache (instant)
 * 2. MongoDB Cache (fast)
 * 3. Supabase (1,249 jobs, <100ms)
 * 4. Cheerio/Puppeteer scrapers (if < 10 jobs, TOP 3 keywords)
 * 5. Perplexity (last resort, if still < 10 jobs)
 */

import { NextRequest, NextResponse } from 'next/server'
import { getServerSession } from 'next-auth/next'
import { authOptions } from '@/lib/auth'
import { dbService } from '@/lib/database'
import { JobAggregator } from '@/lib/job-aggregator'
import { PerplexityIntelligenceService } from '@/lib/perplexity-intelligence'
import { isRateLimited } from '@/lib/rate-limit'
import Resume from '@/models/Resume'
import { jobSearchCacheService } from '@/services/job-search-cache.service'
import { validateJob } from '@/lib/validators/job-validator'
import { DataSanitizer } from '@/lib/validators/data-sanitizer'
import { deduplicateJobs } from '@/lib/job-deduplication'

export const dynamic = 'force-dynamic'
export const runtime = 'nodejs'
export const maxDuration = 60 // Increased to handle Perplexity API calls which can take longer

interface JobSearchRequest {
  keywords: string
  location?: string
  sources?: string[] // Specific boards to search
  limit?: number
  remote?: boolean
  salaryMin?: number
  experienceLevel?: 'entry' | 'mid' | 'senior' | 'executive'
  workType?: 'remote' | 'hybrid' | 'onsite' | 'any'
  useResumeMatching?: boolean // Use resume for skill matching
  targetIndustry?: string // ENTERPRISE: User wants to switch industries (e.g., "Technology", "Healthcare")
  disableIndustryWeighting?: boolean // ENTERPRISE: User wants equal weight across all industries
}

export async function POST(request: NextRequest) {
  try {
    // CRITICAL FIX: Parse body and validate location BEFORE authentication
    // This allows testing location validation without auth
    const body: JobSearchRequest = await request.json()
    let { 
      keywords, 
      location, 
      sources, 
      limit = 25, 
      remote,
      salaryMin,
      experienceLevel,
      workType,
      targetIndustry,
      disableIndustryWeighting
    } = body
    
    console.log('═══════════════════════════════════════════════════════')
    console.log('[JOB_SEARCH] NEW SEARCH REQUEST')
    console.log('═══════════════════════════════════════════════════════')
    console.log('[JOB_SEARCH] Job Title:', keywords)
    console.log('[JOB_SEARCH] Location:', location || 'UNDEFINED')
    console.log('[JOB_SEARCH] Max Results:', limit)
    console.log('[JOB_SEARCH] Work Type:', workType || 'any')
    console.log('─────────────────────────────────────────────────────────')

    // CRITICAL: Validate location BEFORE authentication check
    if (!location || location.trim().length < 2) {
      console.error('[JOB_SEARCH] ❌ MISSING LOCATION')
      return NextResponse.json({
        success: false,
        error: 'Location is required for job search',
        suggestion: 'Upload your resume to extract location, or manually enter city and state/province',
        errorCode: 'LOCATION_REQUIRED'
      }, { status: 400 })
    }

    // Reject "Canada" or "United States" (too broad)
    const normalizedLocation = location.toLowerCase().trim()
    if (['canada', 'united states', 'usa', 'us'].includes(normalizedLocation)) {
      console.error('[JOB_SEARCH] ❌ LOCATION TOO BROAD:', location)
      return NextResponse.json({
        success: false,
        error: 'Location is too broad. Please specify a city and state/province.',
        example: 'Examples: Seattle, WA or Toronto, ON or Vancouver, BC',
        errorCode: 'LOCATION_TOO_BROAD'
      }, { status: 400 })
    }

    console.log('[JOB_SEARCH] ✅ Location valid, proceeding with authentication...')

    // NOW check authentication after location validation passes
    const session = await getServerSession(authOptions)
    if (!session?.user?.id) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    }

    // Rate limiting
    if (await isRateLimited(session.user.id, 'job-search')) {
      return NextResponse.json({ 
        error: 'Too many searches. Please wait a moment.' 
      }, { status: 429 })
    }

    await dbService.connect()

    let useResumeMatching = body.useResumeMatching || false

    if (!keywords || keywords.trim().length < 2) {
      return NextResponse.json({ 
        error: 'Please provide valid search keywords' 
      }, { status: 400 })
    }

    console.log(`[JOB_SEARCH] User ${session.user.id} searching: "${keywords}" in ${location} (Resume matching: ${useResumeMatching})`)

    // CRITICAL FIX: Get cached jobs but ALWAYS search for new ones too
    const cachedJobs = await jobSearchCacheService.getCachedJobs({
      keywords,
      location,
      workType,
      experienceLevel,
      userId: session.user.id
    });

    if (cachedJobs && cachedJobs.length > 0) {
      console.log(`[JOB_CACHE] Found ${cachedJobs.length} cached jobs - will merge with NEW search results`);
    } else {
      console.log(`[JOB_CACHE] No cached jobs found - performing fresh search`);
    }

    let result: any
    let jobs: any[] = []
    let metadata: any = {}

    // Option 1: Resume-matched search with INDUSTRY WEIGHTING (most powerful)
    if (useResumeMatching) {
      try {
        // Get user's resume
        const resumeDoc = await Resume.findOne({ userId: session.user.id })
          .sort({ createdAt: -1 })
          .lean()
        
        const extractedText = (resumeDoc as any)?.extractedText
        
        if (!resumeDoc || !extractedText) {
          return NextResponse.json({ 
            error: 'Please upload a resume first to use resume matching' 
          }, { status: 400 })
        }

        console.log(`[JOB_SEARCH] Using resume matching with industry weighting for user ${session.user.id}`)

        // ENTERPRISE FEATURE: Analyze career timeline for industry weighting
        let careerTimeline: any = null
        let effectivePrimaryIndustry: any = null
        
        // Skip industry analysis if user explicitly disabled it
        if (!disableIndustryWeighting) {
          try {
            careerTimeline = await PerplexityIntelligenceService.extractCareerTimeline(extractedText)
            console.log('[JOB_SEARCH] Career timeline:', {
              industries: careerTimeline.industries.map((i: any) => `${i.name} (${i.percentage}%)`).join(', '),
              primaryIndustry: careerTimeline.industries[0]?.name,
              hasTransition: !!careerTimeline.careerTransition,
              userTargetIndustry: targetIndustry || 'none'
            })
            
            // ENTERPRISE: User wants to switch industries
            if (targetIndustry && targetIndustry.trim()) {
              // Find matching industry from resume, or create synthetic one
              const normalizedTarget = targetIndustry.toLowerCase()
              effectivePrimaryIndustry = careerTimeline.industries.find(
                (i: any) => i?.name?.toLowerCase()?.includes(normalizedTarget)
              )

              if (effectivePrimaryIndustry) {
                console.log(`[JOB_SEARCH] User targeting industry switch TO: ${effectivePrimaryIndustry.name}`)
              } else {
                // User wants to switch to an entirely new industry not in their history
                console.log(`[JOB_SEARCH] User switching to NEW industry: ${targetIndustry} (no prior experience)`)
                effectivePrimaryIndustry = {
                  name: targetIndustry,
                  yearsOfExperience: 0,
                  keywords: keywords
                    .split(',')
                    .map((k: string) => k.trim())
                    .filter(Boolean),
                  percentage: 100 // Give full weight to target industry
                }
              }
            } else {
              // Default: Use longest-tenure industry
              effectivePrimaryIndustry = careerTimeline.industries[0]
            }
          } catch (err) {
            console.warn('[JOB_SEARCH] Career timeline extraction failed, using standard matching:', err)
          }
        } else {
          console.log('[JOB_SEARCH] Industry weighting DISABLED by user preference')
        }

        // CRITICAL: If career timeline exists, weight job results by industry tenure
        let industryWeightedLimit = limit
        
        if (effectivePrimaryIndustry) {
          // Calculate industry-based search distribution
          const primaryPercentage = effectivePrimaryIndustry.percentage / 100
          
          // EXAMPLE: If 95% of career in Transportation, show 95% transport jobs
          // UNLESS user is switching industries, then show 100% of new industry
          industryWeightedLimit = targetIndustry ? limit : Math.ceil(limit * primaryPercentage)
          
          console.log('[JOB_SEARCH] Industry weighting:', {
            primaryIndustry: effectivePrimaryIndustry.name,
            primaryPercentage: `${effectivePrimaryIndustry.percentage}%`,
            adjustedLimit: industryWeightedLimit,
            keywords: effectivePrimaryIndustry.keywords?.join(', ') || 'none',
            isSwitching: !!targetIndustry
          })
          
          // Boost keywords from target/primary industry (if available)
          if (effectivePrimaryIndustry.keywords && Array.isArray(effectivePrimaryIndustry.keywords) && effectivePrimaryIndustry.keywords.length > 0) {
            const industryKeywords = effectivePrimaryIndustry.keywords.slice(0, 5).join(', ')
            keywords = `${industryKeywords}, ${keywords}`.trim()
          }
        }

        // Use JobAggregator (Supabase → Scrapers → Perplexity fallback)
        console.log('[JOB_SEARCH] Using JobAggregator (Supabase first):', {
          keywords: Array.isArray(keywords) ? keywords : [keywords],
          location,
          workType: workType || 'any',
          maxResults: limit
        })
        
        const aggregator = JobAggregator.getInstance()
        const aggregatorResult = await aggregator.searchJobs({
          keywords: Array.isArray(keywords) ? keywords : keywords.split(',').map((k: string) => k.trim()),
          location,
          workType: workType || 'any',
          maxResults: limit
        })

        console.log('[JOB_SEARCH] 🎯 JobAggregator result:', {
          jobsFound: aggregatorResult.jobs.length,
          source: aggregatorResult.source,
          cached: aggregatorResult.cached
        })

        // Convert JobListing format to expected format with validation
        jobs = aggregatorResult.jobs
          .filter(job => {
            // Validate required fields
            if (!job.title || !job.company || !job.url) {
              console.warn('[JOB_SEARCH] ⚠️ Skipping job with missing fields')
              return false
            }
            return true
          })
          .map(job => ({
            title: job.title,
            company: job.company,
            location: job.location || 'Location not specified',
            url: job.url,
            description: job.description || '',
            summary: job.description?.substring(0, 200) || 'No description available',
            salary: job.salary || null,
            postedDate: job.postedDate?.toISOString().split('T')[0] || new Date().toISOString().split('T')[0],
            source: job.source || 'unknown',
            skillMatchPercent: job.skillMatchScore || 0,
            skills: job.skills || [],
            workType: job.workType || 'onsite'
          }))

        result = {
          success: true,
          data: jobs,
          cached: aggregatorResult.cached,
          metadata: {
            source: aggregatorResult.source,
            timestamp: aggregatorResult.timestamp
          }
        }
        
        // POST-PROCESSING: Re-rank jobs by industry tenure (respects user preferences)
        if (effectivePrimaryIndustry && !disableIndustryWeighting && effectivePrimaryIndustry.keywords && Array.isArray(effectivePrimaryIndustry.keywords)) {
          const primaryKeywords = effectivePrimaryIndustry.keywords.map((k: string) => k.toLowerCase())
          
          jobs = jobs.map((job: any) => {
            // Calculate industry match score
            const jobTitle = (job.title || '').toLowerCase()
            const jobDescription = (job.description || '').toLowerCase()
            const jobCompany = (job.company || '').toLowerCase()
            const fullText = `${jobTitle} ${jobDescription} ${jobCompany}`
            
            let industryMatchCount = 0
            primaryKeywords.forEach((keyword: string) => {
              if (fullText.includes(keyword)) industryMatchCount++
            })
            
            const industryMatchScore = industryMatchCount / primaryKeywords.length
            
            // Boost jobs from primary/target industry
            const originalScore = job.skillMatchScore || 0.5
            // If user is switching industries, give HIGHER boost (up to 75%)
            const boostMultiplier = targetIndustry ? 0.75 : 0.5
            const boostedScore = originalScore * (1 + industryMatchScore * boostMultiplier)
            
            return {
              ...job,
              skillMatchScore: Math.min(boostedScore, 1.0), // Cap at 1.0
              industryMatchScore,
              primaryIndustry: effectivePrimaryIndustry.name,
              isSwitchingIndustries: !!targetIndustry
            }
          }).sort((a: any, b: any) => (b.skillMatchScore || 0) - (a.skillMatchScore || 0)) // Re-sort by boosted score
          
          const matchedJobs = jobs.filter((j: any) => j.industryMatchScore > 0.3).length
          console.log(`[JOB_SEARCH] Applied industry weighting boost to ${jobs.length} jobs (${matchedJobs} strong matches)`)
        }
        
        metadata = {
          ...result.metadata,
          useResumeMatching: true,
          skillMatchingEnabled: true,
          industryWeighting: effectivePrimaryIndustry ? {
            primaryIndustry: effectivePrimaryIndustry.name,
            primaryPercentage: effectivePrimaryIndustry.percentage,
            careerTransition: careerTimeline?.careerTransition,
            userTargetIndustry: targetIndustry || null,
            disabledByUser: disableIndustryWeighting || false
          } : null
        }

        console.log(`[JOB_SEARCH] Resume matching found ${jobs.length} jobs with skill scores and industry weighting`)

      } catch (error) {
        console.error('[JOB_SEARCH] Resume matching failed, falling back to standard search:', error)
        // Fall back to standard search
        useResumeMatching = false
      }
    }

    // Option 2: Standard job listing search (JobAggregator with Supabase)
    if (!useResumeMatching || jobs.length === 0) {
      console.log(`[JOB_SEARCH] Using JobAggregator for standard search (Supabase first)`, {
        keywords,
        location,
        limit,
        workType: workType || (remote ? 'remote' : undefined)
      })

      const aggregator = JobAggregator.getInstance()
      const aggregatorResult = await aggregator.searchJobs({
        keywords: Array.isArray(keywords) ? keywords : keywords.split(',').map((k: string) => k.trim()),
        location,
        workType: workType || 'any',
        maxResults: limit
      })

      console.log(`[JOB_SEARCH] JobAggregator returned:`, {
        source: aggregatorResult.source,
        cached: aggregatorResult.cached,
        jobCount: aggregatorResult.jobs.length,
        sample: aggregatorResult.jobs[0] ? {
          title: aggregatorResult.jobs[0].title,
          company: aggregatorResult.jobs[0].company,
          hasUrl: !!aggregatorResult.jobs[0].url
        } : null
      })

      // Convert JobListing format to expected format with validation
      const jobsResult = aggregatorResult.jobs
        .filter(job => {
          // Validate required fields
          if (!job.title || !job.company || !job.url) {
            console.warn('[JOB_SEARCH] ⚠️ Skipping job with missing fields')
            return false
          }
          return true
        })
        .map(job => ({
          title: job.title,
          company: job.company,
          location: job.location || 'Location not specified',
          url: job.url,
          description: job.description || '',
          summary: job.description?.substring(0, 200) || 'No description available',
          salary: job.salary || null,
          postedDate: job.postedDate?.toISOString().split('T')[0] || new Date().toISOString().split('T')[0],
          source: job.source || 'unknown',
          skillMatchPercent: job.skillMatchScore || 0,
          skills: job.skills || [],
          workType: job.workType || 'onsite'
        }))

      jobs = Array.isArray(jobsResult) ? jobsResult : []
      console.log(`[JOB_SEARCH] Standard search returned type: ${typeof jobsResult}, isArray: ${Array.isArray(jobsResult)}, length: ${jobs.length}`)

      metadata = {
        useResumeMatching: false,
        searchedBoards: sources?.length || 15,
        canadianPriority: location.toLowerCase().includes('canada')
      }

      console.log(`[JOB_SEARCH] Standard search found ${jobs.length} jobs`)
      if (jobs.length > 0) {
        console.log(`[JOB_SEARCH] First job sample:`, JSON.stringify(jobs[0]).substring(0, 200))
      }
    }

    // Save search history
    try {
      const { default: SearchHistory } = await import('@/models/SearchHistory')
      await SearchHistory.create({
        userId: session.user.id,
        keywords,
        location,
        resultsCount: jobs.length,
        sources: sources || ['all'],
        aiUsed: useResumeMatching,
        searchDate: new Date()
      })
    } catch (error) {
      console.error('[JOB_SEARCH] Failed to save search history:', error)
      // Non-critical, continue
    }

    // IMPROVED: Mark confidential jobs instead of filtering them out
    let processedJobs = jobs.map((job: any) => {
      const company = (job.company || '').toLowerCase().trim()
      const title = (job.title || '').toLowerCase().trim()
      
      // Only filter out COMPLETELY invalid jobs (empty title/company)
      const isCompletelyInvalid = (company === '' && title === '')
      
      // Mark confidential companies but keep them
      const confidentialCompanies = ['confidential', 'confidential company', 'undisclosed', 'private']
      const isConfidential = confidentialCompanies.includes(company)
      
      return {
        ...job,
        isConfidential,
        isCompletelyInvalid,
        note: isConfidential ? 'Company name not disclosed in posting' : undefined
      }
    }).filter((job: any) => !job.isCompletelyInvalid) // Only filter completely invalid

    // 🚫 CRITICAL: REMOVE ALL CONFIDENTIAL JOBS - DO NOT SHOW THEM AT ALL
    const confidentialCount = processedJobs.filter((j: any) => j.isConfidential).length
    processedJobs = processedJobs.filter((j: any) => {
      const isConfidential = j.isConfidential || 
        j.title?.toLowerCase().includes('confidential') ||
        j.company?.toLowerCase().includes('confidential') ||
        j.company?.toLowerCase() === 'confidential'
      
      if (isConfidential) {
        console.log(`[JOB_SEARCH] 🚫 REJECTED CONFIDENTIAL JOB: "${j.title}" at "${j.company}"`)
      }
      
      return !isConfidential
    })
    
    console.log(`[JOB_SEARCH] Processed ${jobs.length} jobs, REJECTED ${confidentialCount} confidential jobs, ${processedJobs.length} valid jobs kept`)

    // CRITICAL FIX: Merge cached jobs with new results (remove duplicates by URL)
    let finalJobs = [...processedJobs]
    if (cachedJobs && cachedJobs.length > 0) {
      const newJobUrls = new Set(processedJobs.map((j: any) => j.url).filter(Boolean))
      // Also filter confidential from cached jobs
      const uniqueCachedJobs = cachedJobs.filter((cj: any) => {
        const isConfidential = cj.isConfidential || 
          cj.title?.toLowerCase().includes('confidential') ||
          cj.company?.toLowerCase().includes('confidential') ||
          cj.company?.toLowerCase() === 'confidential'
        return !newJobUrls.has(cj.url) && !isConfidential
      })
      finalJobs = [...processedJobs, ...uniqueCachedJobs]
      console.log(`[JOB_CACHE] Merged ${uniqueCachedJobs.length} unique cached jobs with ${processedJobs.length} new jobs = ${finalJobs.length} total`)
    }

    // 🚀 NEW: Cache the search results for 3 weeks
    if (processedJobs.length > 0) {
      await jobSearchCacheService.cacheSearchResults(
        {
          keywords,
          location,
          workType,
          experienceLevel,
          userId: session.user.id
        },
        processedJobs
      );
      console.log(`[JOB_CACHE] ✅ Cached ${processedJobs.length} jobs for future searches`);
    }

    // ✅ FIX #4: VALIDATE each job
    console.log(`[JOB_SEARCH] Validating ${finalJobs.length} jobs...`)
    console.log('[JOB_SEARCH] Sample job data:', finalJobs[0] ? {
      title: finalJobs[0].title,
      company: finalJobs[0].company,
      descLength: finalJobs[0].description?.length || 0,
      url: finalJobs[0].url
    } : 'No jobs to sample')
    
    const validatedJobs = finalJobs.filter((job: any) => {
      const validation = validateJob(job)
      if (!validation.valid) {
        console.log('[VALIDATOR] ❌ Rejected:', job.title, 'at', job.company)
        if (validation.issues && validation.issues.length > 0) {
          console.log('[VALIDATOR] Reasons:', validation.issues.join(', '))
        }
        console.log('[VALIDATOR] Job data:', {
          descLength: job.description?.length || 0,
          url: job.url,
          location: job.location
        })
        return false
      }
      return true
    })
    console.log(`[JOB_SEARCH] Validation: ${finalJobs.length} → ${validatedJobs.length}`)

    // ✅ FIX #5: DEDUPLICATE jobs
    const uniqueJobs = deduplicateJobs(validatedJobs)
    console.log(`[JOB_SEARCH] Deduplication: ${validatedJobs.length} → ${uniqueJobs.length}`)

    // ✅ FIX #4: SANITIZE output
    const sanitizedJobs = uniqueJobs.map((job: any) => DataSanitizer.sanitizeJobData(job))
    console.log(`[JOB_SEARCH] ✅ Final jobs: ${sanitizedJobs.length}`)

    // Get recommended boards for this location
    const recommendedBoards = PerplexityIntelligenceService.getRecommendedBoards(location)

    return NextResponse.json({
      success: true,
      query: { keywords, location, sources },
      totalResults: sanitizedJobs.length,
      returnedResults: Math.min(sanitizedJobs.length, limit),
      jobs: sanitizedJobs.slice(0, limit),
      metadata: {
        ...metadata,
        searchedAt: new Date().toISOString(),
        cachedResults: cachedJobs ? cachedJobs.length : 0,
        newResults: processedJobs.length,
        totalMerged: finalJobs.length,
        validated: validatedJobs.length,
        unique: uniqueJobs.length,
        final: sanitizedJobs.length
      },
      recommendations: {
        priorityBoards: recommendedBoards.slice(0, 5),
        reasoning: `Recommended job boards for ${location || 'your location'}`
      },
      sources: [...new Set(sanitizedJobs.map((j: any) => j.source || 'Unknown'))]
    })

  } catch (error: any) {
    console.error('❌❌❌ [JOB_SEARCH] CRITICAL ERROR ❌❌❌')
    console.error('[JOB_SEARCH] Error type:', error?.constructor?.name)
    console.error('[JOB_SEARCH] Error message:', error?.message)
    console.error('[JOB_SEARCH] Error stack:', error?.stack)
    
    // Get session for error logging
    const session = await getServerSession(authOptions)
    console.error('[JOB_SEARCH] User ID:', session?.user?.id)
    
    return NextResponse.json({ 
      error: 'Job search failed', 
      details: error?.message || 'Unknown error',
      errorType: error?.constructor?.name,
      timestamp: new Date().toISOString()
    }, { status: 500 })
  }
}

/**
 * GET endpoint for search history and available job boards
 */
export async function GET(request: NextRequest) {
  try {
    const session = await getServerSession(authOptions)
    if (!session?.user?.id) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    }

    await dbService.connect()

    const url = new URL(request.url)
    const action = url.searchParams.get('action')

    // Get available job boards
    if (action === 'boards') {
      const boards = PerplexityIntelligenceService.getAvailableJobBoards()
      return NextResponse.json({
        success: true,
        boards,
        totalBoards: boards.length
      })
    }

    // Get search history (default)
    const { default: SearchHistory } = await import('@/models/SearchHistory')
    const history = await SearchHistory.find({ userId: session.user.id })
      .sort({ searchDate: -1 })
      .limit(20)

    return NextResponse.json({
      success: true,
      history
    })

  } catch (error) {
    console.error('[JOB_SEARCH] Failed to fetch data:', error)
    return NextResponse.json({ 
      error: 'Failed to fetch data' 
    }, { status: 500 })
  }
}
</file>

<file path="src/lib/supabase.ts">
/**
 * Supabase Client Configuration
 * Replaces MongoDB for job caching
 */

import { createClient } from '@supabase/supabase-js'
import type { Job, Company, SalaryData, DownloadHistory, JobSearchParams, JobSearchResult } from '@/types/supabase'

// Lazy initialization to avoid build-time errors
let _supabase: ReturnType<typeof createClient> | null = null
let _supabaseAdmin: ReturnType<typeof createClient> | null = null

function getSupabaseClient() {
  if (!_supabase) {
    const url = process.env.NEXT_PUBLIC_SUPABASE_URL
    const key = process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY
    
    if (!url || !key) {
      // During build, return a dummy client that won't be used
      if (typeof window === 'undefined' && !process.env.SUPABASE_SERVICE_ROLE_KEY) {
        console.warn('[Supabase] Environment variables not set during build - using dummy client')
        return createClient('https://dummy.supabase.co', 'dummy-key')
      }
      throw new Error('Missing Supabase environment variables')
    }
    
    _supabase = createClient(url, key)
  }
  return _supabase
}

function getSupabaseAdminClient() {
  if (!_supabaseAdmin) {
    const url = process.env.NEXT_PUBLIC_SUPABASE_URL
    const key = process.env.SUPABASE_SERVICE_ROLE_KEY
    
    if (!url || !key) {
      // During build, return a dummy client that won't be used
      if (typeof window === 'undefined' && !key) {
        console.warn('[Supabase] Service key not set during build - using dummy client')
        return createClient('https://dummy.supabase.co', 'dummy-key')
      }
      throw new Error('Missing Supabase environment variables')
    }
    
    _supabaseAdmin = createClient(url, key)
  }
  return _supabaseAdmin
}

// Public client (for client-side operations)
export const supabase = getSupabaseClient()

// Service client (for server-side operations with elevated permissions)
export const supabaseAdmin = getSupabaseAdminClient()

/**
 * Insert jobs in bulk (upsert to handle duplicates)
 */
export async function bulkInsertJobs(jobs: Partial<Job>[]): Promise<{ success: number; errors: number }> {
  try {
    const { data, error } = await supabaseAdmin
      .from('jobs')
      .upsert(jobs, {
        onConflict: 'external_id,source',
        ignoreDuplicates: false
      })
      .select('id')

    if (error) {
      console.error('[Supabase] Bulk insert error:', error)
      return { success: 0, errors: jobs.length }
    }

    return { success: data?.length || 0, errors: 0 }
  } catch (error) {
    console.error('[Supabase] Bulk insert failed:', error)
    return { success: 0, errors: jobs.length }
  }
}

/**
 * Search jobs with full-text search
 */
export async function searchJobs(params: JobSearchParams): Promise<JobSearchResult> {
  try {
    let query = supabase
      .from('jobs')
      .select('*', { count: 'exact' })
      .gt('expires_at', new Date().toISOString())

    // Search on title, company, description using ILIKE (since fts column doesn't exist)
    if (params.query) {
      const searchTerms = params.query.split(' ').filter(term => term.length > 0)
      
      // Search across title, company, and description
      if (searchTerms.length > 0) {
        // Use OR conditions to search multiple fields
        const orConditions = searchTerms.map(term => {
          const escapedTerm = term.replace(/[%_]/g, '\\$&')
          return `title.ilike.%${escapedTerm}%,company.ilike.%${escapedTerm}%,description.ilike.%${escapedTerm}%`
        }).join(',')
        
        query = query.or(orConditions)
      }
    }

    // Location filter
    if (params.location) {
      query = query.ilike('location', `%${params.location}%`)
    }

    // Source filter
    if (params.source && params.source.length > 0) {
      query = query.in('source', params.source)
    }

    // Job type filter
    if (params.job_type) {
      query = query.eq('job_type', params.job_type)
    }

    // Remote type filter
    if (params.remote_type) {
      query = query.eq('remote_type', params.remote_type)
    }

    // Salary filter
    if (params.salary_min) {
      query = query.gte('salary_min', params.salary_min)
    }

    // Pagination
    const limit = params.limit || 50
    const offset = params.offset || 0
    query = query.range(offset, offset + limit - 1)

    // Order by relevance (newest first)
    query = query.order('scraped_at', { ascending: false })

    const { data, error, count } = await query

    if (error) {
      console.error('[Supabase] Search error:', error)
      return { jobs: [], total: 0, page: 1, limit }
    }

    return {
      jobs: data as Job[],
      total: count || 0,
      page: Math.floor(offset / limit) + 1,
      limit
    }
  } catch (error) {
    console.error('[Supabase] Search failed:', error)
    return { jobs: [], total: 0, page: 1, limit: params.limit || 50 }
  }
}

/**
 * Get job statistics
 */
export async function getJobStats(): Promise<{
  total: number
  active: number
  expired: number
  bySource: Record<string, number>
}> {
  try {
    // Total jobs
    const { count: total } = await supabase
      .from('jobs')
      .select('*', { count: 'exact', head: true })

    // Active jobs
    const { count: active } = await supabase
      .from('jobs')
      .select('*', { count: 'exact', head: true })
      .gt('expires_at', new Date().toISOString())

    // Jobs by source
    const { data: sourceData } = await supabase
      .from('jobs')
      .select('source')
      .gt('expires_at', new Date().toISOString())

    const bySource: Record<string, number> = {}
    sourceData?.forEach(job => {
      bySource[job.source] = (bySource[job.source] || 0) + 1
    })

    return {
      total: total || 0,
      active: active || 0,
      expired: (total || 0) - (active || 0),
      bySource
    }
  } catch (error) {
    console.error('[Supabase] Stats failed:', error)
    return { total: 0, active: 0, expired: 0, bySource: {} }
  }
}

/**
 * Log download history
 */
export async function logDownloadHistory(history: Partial<DownloadHistory>): Promise<void> {
  try {
    await supabaseAdmin
      .from('download_history')
      .insert(history)
  } catch (error) {
    console.error('[Supabase] Failed to log download history:', error)
  }
}

/**
 * Get or create company
 */
export async function upsertCompany(company: Partial<Company>): Promise<Company | null> {
  try {
    const { data, error } = await supabaseAdmin
      .from('companies')
      .upsert(company, {
        onConflict: 'normalized_name',
        ignoreDuplicates: false
      })
      .select()
      .single()

    if (error) {
      console.error('[Supabase] Company upsert error:', error)
      return null
    }

    return data as Company
  } catch (error) {
    console.error('[Supabase] Company upsert failed:', error)
    return null
  }
}

/**
 * Delete expired jobs (called by cron)
 */
export async function deleteExpiredJobs(): Promise<number> {
  try {
    const { data, error } = await supabaseAdmin
      .from('jobs')
      .delete()
      .lt('expires_at', new Date().toISOString())
      .select('id')

    if (error) {
      console.error('[Supabase] Delete expired jobs error:', error)
      return 0
    }

    return data?.length || 0
  } catch (error) {
    console.error('[Supabase] Delete expired jobs failed:', error)
    return 0
  }
}
</file>

<file path="src/lib/job-aggregator.ts">
/**
 * Job Aggregator with Multi-Layer Caching
 * 
 * Strategy:
 * 1. Check Redis cache (instant)
 * 2. Check MongoDB JobSearchCache (fast)
 * 3. Search Supabase (1,249 jobs, <100ms)
 * 4. If < 10 jobs: Try Cheerio/Puppeteer scrapers (TOP 3 keywords, ~30s)
 * 5. If still < 10: Try Perplexity (last resort, TOP 3 keywords)
 * 6. Cache all results for future users
 */

import { RedisCache } from './redis-cache'
import JobSearchCache, { IJobSearchCache } from '@/models/JobSearchCache'
import { getJobScraper, JobListing } from './job-scraper-service'
import { PerplexityIntelligenceService } from './perplexity-intelligence'
import { getAdzunaClient } from './adzuna-api-client'
import { getJSearchClient } from './jsearch-api-client'

const redis = RedisCache.getInstance()
const adzuna = getAdzunaClient()
const jsearch = getJSearchClient()

export interface JobSearchParams {
  keywords: string[]
  location: string
  radiusKm?: number
  workType?: 'remote' | 'hybrid' | 'onsite' | 'any'
  experienceLevel?: 'entry' | 'mid' | 'senior' | 'executive'
  maxResults?: number
}

export interface JobSearchResult {
  jobs: JobListing[]
  source: 'redis' | 'mongodb' | 'supabase' | 'adzuna' | 'jsearch' | 'perplexity' | 'scraper' | 'hybrid'
  cached: boolean
  timestamp: Date
  searchCount?: number
}

export class JobAggregator {
  private static instance: JobAggregator

  static getInstance(): JobAggregator {
    if (!JobAggregator.instance) {
      JobAggregator.instance = new JobAggregator()
    }
    return JobAggregator.instance
  }

  /**
   * Generate cache key from search params
   */
  private generateCacheKey(params: JobSearchParams): string {
    const normalizedKeywords = params.keywords
      .map(k => k.toLowerCase().trim())
      .sort()
      .join(',')
    
    return `jobs:${normalizedKeywords}:${params.location.toLowerCase()}:${params.workType || 'any'}:${params.radiusKm || 70}`
  }

  /**
   * Search jobs with multi-layer caching
   */
  async searchJobs(params: JobSearchParams): Promise<JobSearchResult> {
    const {
      keywords,
      location,
      radiusKm = 70,
      workType = 'any',
      experienceLevel,
      maxResults = 100
    } = params

    console.log('[JOB_AGGREGATOR] Starting job search:', {
      keywords: keywords.slice(0, 5),
      location,
      radiusKm,
      maxResults
    })

    // LAYER 1: Check Redis cache (instant)
    const redisKey = this.generateCacheKey(params)
    const redisResult = await redis.get<JobListing[]>(redisKey)
    
    if (redisResult && redisResult.length > 0) {
      console.log(`[JOB_AGGREGATOR] ✅ Redis cache HIT: ${redisResult.length} jobs`)
      return {
        jobs: redisResult.slice(0, maxResults),
        source: 'redis',
        cached: true,
        timestamp: new Date()
      }
    }

    console.log('[JOB_AGGREGATOR] Redis cache MISS')

    // LAYER 2: Check MongoDB JobSearchCache (fast)
    const mongoResult = await this.searchMongoCache(keywords, location, workType)
    
    if (mongoResult && mongoResult.jobs.length > 0) {
      console.log(`[JOB_AGGREGATOR] ✅ MongoDB cache HIT: ${mongoResult.jobs.length} jobs`)
      
      // Convert to JobListing format
      const jobs: JobListing[] = mongoResult.jobs.map(job => ({
        jobId: job.jobId,
        title: job.title,
        company: job.company,
        location: job.location,
        description: job.description || 'No description available',
        url: job.url,
        source: job.source,
        salary: job.salary,
        postedDate: job.postedDate,
        workType: (job.workType as 'remote' | 'hybrid' | 'onsite') || 'onsite',
        skillMatchScore: job.skillMatchScore,
        skills: []
      }))

      // Cache in Redis for next time
      await redis.set(redisKey, jobs, 3600) // 1 hour

      // Update search count
      mongoResult.searchCount += 1
      mongoResult.lastSearched = new Date()
      await mongoResult.save()

      return {
        jobs: jobs.slice(0, maxResults),
        source: 'mongodb',
        cached: true,
        timestamp: new Date(),
        searchCount: mongoResult.searchCount
      }
    }

    console.log('[JOB_AGGREGATOR] MongoDB cache MISS')

    // LAYER 3: Search Supabase (1,249 jobs available!) - Use ALL keywords
    console.log('[JOB_AGGREGATOR] Searching Supabase with ALL keywords...')
    let allJobs: JobListing[] = []
    let source: 'supabase' | 'scraper' | 'perplexity' | 'hybrid' = 'supabase'

    try {
      const { searchJobs } = await import('./supabase')
      const supabaseResult = await searchJobs({
        query: keywords.join(' '), // ALL keywords for cache search
        location,
        limit: maxResults
      })

      if (supabaseResult.jobs.length > 0) {
        console.log(`[JOB_AGGREGATOR] ✅ Supabase found ${supabaseResult.jobs.length} jobs`)
        
        // Convert Supabase jobs to JobListing format with validation
        allJobs = supabaseResult.jobs
          .filter(job => {
            // Validate required fields
            if (!job.id || !job.title || !job.company || !job.url) {
              console.warn('[JOB_AGGREGATOR] ⚠️ Skipping invalid job - missing required fields:', {
                id: job.id,
                hasTitle: !!job.title,
                hasCompany: !!job.company,
                hasUrl: !!job.url
              })
              return false
            }
            return true
          })
          .map(job => ({
            jobId: job.id,
            title: job.title,
            company: job.company,
            location: job.location || 'Location not specified',
            description: job.description || 'No description available',
            url: job.url,
            source: job.source,
            salary: job.salary_min && job.salary_max ? `$${job.salary_min} - $${job.salary_max}` : undefined,
            postedDate: job.scraped_at ? new Date(job.scraped_at) : undefined,
            workType: (job.remote_type as 'remote' | 'hybrid' | 'onsite') || 'onsite',
            skillMatchScore: 0,
            skills: []
          } as JobListing))

        // If we got >= 10 jobs from Supabase, cache and return immediately
        if (allJobs.length >= 10) {
          console.log(`[JOB_AGGREGATOR] ✅ Supabase returned ${allJobs.length} jobs (>= 10), skipping fallbacks`)
          await redis.set(redisKey, allJobs, 3600)
          
          return {
            jobs: allJobs.slice(0, maxResults),
            source: 'supabase',
            cached: false,
            timestamp: new Date()
          }
        }
      }
    } catch (error) {
      console.error('[JOB_AGGREGATOR] Supabase search failed:', error)
    }

    console.log(`[JOB_AGGREGATOR] Supabase returned ${allJobs.length} jobs (< 10), triggering fallbacks...`)

    // LAYER 4: If < 10 jobs, use Cheerio/Puppeteer scrapers with TOP 3 keywords
    if (allJobs.length < 10) {
      console.log('[JOB_AGGREGATOR] Using Cheerio/Puppeteer scrapers with TOP 3 keywords...')
      source = 'scraper'
      
      try {
        const top3Keywords = keywords.slice(0, 3)
        console.log(`[JOB_AGGREGATOR] Scraping with keywords: ${top3Keywords.join(', ')}`)
        
        const scrapedJobs = await this.searchWithScrapers(
          top3Keywords, // TOP 3 keywords only
          location,
          10 // Target 10 jobs per keyword = 30 total
        )

        if (scrapedJobs.length > 0) {
          console.log(`[JOB_AGGREGATOR] ✅ Scrapers found ${scrapedJobs.length} jobs`)
          allJobs = [...allJobs, ...scrapedJobs]
          source = allJobs.length > scrapedJobs.length ? 'hybrid' : 'scraper'
        }
      } catch (error) {
        console.error('[JOB_AGGREGATOR] Scrapers failed:', error)
      }
    }

    // LAYER 5: If STILL < 10 jobs, use Perplexity as LAST RESORT with TOP 3 keywords
    if (allJobs.length < 10) {
      console.log('[JOB_AGGREGATOR] Still < 10 jobs, using Perplexity as last resort with TOP 3 keywords...')
      source = 'perplexity'
      
      try {
        const top3Keywords = keywords.slice(0, 3)
        console.log(`[JOB_AGGREGATOR] Perplexity with keywords: ${top3Keywords.join(', ')}`)
        
        const perplexityJobs = await this.searchWithPerplexity(
          top3Keywords, // TOP 3 keywords only
          location,
          10 // Target 10 jobs
        )

        if (perplexityJobs.length > 0) {
          console.log(`[JOB_AGGREGATOR] ✅ Perplexity found ${perplexityJobs.length} jobs`)
          allJobs = [...allJobs, ...perplexityJobs]
          source = allJobs.length > perplexityJobs.length ? 'hybrid' : 'perplexity'
        }
      } catch (error) {
        console.error('[JOB_AGGREGATOR] Perplexity failed:', error)
      }
    }

    // Deduplicate by composite key (company + title + location)
    const uniqueJobs = Array.from(
      new Map(allJobs.map(job => {
        // Create composite key for better deduplication
        const key = `${job.company}|${job.title}|${job.location}`.toLowerCase().trim()
        return [key, job]
      })).values()
    )

    console.log(`[JOB_AGGREGATOR] Total unique jobs after deduplication: ${uniqueJobs.length}`)

    // LAYER 6: Cache results for future users
    if (uniqueJobs.length > 0) {
      await redis.set(redisKey, uniqueJobs, 3600)
      console.log(`[JOB_AGGREGATOR] ✅ Cached ${uniqueJobs.length} jobs in Redis`)
    }

    return {
      jobs: uniqueJobs.slice(0, maxResults),
      source,
      cached: false,
      timestamp: new Date()
    }
  }

  /**
   * Search MongoDB cache
   */
  private async searchMongoCache(
    keywords: string[],
    location: string,
    workType?: string
  ): Promise<IJobSearchCache | null> {
    try {
      const normalizedKeywords = keywords
        .map(k => k.toLowerCase().trim())
        .sort()

      // Find matching cache entry
      const cache = await JobSearchCache.findOne({
        normalizedKeywords: { $all: normalizedKeywords },
        location: new RegExp(location, 'i'),
        $or: [
          { workType: workType },
          { workType: 'any' },
          { workType: { $exists: false } }
        ],
        expiresAt: { $gt: new Date() }
      }).sort({ lastSearched: -1 })

      return cache
    } catch (error) {
      console.error('[JOB_AGGREGATOR] MongoDB search error:', error)
      return null
    }
  }

  /**
   * Search with Adzuna API (FREE)
   */
  private async searchWithAdzuna(
    keywords: string[],
    location: string,
    maxResults: number
  ): Promise<JobListing[]> {
    const jobs: JobListing[] = []

    // Determine country code from location
    let country: 'ca' | 'us' | 'gb' = 'ca'
    const locationLower = location.toLowerCase()
    if (locationLower.includes('usa') || locationLower.includes('united states')) {
      country = 'us'
    } else if (locationLower.includes('uk') || locationLower.includes('united kingdom')) {
      country = 'gb'
    }

    // Search for each keyword
    for (const keyword of keywords) {
      try {
        const results = await adzuna.searchJobs({
          what: keyword,
          where: location,
          country,
          resultsPerPage: Math.min(50, Math.ceil(maxResults / keywords.length)),
          sortBy: 'relevance',
          maxDaysOld: 30 // Only jobs from last 30 days
        })

        if (results.results.length > 0) {
          const converted = results.results.map(job => adzuna.convertToJobListing(job))
          jobs.push(...converted)
        }
      } catch (error) {
        console.error(`[JOB_AGGREGATOR] Adzuna error for "${keyword}":`, error)
      }
    }

    return jobs
  }

  /**
   * Search with JSearch API (FREE, aggregates Indeed/LinkedIn/Glassdoor)
   */
  private async searchWithJSearch(
    keywords: string[],
    location: string,
    maxResults: number
  ): Promise<JobListing[]> {
    const jobs: JobListing[] = []

    // Determine country code from location
    let country = 'us'
    const locationLower = location.toLowerCase()
    if (locationLower.includes('canada') || locationLower.includes('canadian')) {
      country = 'ca'
    } else if (locationLower.includes('uk') || locationLower.includes('united kingdom')) {
      country = 'gb'
    }

    // Search for each keyword
    for (const keyword of keywords) {
      try {
        const result = await jsearch.searchJobs({
          query: keyword,
          location,
          page: 1,
          numPages: 1,
          datePosted: 'month',
          country
        })

        if (result.data && result.data.length > 0) {
          const converted = result.data.map(job => jsearch.convertToJobListing(job))
          jobs.push(...converted)
        }
      } catch (error) {
        console.error(`[JOB_AGGREGATOR] JSearch error for "${keyword}":`, error)
      }
    }

    return jobs
  }

  /**
   * Search with Perplexity
   */
  private async searchWithPerplexity(
    keywords: string[],
    location: string,
    maxResults: number
  ): Promise<JobListing[]> {
    const jobs: JobListing[] = []

    // Search for each keyword
    for (const keyword of keywords) {
      try {
        const result = await PerplexityIntelligenceService.jobMarketAnalysisV2(
          location,
          keyword, // Use keyword as minimal resume text
          {
            roleHint: keyword,
            maxResults: Math.ceil(maxResults / keywords.length)
          }
        )

        if (result.data && result.data.length > 0) {
          // Convert Perplexity format to scraper format
          const convertedJobs: JobListing[] = result.data.map(job => ({
            jobId: job.url ? `perplexity_${Buffer.from(job.url).toString('base64').slice(0, 16)}` : `perplexity_${Date.now()}`,
            title: job.title,
            company: job.company,
            location: job.location,
            description: job.summary || 'No description available',
            url: job.url,
            source: job.source || 'perplexity',
            salary: job.salary || undefined,
            postedDate: job.postedDate ? new Date(job.postedDate) : undefined,
            workType: job.workType,
            experienceLevel: job.experienceLevel,
            skills: job.skills,
            skillMatchScore: job.skillMatchPercent
          }))
          jobs.push(...convertedJobs)
        }
      } catch (error) {
        console.error(`[JOB_AGGREGATOR] Perplexity error for "${keyword}":`, error)
      }
    }

    return jobs
  }

  /**
   * Search with Puppeteer scrapers
   */
  private async searchWithScrapers(
    keywords: string[],
    location: string,
    maxResults: number
  ): Promise<JobListing[]> {
    const scraper = getJobScraper()
    const jobs: JobListing[] = []

    // Search for each keyword
    for (const keyword of keywords) {
      try {
        const result = await scraper.aggregateJobs(keyword, location, {
          maxResults: Math.ceil(maxResults / keywords.length)
        })

        if (result.length > 0) {
          jobs.push(...result)
        }
      } catch (error) {
        console.error(`[JOB_AGGREGATOR] Scraper error for "${keyword}":`, error)
      }
    }

    return jobs
  }

  /**
   * Cache results in both Redis and MongoDB
   */
  private async cacheResults(
    params: JobSearchParams,
    jobs: JobListing[]
  ): Promise<void> {
    const { keywords, location, workType } = params

    try {
      // Cache in Redis (1 hour)
      const redisKey = this.generateCacheKey(params)
      await redis.set(redisKey, jobs, 3600)
      console.log(`[JOB_AGGREGATOR] Cached ${jobs.length} jobs in Redis`)

      // Cache in MongoDB (3 weeks)
      const normalizedKeywords = keywords
        .map(k => k.toLowerCase().trim())
        .sort()

      const mongoJobs = jobs.map(job => ({
        jobId: job.jobId,
        title: job.title,
        company: job.company,
        location: job.location,
        description: job.description,
        url: job.url,
        source: job.source,
        salary: job.salary,
        postedDate: job.postedDate,
        workType: job.workType,
        skillMatchScore: job.skillMatchScore,
        viewedBy: [],
        appliedBy: [],
        savedBy: []
      }))

      await JobSearchCache.create({
        keywords,
        normalizedKeywords,
        location,
        workType: workType || 'any',
        jobs: mongoJobs,
        searchCount: 1,
        lastSearched: new Date(),
        expiresAt: new Date(Date.now() + 21 * 24 * 60 * 60 * 1000) // 3 weeks
      })

      console.log(`[JOB_AGGREGATOR] Cached ${jobs.length} jobs in MongoDB`)
    } catch (error) {
      console.error('[JOB_AGGREGATOR] Cache error:', error)
    }
  }

  /**
   * Search jobs by resume keywords (main entry point)
   */
  async searchByResumeKeywords(
    resumeKeywords: string[],
    location: string,
    radiusKm: number = 70,
    maxResults: number = 100
  ): Promise<JobSearchResult> {
    console.log('[JOB_AGGREGATOR] Searching by resume keywords:', {
      keywordCount: resumeKeywords.length,
      location,
      radiusKm
    })

    // Use top 10 keywords
    const topKeywords = resumeKeywords.slice(0, 10)

    return this.searchJobs({
      keywords: topKeywords,
      location,
      radiusKm,
      maxResults
    })
  }

  /**
   * Get cache statistics
   */
  async getCacheStats(): Promise<{
    redisStats: any
    mongoStats: {
      totalCaches: number
      totalJobs: number
      avgJobsPerCache: number
    }
  }> {
    const redisStats = await redis.getStats()

    const mongoCaches = await JobSearchCache.countDocuments()
    const mongoAgg = await JobSearchCache.aggregate([
      {
        $project: {
          jobCount: { $size: '$jobs' }
        }
      },
      {
        $group: {
          _id: null,
          totalJobs: { $sum: '$jobCount' },
          avgJobs: { $avg: '$jobCount' }
        }
      }
    ])

    return {
      redisStats,
      mongoStats: {
        totalCaches: mongoCaches,
        totalJobs: mongoAgg[0]?.totalJobs || 0,
        avgJobsPerCache: Math.round(mongoAgg[0]?.avgJobs || 0)
      }
    }
  }
}

export default JobAggregator
</file>

<file path="src/lib/perplexity-intelligence.ts">
// FIXED: Universal crypto support (browser + Node.js)
let crypto: any
try {
  crypto = require('crypto')
} catch {
  // Browser environment - will use fallback
  crypto = null
}
import { PerplexityService } from './perplexity-service'
import { 
  CANADIAN_JOB_BOARDS, 
  MAJOR_JOB_BOARDS, 
  OPEN_API_BOARDS,
  ATS_PLATFORMS,
  DISCOVERY_PRIORITY_ORDER
} from './public-job-boards-config'
import { parseAIResponse } from './utils/ai-response-parser'
import { getCoverLetterTemplateById } from './cover-letter-templates'

// Environment
const CACHE_TTL_MS = Number(process.env.PPX_CACHE_TTL_MS || 24 * 60 * 60 * 1000)
const MAX_RETRY_ATTEMPTS = Number(process.env.PPX_MAX_RETRIES || 3)
const RETRY_DELAY_MS = Number(process.env.PPX_RETRY_DELAY || 1000)

type CacheRecord = {
  value: unknown
  metadata: { createdAt: number; hitCount: number; lastAccessed: number }
  expiresAt: number
}

// Simple Map-based cache with TTL
const cache = new Map<string, CacheRecord>()

// Cache cleanup interval (every hour)
setInterval(() => {
  const now = Date.now()
  for (const [key, record] of cache.entries()) {
    if (now > record.expiresAt) {
      cache.delete(key)
    }
  }
}, 60 * 60 * 1000)

function makeKey(prefix: string, payload: unknown): string {
  const raw = typeof payload === 'string' ? payload : JSON.stringify(payload)
  
  // Use crypto if available (Node.js), otherwise simple hash (browser)
  if (crypto && crypto.createHash) {
    return `${prefix}:${crypto.createHash('sha256').update(raw).digest('hex')}`
  }
  
  // Browser fallback: simple hash
  let hash = 0
  for (let i = 0; i < raw.length; i++) {
    const char = raw.charCodeAt(i)
    hash = ((hash << 5) - hash) + char
    hash = hash & hash
  }
  return `${prefix}:${Math.abs(hash).toString(36)}`
}

function getCache(key: string): unknown | undefined {
  const entry = cache.get(key)
  if (!entry) return undefined
  
  // Check if expired
  if (Date.now() > entry.expiresAt) {
    cache.delete(key)
    return undefined
  }
  
  entry.metadata.hitCount += 1
  entry.metadata.lastAccessed = Date.now()
  return entry.value
}

function setCache(key: string, value: unknown) {
  cache.set(key, {
    value,
    expiresAt: Date.now() + CACHE_TTL_MS,
    metadata: {
      createdAt: Date.now(),
      hitCount: 0,
      lastAccessed: Date.now()
    }
  })
}

function createClient(): PerplexityService { return new PerplexityService() }

// ---------- Enhanced helpers (ids, retry, enrichment) ----------
function generateRequestId(): string {
  if (crypto && crypto.randomBytes) {
    return crypto.randomBytes(8).toString('hex')
  }
  // Browser fallback
  return Math.random().toString(36).substr(2, 16) + Date.now().toString(36)
}

// FIXED: Add timeout protection
function withTimeout<T>(promise: Promise<T>, ms: number): Promise<T> {
  return Promise.race([
    promise,
    new Promise<T>((_, reject) => 
      setTimeout(() => reject(new Error(`Request timeout after ${ms}ms`)), ms)
    )
  ])
}

async function withRetry<T>(
  operation: () => Promise<T>,
  maxAttempts: number = MAX_RETRY_ATTEMPTS,
  logger?: { warn?: (message: string, context?: Record<string, unknown>) => void },
  timeoutMs: number = 30000 // 30 second default timeout
): Promise<T> {
  let lastError: unknown
  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    try {
      return await withTimeout(operation(), timeoutMs)
    } catch (err) {
      lastError = err
      if (attempt === maxAttempts) break
      const baseDelay = RETRY_DELAY_MS * Math.pow(2, attempt - 1)
      const jitter = Math.random() * RETRY_DELAY_MS
      const delay = baseDelay + jitter
      logger?.warn?.('Retrying Perplexity operation', {
        attempt,
        delay,
        error: err instanceof Error ? err.message : String(err)
      })
      await new Promise(resolve => setTimeout(resolve, delay))
    }
  }
  throw (lastError instanceof Error ? lastError : new Error('Operation failed'))
}

// Removed unused PerplexityError class - using standard Error instead

// CRITICAL: This generates PATTERN-BASED emails (NOT VERIFIED)
// These are stored as "alternativeEmails" with emailType: 'pattern' and low confidence
// NEVER present these as verified contacts - they are guesses based on common patterns
function inferEmails(name: string, companyDomain: string): string[] {
  if (!name || !companyDomain) return []
  const parts = name.toLowerCase().split(' ').filter(Boolean)
  if (parts.length < 2) return []
  const first = parts[0]
  const last = parts[parts.length - 1]
  const patterns = [
    `${first}.${last}@${companyDomain}`,
    `${first}${last}@${companyDomain}`,
    `${first[0]}${last}@${companyDomain}`,
    `${first}@${companyDomain}`,
    `${last}@${companyDomain}`,
    `${first}.${last[0]}@${companyDomain}`
  ]
  return patterns
}

function normalizeSkills(skills: string[]): string[] {
  const mapping: Record<string, string> = {
    javascript: 'JavaScript', js: 'JavaScript',
    typescript: 'TypeScript', ts: 'TypeScript',
    react: 'React', reactjs: 'React',
    node: 'Node.js', nodejs: 'Node.js',
    python: 'Python', py: 'Python',
    sales: 'Sales', selling: 'Sales',
    crm: 'CRM', 'customer relationship management': 'CRM',
    ai: 'Artificial Intelligence', 'artificial intelligence': 'Artificial Intelligence',
    'machine learning': 'Machine Learning', ml: 'Machine Learning'
  }
  return (skills || []).map(s => {
    const k = s.toLowerCase().trim()
    return mapping[k] || s
  })
}

// CRITICAL FIX: Calculate years of experience from resume text
// Prevents double-counting overlapping periods and filters out education dates
function calculateYearsFromResume(resumeText: string): number {
  // Extract only the work experience section to avoid counting education dates
  const experienceSection = extractExperienceSection(resumeText)
  
  // Match date ranges in various formats
  const dateRegex = /(\w+\s+\d{4}|(\d{1,2}\/\d{4}))\s*[-–—]\s*(\w+\s+\d{4}|Present|Current|(\d{1,2}\/\d{4}))/gi
  const matches = Array.from(experienceSection.matchAll(dateRegex))
  
  // Parse all date ranges into start/end pairs
  const periods: Array<{ start: Date; end: Date }> = []
  for (const match of matches) {
    try {
      const startStr = match[1]
      const endStr = match[3]
      
      const startDate = new Date(startStr)
      const endDate = endStr.match(/Present|Current/i) ? new Date() : new Date(endStr)
      
      // Validate dates are reasonable (not in future, not before 1970)
      if (startDate.getFullYear() < 1970 || startDate.getFullYear() > new Date().getFullYear()) continue
      if (endDate.getFullYear() < 1970 || endDate.getFullYear() > new Date().getFullYear() + 1) continue
      if (startDate > endDate) continue // Skip invalid ranges
      
      const months = (endDate.getFullYear() - startDate.getFullYear()) * 12 + 
                    (endDate.getMonth() - startDate.getMonth())
      
      // Sanity check: skip periods longer than 50 years or negative
      if (months > 0 && months < 600) {
        periods.push({ start: startDate, end: endDate })
      }
    } catch (e) {
      // Skip invalid dates
      continue
    }
  }
  
  // If no valid periods found, return 0
  if (periods.length === 0) return 0
  
  // Sort periods by start date
  periods.sort((a, b) => a.start.getTime() - b.start.getTime())
  
  // Merge overlapping periods to avoid double-counting
  const merged: Array<{ start: Date; end: Date }> = []
  let current = periods[0]
  
  for (let i = 1; i < periods.length; i++) {
    const next = periods[i]
    
    // If periods overlap or are adjacent, merge them
    if (next.start <= current.end) {
      current.end = new Date(Math.max(current.end.getTime(), next.end.getTime()))
    } else {
      // No overlap, push current and start new period
      merged.push(current)
      current = next
    }
  }
  merged.push(current)
  
  // Calculate total months from merged periods
  let totalMonths = 0
  for (const period of merged) {
    const months = (period.end.getFullYear() - period.start.getFullYear()) * 12 + 
                  (period.end.getMonth() - period.start.getMonth())
    totalMonths += months
  }
  
  const years = Math.round(totalMonths / 12)
  
  // CRITICAL FIX: Cap at realistic maximum
  // Assume candidate started working at age 18, max age 65
  // Most candidates are 25-45, so cap at 25 years to be safe
  const maxRealisticYears = 25
  const cappedYears = Math.min(years, maxRealisticYears)
  
  // If calculated years seem unrealistic (>15), round down to nearest 5
  if (cappedYears > 15) {
    return Math.floor(cappedYears / 5) * 5
  }
  
  return cappedYears
}

// Extract work experience section from resume to avoid counting education dates
function extractExperienceSection(resumeText: string): string {
  const text = resumeText.toLowerCase()
  
  // Find work experience section markers
  const experienceMarkers = [
    'work experience',
    'professional experience',
    'employment history',
    'experience',
    'work history',
    'career history'
  ]
  
  // Find education section markers to exclude
  const educationMarkers = [
    'education',
    'academic background',
    'academic history',
    'degrees'
  ]
  
  let experienceStart = -1
  let experienceMarker = ''
  
  // Find the earliest experience marker
  for (const marker of experienceMarkers) {
    const index = text.indexOf(marker)
    if (index !== -1 && (experienceStart === -1 || index < experienceStart)) {
      experienceStart = index
      experienceMarker = marker
    }
  }
  
  // If no experience section found, use entire resume (fallback)
  if (experienceStart === -1) return resumeText
  
  // Find where experience section ends (usually at education or end of document)
  let experienceEnd = resumeText.length
  for (const marker of educationMarkers) {
    const index = text.indexOf(marker, experienceStart + experienceMarker.length)
    if (index !== -1 && index < experienceEnd) {
      experienceEnd = index
    }
  }
  
  return resumeText.substring(experienceStart, experienceEnd)
}

// Enhanced response wrappers (non-breaking: used by new V2 methods only)
export type RequestMetadata = { 
  requestId: string
  timestamp: number
  duration?: number
  error?: string
  boardsSearched?: number
  resultsCount?: number
  attemptedCleanups?: string[]
  contactsFound?: number
  withEmails?: number
  agent_iterations?: number
  tools_used?: string[]
  reasoning?: string
  confidence?: number
  method?: string
  sources?: number
}
export type EnhancedResponse<T> = { success: boolean; data: T; metadata: RequestMetadata; cached: boolean }

export interface IntelligenceRequest {
  company: string
  role?: string
  geo?: string
}

export interface IntelligenceResponse {
  company: string
  freshness: string
  sources: Array<{ title: string; url: string }>
  confidence: number
  financials: Array<{ metric: string; value: string; confidence: number; source?: string }>
  culture: Array<{ point: string; confidence: number; source?: string }>
  salaries: Array<{ title: string; range: string; currency?: string; geo?: string; source?: string; confidence: number }>
  contacts: Array<{ name: string; title: string; url?: string; source?: string; confidence: number }>
  growth: Array<{ signal: string; source?: string; confidence: number }>
  summary: string
  description: string
  size: string
  revenue: string
  industry: string
  founded: string
  headquarters: string
  psychology: string
  marketIntelligence: string
  // CRITICAL: New comprehensive intelligence fields
  recentNews?: Array<{ title: string; date: string; url: string; summary: string }>
  socialMedia?: {
    linkedin?: string
    twitter?: string
    facebook?: string
    instagram?: string
    youtube?: string
  }
  glassdoorRating?: {
    overallRating?: number
    ceoApproval?: number
    recommendToFriend?: number
    reviewCount?: number
    url?: string
  }
  stockProfile?: {
    ticker?: string
    exchange?: string
    currentPrice?: string
    marketCap?: string
    isPublic?: boolean
  }
}

// V2 Data structures (for job listings and contacts)
export interface JobListing {
  title: string
  company: string
  location: string
  address?: string | null
  url: string
  source?: string
  summary: string
  postedDate: string
  salary?: string | null
  skillMatchPercent: number
  skills: string[]
  workType?: 'remote' | 'hybrid' | 'onsite'
  experienceLevel?: 'entry' | 'mid' | 'senior' | 'executive'
  contacts: {
    hrEmail?: string | null
    hiringManagerEmail?: string | null
    generalEmail?: string | null
    phone?: string | null
    linkedinProfiles: string[]
  }
  benefits?: string[]
  requirements?: string[]
}

export interface HiringContact {
  name: string
  title: string
  department: string
  linkedinUrl?: string | null
  email?: string | null
  emailType?: 'public' | 'inferred' | 'pattern'
  source: string
  confidence: number
  phone?: string | null
  alternativeEmails?: string[]
  discoveryMethod?: string
}

export interface QuickSearchItem {
  title: string
  url: string
  snippet: string
  source: string
  postedDate?: string
  location?: string
  company?: string
  date?: string
}

const SYSTEM = `You are a research analyst using real-time web tools.
CRITICAL: Your response must be ONLY valid JSON. NO explanatory text, NO markdown, NO commentary.
Rules:
- Use only public sources and respect robots.txt by following links provided by Perplexity tools.
- Always return ONLY structured JSON matching the requested schema.
- Include 5-10 source citations with titles and URLs.
- Provide confidence scores (0-1) for each data point and overall.
- Mark estimates or unverified signals clearly.
- NEVER add text before or after the JSON response.
`

interface ComprehensiveJobResearchData {
  jobAnalysis: {
    matchScore: number
    matchingSkills: string[]
    missingSkills: string[]
    skillsToHighlight: string[]
    recommendations: string[]
    estimatedFit: string
  }
  companyIntel: {
    company: string
    description: string
    size?: string
    revenue?: string
    industry?: string
    founded?: string
    headquarters?: string
    website?: string
    marketPosition?: string
  }
  companyPsychology: {
    culture: string
    values: string[]
    managementStyle?: string
    workEnvironment?: string
  }
  hiringContacts: Array<{
    name: string
    title: string
    department?: string
    email?: string
    linkedinUrl?: string
    authority: 'decision maker' | 'recruiter' | 'manager' | 'coordinator'
    confidence: number
    contactMethod?: string
  }>
  marketIntelligence: {
    competitivePosition?: string
    industryTrends?: string
    financialStability?: string
    recentPerformance?: string
  }
  news: Array<{
    title: string
    summary: string
    url: string
    date?: string
    source?: string
    impact?: string
  }>
  reviews: Array<{
    platform: string
    rating?: number
    summary: string
    url: string
    pros?: string[]
    cons?: string[]
  }>
  compensation: {
    salaryRange?: string
    benefits?: string
  }
  strategicRecommendations: {
    applicationStrategy: string
    contactStrategy: string
    interviewPrep: string[]
  }
  sources: string[]
  confidenceLevel: number
}

interface EnhancedCompanyResearchData {
  companyIntelligence: {
    name: string
    industry?: string
    founded?: string
    headquarters?: string
    employeeCount?: string
    revenue?: string
    website?: string
    description?: string
    marketPosition?: string
    financialStability?: string
    recentPerformance?: string
  }
  hiringContactIntelligence: {
    officialChannels?: {
      careersPage?: string
      jobsEmail?: string
      hrEmail?: string
      phone?: string
      address?: string
    }
    keyContacts?: Array<{
      name: string
      title: string
      department?: string
      linkedinUrl?: string
      email?: string
      authority?: string
      contactMethod?: string
    }>
    emailFormat?: string
    socialMedia?: Record<string, string>
  }
  companyPsychology?: {
    culture?: string
    values?: string[]
    managementStyle?: string
    workEnvironment?: string
  }
  reviewAnalysis?: {
    glassdoor?: {
      rating?: number
      reviewCount?: number
      ceoApproval?: string | number
      recommendToFriend?: string | number
      pros?: string[]
      cons?: string[]
    }
    employeeSentiment?: string
  }
  aiAutomationThreat?: {
    roleRisk?: string
    automationProbability?: string
    timeframe?: string
    companyAIAdoption?: string
    futureOutlook?: string
    recommendations?: string[]
  }
  recentNews?: Array<{
    headline?: string
    date?: string
    source?: string
    url?: string
    impact?: string
  }>
  compensation?: {
    salaryRange?: string
    benefits?: string
  }
  redFlags?: string[]
  strategicRecommendations?: {
    applicationStrategy?: string
    contactStrategy?: string
    interviewPrep?: string[]
  }
  sources?: string[]
  confidenceLevel?: number
}

export class PerplexityIntelligenceService {
  /**
   * CRITICAL FIX: Scrapes job URL to get full description when Perplexity returns incomplete data
   * Fallback for when descriptions are too short
   */
  private static async scrapeJobURL(url: string): Promise<string> {
    try {
      const response = await fetch(url, {
        headers: {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        },
        signal: AbortSignal.timeout(10000) // 10 second timeout
      })
      
      if (!response.ok) return ''
      
      const html = await response.text()
      
      // Try multiple common job description selectors
      const patterns = [
        /<div[^>]*class="[^"]*description[^"]*"[^>]*>(.*?)<\/div>/is,
        /<div[^>]*id="[^"]*description[^"]*"[^>]*>(.*?)<\/div>/is,
        /<section[^>]*class="[^"]*job-description[^"]*"[^>]*>(.*?)<\/section>/is,
        /<div[^>]*class="[^"]*job-details[^"]*"[^>]*>(.*?)<\/div>/is
      ]
      
      for (const pattern of patterns) {
        const match = html.match(pattern)
        if (match && match[1]) {
          // Strip HTML tags and clean up
          const cleaned = match[1]
            .replace(/<script[^>]*>.*?<\/script>/gis, '')
            .replace(/<style[^>]*>.*?<\/style>/gis, '')
            .replace(/<[^>]+>/g, ' ')
            .replace(/\s+/g, ' ')
            .trim()
          
          if (cleaned.length > 150) {
            return cleaned
          }
        }
      }
      
      return ''
    } catch (error) {
      if (process.env.PPX_DEBUG === 'true') {
        console.warn(`[SCRAPE] Failed to scrape ${url}:`, error)
      }
      return ''
    }
  }

  /**
   * CRITICAL FIX: Validates job listings response from Perplexity
   * Filters out incomplete, fake, or low-quality jobs
   */
  private static validateJobListings(jobs: JobListing[], minRequired: number): JobListing[] {
    const validated = jobs.filter((job: JobListing) => {
      // ❌ REJECT: Empty or short descriptions
      if (!job.summary || job.summary.trim().length < 150) {
        if (process.env.PPX_DEBUG === 'true') {
          console.warn(`[VALIDATE] Rejecting ${job.title} - description too short (${job.summary?.length || 0} chars)`)
        }
        return false
      }
      
      // ❌ REJECT: Confidential companies
      const confidentialKeywords = ['confidential', 'various', 'tbd', 'multiple', 'undisclosed', 'anonymous', 'private', 'stealth', 'hidden']
      const company = String(job.company || '').toLowerCase().trim()
      if (confidentialKeywords.some(kw => company.includes(kw)) || company.length < 3) {
        if (process.env.PPX_DEBUG === 'true') {
          console.warn(`[VALIDATE] Rejecting ${job.title} - confidential company: ${job.company}`)
        }
        return false
      }
      
      // ❌ REJECT: No valid URL
      if (!job.url || !job.url.includes('http')) {
        if (process.env.PPX_DEBUG === 'true') {
          console.warn(`[VALIDATE] Rejecting ${job.title} - invalid URL: ${job.url}`)
        }
        return false
      }
      
      // ✅ ACCEPT
      return true
    })
    
    // Warn if too many filtered out
    if (validated.length < minRequired * 0.5 && process.env.PPX_DEBUG === 'true') {
      console.warn(`[VALIDATE] Only ${validated.length}/${minRequired} jobs passed validation (${Math.round(validated.length/minRequired*100)}%)`)
    }
    
    return validated
  }

  /**
   * CRITICAL FIX: Validates hiring contacts response from Perplexity
   * Filters out fake emails, personal domains, pattern-based guesses
   */
  private static validateHiringContacts(contacts: HiringContact[]): HiringContact[] {
    const validated = contacts.filter((contact: HiringContact) => {
      // ❌ REJECT: No email and no LinkedIn
      if (!contact.email && !contact.linkedinUrl) {
        if (process.env.PPX_DEBUG === 'true') {
          console.warn(`[VALIDATE] Rejecting ${contact.name} - no contact method`)
        }
        return false
      }
      
      // ❌ REJECT: Personal email domains (if email exists)
      if (contact.email) {
        const personalDomains = ['gmail', 'yahoo', 'hotmail', 'outlook', 'aol', 'icloud', 'protonmail']
        if (personalDomains.some(d => contact.email!.toLowerCase().includes(d))) {
          if (process.env.PPX_DEBUG === 'true') {
            console.warn(`[VALIDATE] Rejecting ${contact.email} - personal domain`)
          }
          return false
        }
        
        // ❌ REJECT: Template/placeholder emails
        if (contact.email.includes('[') || contact.email.includes('VISIT') || contact.email.includes('example') || contact.email.includes('domain.')) {
          if (process.env.PPX_DEBUG === 'true') {
            console.warn(`[VALIDATE] Rejecting ${contact.email} - template email`)
          }
          return false
        }
      }
      
      // ✅ ACCEPT
      return true
    })
    
    return validated
  }

  // V2: Enhanced company research with retries and metadata
  static async researchCompanyV2(input: IntelligenceRequest): Promise<EnhancedResponse<IntelligenceResponse>> {
    const requestId = generateRequestId()
    const started = Date.now()
    const key = makeKey('ppx:research:v2', input)
    const cached = getCache(key) as IntelligenceResponse | undefined
    if (cached) {
      return { success: true, data: cached, metadata: { requestId, timestamp: started, duration: Date.now() - started }, cached: true }
    }
    try {
      const userPrompt = `COMPREHENSIVE RESEARCH TASK: Search for contacts, emails, website, and complete intelligence for ${input.company}${input.role ? ` (role: ${input.role})` : ''}${input.geo ? ` in ${input.geo}` : ''}.

**MANDATORY SEARCH SOURCES:**
- Use Google search extensively
- Search LinkedIn company page AND individual employee profiles
- Search all social media platforms (Twitter, Facebook, Instagram, YouTube)
- Search company website thoroughly
- Search business directories (BBB, Yellow Pages, ZoomInfo, etc.)
- Search news sources and press releases
- Search Glassdoor for reviews and salaries
- Search stock exchanges if publicly traded

**RETURN DETAILED JSON with ALL fields below:**
{
  "company": string (full legal name),
  "description": string (detailed company overview - NOT "No description available"),
  "size": string (employee count with source),
  "revenue": string (annual revenue estimate with source),
  "industry": string (specific industry classification),
  "founded": string (year or date with source),
  "headquarters": string (full address with city, province/state, postal code),
  "psychology": string (company culture, values, workplace environment - from Glassdoor/employee reviews),
  "marketIntelligence": string (market position, competitive landscape, growth trends - detailed analysis),
  "freshness": string (ISO datetime of research),
  "sources": [{"title": string, "url": string}] (minimum 8 sources, up to 20),
  "confidence": number (0 to 1),
  "financials": [{"metric": string, "value": string, "confidence": number, "source": string}],
  "culture": [{"point": string, "confidence": number, "source": string}] (from Glassdoor/reviews),
  "salaries": [{"title": string, "range": string, "currency": string, "geo": string, "source": string, "confidence": number}],
  "contacts": [{"name": string, "title": string, "email": string, "url": string, "source": string, "confidence": number}] (executives, managers, recruiters from LinkedIn with emails),
  "generalEmail": string (company general inbox: careers@, hr@, jobs@, info@, hello@, contact@ - MANDATORY),
  "careersPage": string (company careers/jobs page URL),
  "growth": [{"signal": string, "source": string, "confidence": number}],
  "summary": string (comprehensive 2-3 paragraph summary),
  "recentNews": [{"title": string, "date": string, "url": string, "summary": string}] (last 6 months),
  "socialMedia": {"linkedin": string, "twitter": string, "facebook": string, "instagram": string, "youtube": string},
  "glassdoorRating": {"overallRating": number, "ceoApproval": number, "recommendToFriend": number, "reviewCount": number, "url": string},
  "stockProfile": {"ticker": string, "exchange": string, "currentPrice": string, "marketCap": string, "isPublic": boolean}
}

**CRITICAL REQUIREMENTS:**
1. Search company website for About page, Contact page, Leadership/Team page
2. **MANDATORY**: Extract company general email from website footer/contact page (careers@, hr@, jobs@, info@, hello@, contact@)
3. **MANDATORY**: Find company careers/jobs page URL
4. Search "site:linkedin.com/company/${input.company}" for official company page
5. Search "site:linkedin.com ${input.company} CEO OR president OR manager" for executive contacts WITH emails
6. Search "${input.company} headquarters address phone email"
7. Search "${input.company} site:glassdoor.com" for reviews and culture insights
8. Search "${input.company} revenue employees industry" for business intelligence
9. DO NOT return "Unknown", "No description available", or "No data" - search multiple sources until you find information
10. Include REAL contact information (names, titles, emails, LinkedIn URLs) - minimum 3 contacts if company has >10 employees
11. **APP IS USELESS WITHOUT CONTACT INFO** - Always return at least generalEmail even if no specific contacts found`
      const out = await withRetry(async () => {
        const client = createClient()
        const user = userPrompt
        const res = await client.makeRequest(SYSTEM, user, { temperature: 0.2, maxTokens: 3000, model: 'sonar-pro' })
        if (!res.content?.trim()) throw new Error('Empty response')
        return res
      })
      const context = {
        requestId,
        prompts: { system: SYSTEM, user: userPrompt },
        timestamp: started,
        duration: Date.now() - started
      }
      const parsed = parseAIResponse<IntelligenceResponse>(out.content ?? '', { stripMarkdown: true, extractFirst: true }, context)
      parsed.company = parsed.company || input.company
      parsed.freshness = parsed.freshness || new Date().toISOString()
      parsed.sources = Array.isArray(parsed.sources) ? parsed.sources.slice(0, 12) : []
      parsed.confidence = typeof parsed.confidence === 'number' ? Math.max(0, Math.min(1, parsed.confidence)) : 0.6
      if (Array.isArray(parsed.contacts)) {
        parsed.contacts = parsed.contacts.map(c => ({ ...c, url: c.url }))
      }
      setCache(key, parsed)
      return { success: true, data: parsed, metadata: { requestId, timestamp: started, duration: Date.now() - started }, cached: false }
    } catch (e) {
      const fb: IntelligenceResponse = {
        company: input.company,
        freshness: new Date().toISOString(),
        sources: [],
        confidence: 0.3,
        financials: [],
        culture: [],
        salaries: [],
        contacts: [],
        growth: [],
        summary: 'Research failed - please retry',
        description: 'No description available',
        size: 'Unknown',
        revenue: 'Unknown',
        industry: 'Unknown',
        founded: 'Unknown',
        headquarters: 'Unknown',
        psychology: 'No insights available',
        marketIntelligence: 'No market data available',
        recentNews: [],
        socialMedia: {},
        glassdoorRating: undefined,
        stockProfile: undefined
      }
      return { success: false, data: fb, metadata: { requestId, timestamp: started, duration: Date.now() - started, error: (e as Error).message }, cached: false }
    }
  }
  // REMOVED: Old researchCompany - Use researchCompanyV2 instead

  static async salaryForRole(role: string, company?: string, geo?: string) {
    const key = makeKey('ppx:salary', { role, company, geo })
    const cached = getCache(key)
    if (cached) return cached
    const client = createClient()
    const user = `Find current salary ranges for ${role}${company ? ` at ${company}` : ''}${geo ? ` in ${geo}` : ''}. Return JSON: items[{title,range,currency,geo,source,confidence}], summary, freshness`;
    try {
      const out = await client.makeRequest(SYSTEM, user, { temperature: 0.2, maxTokens: 900, model: 'sonar-pro' })
      const text = (out.content || '').trim()
      const context = {
        requestId: generateRequestId(),
        prompts: { system: SYSTEM, user },
        timestamp: Date.now(),
        duration: 0
      }
      const parsed = parseAIResponse<Record<string, unknown>>(text, { stripMarkdown: true, extractFirst: true }, context)
      setCache(key, parsed)
      return parsed
    } catch {
      return { items: [], summary: 'Unavailable', freshness: new Date().toISOString() }
    }
  }

  /**
   * Enhanced job listings search across 25+ Canadian and global job boards
   * Integrates with public-job-boards-config.ts for comprehensive coverage
   */
  static async jobListings(
    jobTitle: string, 
    location: string,
    options: {
      boards?: string[] // Specific boards to search (uses DISCOVERY_PRIORITY_ORDER if not specified)
      limit?: number
      includeCanadianOnly?: boolean
    } = {}
  ) {
    const { boards, limit = 50, includeCanadianOnly = false } = options
    const key = makeKey('ppx:jobs', { jobTitle, location, boards, limit })
    const cached = getCache(key)
    if (cached) return cached

    // Determine which boards to search
    const targetBoards = boards || (includeCanadianOnly 
      ? Object.keys(CANADIAN_JOB_BOARDS)
      : DISCOVERY_PRIORITY_ORDER.slice(0, 15) // Top 15 boards
    )

    // Note: targetBoards is used in the Perplexity prompt below to guide source selection

    const client = createClient()
    const SYSTEM_JOBS = `You are an advanced Job Listings Aggregator with real-time web access across 25+ Canadian and global job boards.

PRIORITY CANADIAN SOURCES:
- Job Bank Canada (jobbank.gc.ca) - Government jobs
- AutoJobs (autojobs.com) - Canadian automotive & skilled trades
- SimplyHired Canada (simplyhired.ca) - Canadian aggregator
- Jobboom (jobboom.com) - Bilingual Canadian
- Workopolis (workopolis.com) - Canadian
- Indeed Canada (ca.indeed.com)
- Jooble Canada (ca.jooble.org)
- ZipRecruiter Canada (ziprecruiter.ca)
- Monster Canada (monster.ca)
- Glassdoor Canada (glassdoor.ca)
- Dice Canada (dice.com)
- Careerjet Canada (careerjet.ca)

GLOBAL SOURCES:
- LinkedIn (linkedin.com/jobs)
- Indeed (indeed.com)
- Glassdoor (glassdoor.com)
- Adzuna (adzuna.com)

ATS PLATFORMS (Canadian Tech Companies):
- Greenhouse: Shopify, Hootsuite, Wealthsimple, Faire, Thinkific, Lightspeed
- Lever: Slack, Shopify, Bench, Clio, Clearco, League
- Workable: FreshBooks, Visier, Unbounce, Axonify
- Recruitee: Paytm, Ecobee, Geotab, Auvik, Wave, KOHO
- Ashby: Faire, Clearco, Maple, Borrowell, Shakepay
- Breezy HR: Lumerate, Zymewire, and other Canadian startups
- Communitech Job Board: communitech.ca/companies (Waterloo tech ecosystem)
- RemoteRocketship: remoterocketship.com (Remote Canadian jobs)

🔥 CRITICAL - FOLLOW LINKS AND EXTRACT FULL CONTENT:
For EACH job found, you MUST:
1. Find the job in search results (title, company, location, URL)
2. **FOLLOW THE JOB URL** and visit the actual job posting page
3. **SCRAPE THE COMPLETE JOB DESCRIPTION** from the posting page (all paragraphs, all bullet points)
4. Extract salary, benefits, requirements, responsibilities from the posting page
5. If company name is "Confidential" in search results - **VISIT THE URL** and extract the REAL company name from the posting page
6. If description is missing - **TRY COMPANY CAREERS PAGE** (company.com/careers) or **COMPANY ATS** (company.breezy.hr, company.greenhouse.io)

CRITICAL REQUIREMENTS:
1. **ONLY REAL COMPANY NAMES** - ABSOLUTELY NO CONFIDENTIAL LISTINGS:
   ❌ REJECT AND SKIP: "Confidential", "Various Employers", "Multiple Companies", "Undisclosed", "Private", "TBD", "N/A", "Various [Industry]", "Anonymous", "Stealth", "Hidden"
   ❌ DO NOT INCLUDE jobs where company name is hidden or confidential
   ✅ ONLY INCLUDE: Jobs with real, specific company names (e.g., "Ricoh Canada", "Shopify", "TD Bank", "Lumerate", "Zymewire")
2. **VERIFY COMPANY EXISTS** - Must be a real, identifiable company
3. **SKIP INVALID LISTINGS** - If company name is missing or confidential, DO NOT include it in results
4. **EXTRACT FULL DESCRIPTIONS** - Visit each job URL and scrape complete description (minimum 200 words)
5. Search ONLY publicly accessible listings (no login required)
6. Prioritize Canadian sources for Canadian locations
7. **Extract salary** from job posting page if available
8. Deduplicate across all sources by company + title
9. Rank by: recency → Canadian source priority → relevance
10. Return EXACTLY ${limit} unique listings with REAL company names and FULL descriptions

OUTPUT JSON (MUST BE VALID, COMPLETE JSON):
[{
  "title": string (specific job title, not "Various Positions"),
  "company": string (EXACT company name, not generic),
  "location": string (specific city/province),
  "url": string (direct job posting link),
  "summary": string (200-400 words, COMPLETE job description from posting page),
  "salary": string | null (extracted from posting page),
  "postedDate": "YYYY-MM-DD",
  "source": string (board name),
  "requirements": string[] (key requirements from posting),
  "benefits": string[] (benefits mentioned in posting)
}]`

    const USER_JOBS = `Search for "${jobTitle}" jobs in ${location} across these prioritized sources:
${targetBoards.slice(0, 10).join(', ')}

Return ${limit} unique, recent listings in JSON format. For Canadian locations, prioritize Job Bank, Jobboom, Workopolis first.`

    const requestId = generateRequestId()
    const started = Date.now()
    try {
      const out = await client.makeRequest(SYSTEM_JOBS, USER_JOBS, { 
        temperature: 0.2, 
        maxTokens: Math.min(limit * 500, 30000), // CRITICAL FIX: Increased from 300 to 500 tokens per job for full descriptions
        model: 'sonar-pro' // Use research model for job search
      })
      
      // FIXED: Check for truncation warning
      if (out.content.length > 18000) {
        console.warn('[JOB_LISTINGS] Response may be truncated, consider reducing limit or splitting into batches')
      }
      let text = (out.content || '').trim()
      
      // Extract JSON from response if wrapped in markdown or explanation
      const jsonMatch = text.match(/\[[\s\S]*\]/)
      if (jsonMatch) {
        text = jsonMatch[0]
      }
      
      // FIX: Clean up truncated JSON
      // If JSON ends abruptly without closing ], try to fix it
      if (!text.endsWith(']')) {
        console.warn('[PERPLEXITY] JSON appears truncated, attempting to fix')
        // Find last complete object
        const lastCompleteObj = text.lastIndexOf('}')
        if (lastCompleteObj > 0) {
          text = text.substring(0, lastCompleteObj + 1) + ']'
        }
      }
      
      // FIX: Remove trailing commas before ]
      text = text.replace(/,(\s*)\]/g, '$1]')
      
      const context = {
        requestId,
        prompts: { system: SYSTEM_JOBS, user: USER_JOBS },
        timestamp: started,
        duration: Date.now() - started
      }
      let parsed: unknown
      try {
        parsed = parseAIResponse<unknown>(text, { stripMarkdown: true, extractFirst: true }, context)
      } catch (parseError: unknown) {
        console.error('[PERPLEXITY] JSON parse failed, raw text:', text.substring(0, 500))
        console.error('[PERPLEXITY] Parse error:', parseError)
        return []
      }
      
      const arr = Array.isArray(parsed) ? parsed.slice(0, limit) : []
      
      // CRITICAL FIX: Filter out confidential companies (NO FAKE/INFERRED DATA)
      const filtered = arr.filter((job: unknown) => {
        const jobObj = job as Record<string, unknown>
        const companyRaw = String(jobObj.company || '')
        const company = companyRaw.toLowerCase().trim()
        
        const isConfidential = 
          company.includes('confidential') ||
          company.includes('anonymous') ||
          company.includes('undisclosed') ||
          company.includes('various') ||
          company.includes('multiple') ||
          company.includes('private') ||
          company.includes('stealth') ||
          company.includes('hidden') ||
          company.includes('tbd') ||
          company.includes('n/a') ||
          company === '' ||
          company.length < 3
        
        if (isConfidential) {
          return false
        }
        return true
      })
      
      // Filtered confidential postings
      
      // Enhance with board metadata
      const enhanced = filtered.map((job: unknown) => {
        const jobObj = job as Record<string, unknown>
        return {
          ...jobObj,
          metadata: {
            searchedBoards: targetBoards.length,
            canadianPriority: includeCanadianOnly,
            extractedAt: new Date().toISOString(),
            confidentialFiltered: arr.length - filtered.length
          }
        }
      })
      
      // FIXED: Only cache if we have good success rate (at least 80%)
      const successRate = enhanced.length / limit
      if (enhanced.length > 0 && successRate >= 0.8) {
        setCache(key, enhanced)
        // Cached jobs
      } else if (enhanced.length > 0) {
        // Skipping cache - low success rate
      }
      return enhanced
    } catch (error) {
      console.error('[PERPLEXITY] Job listings failed:', error)
      return []
    }
  }

  // Fast SEARCH API for raw listings from specific domains (outside of template strings)
  static async jobQuickSearch(query: string, domains: string[] = [], maxResults: number = 20, recency: 'day'|'week'|'month'|'year' = 'month'): Promise<QuickSearchItem[]> {
    const key = makeKey('ppx:search', { query, domains, maxResults, recency })
    const cached = getCache(key) as QuickSearchItem[] | undefined
    if (cached) return cached
    try {
      const resp = await fetch('https://api.perplexity.ai/search', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.PERPLEXITY_API_KEY || ''}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          query,
          max_results: Math.max(5, Math.min(25, maxResults)),
          ...(domains.length ? { search_domain_filter: domains } : {}),
          search_recency_filter: recency
        })
      })
      if (!resp.ok) throw new Error('ppx search failed')
      const data = await resp.json() as unknown
      const asRecord = data as Record<string, unknown>
      const arr = (Array.isArray(asRecord?.results) ? (asRecord.results as unknown[]) : (Array.isArray(data as unknown[]) ? (data as unknown[]) : []))
      const mapped: QuickSearchItem[] = arr.map((raw: unknown) => {
        const it = (raw || {}) as Record<string, unknown>
        const title = typeof it.title === 'string' ? it.title : (typeof it.snippet === 'string' ? String(it.snippet) : '')
        const url = typeof it.url === 'string' ? it.url : (typeof it.link === 'string' ? String(it.link) : '')
        const snippet = typeof it.snippet === 'string' ? String(it.snippet) : (typeof it.summary === 'string' ? String(it.summary) : '')
        const source = typeof it.domain === 'string' ? String(it.domain) : (typeof it.source === 'string' ? String(it.source) : '')
        const publishedTime = it.published_time
        const dateField = it.date
        const published = (typeof publishedTime === 'string' ? publishedTime : (typeof dateField === 'string' ? dateField : undefined))
        return { title, url, snippet, source, postedDate: published }
      })
      setCache(key, mapped)
      return mapped
    } catch {
      return []
    }
  }

  // REMOVED: jobMarketAnalysis wrapper - Use jobMarketAnalysisV2 directly
  /**
   * V2: Enhanced job market analysis with options and ranking
   * Now integrated with 25+ Canadian and global job boards
   */
  static async jobMarketAnalysisV2(
    location: string, 
    resumeText: string, 
    options: { 
      roleHint?: string
      workType?: 'remote'|'hybrid'|'onsite'|'any'
      salaryMin?: number
      experienceLevel?: 'entry'|'mid'|'senior'|'executive'
      maxResults?: number
      boards?: string[] // Specify which boards to prioritize
    } = {}
  ): Promise<EnhancedResponse<JobListing[]>> {
    const requestId = generateRequestId()
    const started = Date.now()
    
    // CRITICAL FIX: Don't return cached results immediately - always search for NEW jobs
    // Cache will be merged with new results at the API level (job-search-cache.service)
    // This ensures users always see fresh job postings while benefiting from cached jobs
    const key = makeKey('ppx:jobmarket:v2', { location, resume: resumeText.slice(0,1000), options })
    // REMOVED: Early return of cached results - we always want fresh jobs

    // Determine if location is Canadian for prioritization
    const isCanadian = /canada|canadian|toronto|vancouver|montreal|calgary|ottawa|edmonton|quebec|winnipeg|halifax/i.test(location)
    const targetBoards = options.boards || (isCanadian 
      ? DISCOVERY_PRIORITY_ORDER.filter(b => CANADIAN_JOB_BOARDS[b]).concat(['linkedin', 'indeed', 'glassdoor'])
      : DISCOVERY_PRIORITY_ORDER.slice(0, 15)
    )

    try {
      const out = await withRetry(async () => {
        const client = createClient()
        const prompt = `Find me ${options.roleHint || 'jobs'} in ${location}

Search Indeed, LinkedIn, Glassdoor, Workopolis, and other job boards for active job postings.

For each job found, extract:
- title (exact job title)
- company (real company name, NO "Confidential" or "Unknown")
- location (city, province/state)
- url (direct link to job posting on Indeed/LinkedIn/etc)
- summary (150+ character description of the role and requirements)
- salary (if listed, else null)
- postedDate (YYYY-MM-DD format if available)
- source (job board name: indeed, linkedin, glassdoor, etc)
- workType ("remote" | "hybrid" | "onsite")
- experienceLevel ("entry" | "mid" | "senior" | "executive")

CANDIDATE SKILLS FOR MATCHING:
${resumeText.slice(0, 500)}

Calculate skillMatchPercent (0-100) based on how well candidate skills match job requirements.
Extract skills array from job requirements.

Return MINIMUM ${options.maxResults || 25} jobs as a JSON array.

CRITICAL: Return ONLY valid JSON array, no markdown, no explanations:
[
  {
    "title": "Senior Software Developer",
    "company": "Shopify",
    "location": "${location}",
    "url": "https://ca.indeed.com/viewjob?jk=abc123",
    "summary": "We are seeking a Senior Software Developer with 5+ years experience...",
    "salary": "$100,000 - $130,000",
    "postedDate": "2025-10-27",
    "source": "indeed",
    "skillMatchPercent": 85,
    "skills": ["JavaScript", "React", "Node.js"],
    "workType": "hybrid",
    "experienceLevel": "senior"
  }
]`

        const res = await client.makeRequest(SYSTEM, prompt, { 
          temperature: 0.2, // Slightly higher for more variety
          maxTokens: 20000, // Increased to allow more jobs
          model: 'sonar-pro' // CRITICAL: Use sonar-pro for real-time web search
        })
        if (!res.content?.trim()) throw new Error('Empty job analysis')
        
        console.log('[JOB_SEARCH_V2] Perplexity response received:', {
          contentLength: res.content.length,
          preview: res.content.slice(0, 500)
        })
        
        return res
      })

      console.log('[JOB_SEARCH_V2] Parsing response...')
      console.log('[JOB_SEARCH_V2] Raw content length:', out.content.length)
      console.log('[JOB_SEARCH_V2] Raw content preview:', out.content.slice(0, 500))
      
      let parsed: JobListing[] = []
      
      try {
        // Try parseAIResponse which has 8 strategies including line-by-line
        parsed = parseAIResponse<JobListing[]>(out.content, { 
          stripMarkdown: true, 
          extractFirst: true,
          allowPartial: true,
          throwOnError: false
        })
        
        console.log('[JOB_SEARCH_V2] ✅ Parsing succeeded:', {
          isArray: Array.isArray(parsed),
          count: Array.isArray(parsed) ? parsed.length : 0,
          firstJob: parsed[0] ? { title: parsed[0].title, company: parsed[0].company } : null
        })
      } catch (parseError) {
        console.error('[JOB_SEARCH_V2] ❌ All parsing strategies failed:', {
          error: (parseError as Error).message,
          contentPreview: out.content.slice(0, 1000)
        })
        
        // Last resort: return empty array
        parsed = []
      }
      
      parsed = Array.isArray(parsed) ? parsed.slice(0, options.maxResults || 25) : []
      
      if (parsed.length === 0) {
        console.warn('[JOB_SEARCH_V2] ⚠️ WARNING: Perplexity returned 0 jobs. This might indicate:')
        console.warn('  1. No jobs found for this search')
        console.warn('  2. Perplexity did not perform web search')
        console.warn('  3. Response format is incorrect')
        console.warn('  Content received:', out.content.slice(0, 1000))
      }
      
      // CRITICAL FIX: Enrich jobs with short descriptions by scraping URLs
      const enriched = await Promise.all(
        parsed.map(async (job) => {
          if (job.summary && job.summary.length < 150 && job.url) {
            if (process.env.PPX_DEBUG === 'true') {
              console.log(`[ENRICH] Scraping ${job.url} for full description...`)
            }
            const fullDescription = await this.scrapeJobURL(job.url)
            if (fullDescription) {
              return { ...job, summary: fullDescription }
            }
          }
          return job
        })
      )
      
      // CRITICAL FIX: Validate job listings after enrichment
      parsed = this.validateJobListings(enriched, options.maxResults || 25)
      
      // Enhance and normalize
      parsed = parsed.map(j => ({
        ...j,
        skills: normalizeSkills(j.skills || []),
        skillMatchPercent: Math.max(0, Math.min(100, j.skillMatchPercent || 0)),
        workType: j.workType || 'onsite',
        experienceLevel: j.experienceLevel || 'mid',
        source: j.source || (typeof j.url === 'string' ? (new URL(j.url)).hostname.replace(/^www\./,'') : undefined),
        benefits: j.benefits || [],
        requirements: j.requirements || [],
        metadata: {
          searchedBoards: targetBoards.length,
          isCanadianSearch: isCanadian,
          extractedAt: new Date().toISOString()
        }
      }))

      // Sort by match quality, then recency
      parsed.sort((a,b)=>{
        if (Math.abs(a.skillMatchPercent - b.skillMatchPercent) > 5) {
          return b.skillMatchPercent - a.skillMatchPercent
        }
        return new Date(b.postedDate).getTime() - new Date(a.postedDate).getTime()
      })

      setCache(key, parsed)
      return { 
        success: true, 
        data: parsed,
        metadata: { 
          requestId, 
          timestamp: started, 
          duration: Date.now() - started,
          boardsSearched: targetBoards.length,
          resultsCount: parsed.length
        }, 
        cached: false 
      }
    } catch (e) {
      console.error('[JOB_SEARCH_ERROR] Job search failed:', {
        error: (e as Error).message,
        stack: (e as Error).stack,
        location,
        roleHint: options.roleHint,
        boards: targetBoards.slice(0, 5)
      })
      
      return { 
        success: false, 
        data: [], 
        metadata: { 
          requestId, 
          timestamp: started, 
          duration: Date.now() - started, 
          error: (e as Error).message 
        }, 
        cached: false 
      }
    }
  }

  // V2: Enhanced hiring contacts with email enrichment and discovery
  static async hiringContactsV2(companyName: string): Promise<EnhancedResponse<HiringContact[]>> {
    const requestId = generateRequestId()
    const started = Date.now()
    const key = makeKey('ppx:contacts:v2', { companyName })
    const cached = getCache(key) as HiringContact[] | undefined
    if (cached) return { success: true, data: cached, metadata: { requestId, timestamp: started, duration: Date.now() - started }, cached: true }
    try {
      const out = await withRetry(async () => {
        const client = createClient()
        
        // PERPLEXITY AUDIT FIX: Use optimal configuration
        const { getPerplexityConfig } = await import('./config/perplexity-configs')
        const config = getPerplexityConfig('hiringContacts')
        
        // ULTRA-AGGRESSIVE: Multi-platform exhaustive contact scraping
        const prompt = `HIRING CONTACTS FOR ${companyName}

YOUR TASK: Find 3-5 real hiring contacts (recruiters, HR, talent acquisition) at ${companyName}.

SEARCH THESE PLACES (in order of priority):
1. ${companyName} careers page (/careers, /jobs) - look for "recruiting team" or "contact us"
2. LinkedIn - search "${companyName} recruiter" or "${companyName} talent acquisition"
3. Company website "Team" or "About Us" page
4. ${companyName} company directory or contact page

WHAT TO EXTRACT:
- Full name (REAL names only, not "HR Department")
- Job title (must include recruiter/HR/talent)
- Email address (ONLY if publicly listed)
- LinkedIn profile URL (if available)

CRITICAL RULES:
1. ONLY include contacts you can SEE on public pages
2. DO NOT make up or guess email addresses
3. DO NOT use generic emails like info@, hello@, support@
4. If you find 0 verified contacts, return empty array []
5. Each contact MUST have either a real email OR LinkedIn URL

RETURN THIS JSON (no markdown):
[
  {
    "name": "Sarah Johnson",
    "title": "Senior Technical Recruiter",
    "email": "sarah.johnson@company.com",
    "linkedinUrl": "https://linkedin.com/in/sarahjohnson",
    "source": "Company careers page",
    "confidence": 0.95
  }
]

If NO contacts found, return: []`

        // PERPLEXITY AUDIT FIX: Use optimal token limits + sonar-pro for research
        return client.makeRequest(SYSTEM, prompt, { 
          temperature: config.temperature, 
          maxTokens: config.maxTokens,
          model: 'sonar-pro' // Use research model for multi-source search
        })
      })
      
      // CRITICAL DEBUG: Log raw Perplexity output (Perplexity recommendation)
      if (process.env.PPX_DEBUG === 'true') {
        console.log('[PERPLEXITY RAW]', {
          method: 'hiringContactsV2',
          company: companyName,
          contentLength: out.content.length,
          contentPreview: out.content.slice(0, 500)
        })
      }
      
      // Parse and clean Perplexity response - ENTERPRISE-GRADE JSON EXTRACTION
      let cleanedContent = out.content.trim()
      
      // Step 1: Remove markdown code blocks
      cleanedContent = cleanedContent.replace(/^```(?:json)?\s*/gm, '').replace(/```\s*$/gm, '')
      
      // Step 2: Extract JSON array from any surrounding text
      const jsonMatch = cleanedContent.match(/\[[\s\S]*?\]/);
      if (jsonMatch) {
        cleanedContent = jsonMatch[0]
      } else {
        // Step 3: If no array found, check for explanatory text with JSON after it
        const afterTextMatch = cleanedContent.match(/(?:Here|I found|Below|Results?)[\s\S]*?(\[[\s\S]*?\])/i);
        if (afterTextMatch) {
          cleanedContent = afterTextMatch[1]
        } else {
          console.warn('[HIRING_CONTACTS] No JSON array found in response, returning empty array')
          return { success: false, data: [], metadata: { requestId, timestamp: started, duration: Date.now() - started, error: 'No JSON array in response' }, cached: false }
        }
      }
      
      // PERPLEXITY AUDIT FIX: Use enterprise-grade JSON extraction
      const { extractEnterpriseJSON } = await import('./utils/enterprise-json-extractor')
      const extractionResult = extractEnterpriseJSON(cleanedContent)
      
      if (!extractionResult.success) {
        console.error('[HIRING_CONTACTS] Enterprise JSON extraction failed:', extractionResult.error)
        console.error('[HIRING_CONTACTS] Attempted cleanups:', extractionResult.attemptedCleanups)
        console.error('[HIRING_CONTACTS] Raw content preview:', out.content.slice(0, 500))
        return { 
          success: false, 
          data: [], 
          metadata: { 
            requestId, 
            timestamp: started, 
            duration: Date.now() - started, 
            error: `Enterprise JSON extraction failed: ${extractionResult.error}`,
            attemptedCleanups: extractionResult.attemptedCleanups
          }, 
          cached: false 
        }
      }
      
      // CRITICAL FIX: ALWAYS ensure we have an array (never undefined/null)
      let parsed: HiringContact[] = []
      
      if (Array.isArray(extractionResult.data)) {
        parsed = extractionResult.data.slice(0, 8)
      } else if (extractionResult.data && typeof extractionResult.data === 'object') {
        // Handle case where AI returns single object instead of array
        parsed = [extractionResult.data]
      }
      
      // Enterprise extraction succeeded
      
      // CRITICAL: Validate and filter contacts - reject fake/personal emails
      const personalDomains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'aol.com', 'icloud.com', 'protonmail.com']
      parsed = parsed.filter(contact => {
        // Must have at least one contact method
        if (!contact.email && !contact.phone && !contact.linkedinUrl) {
          console.warn(`[HIRING_CONTACTS] Rejected contact with no contact method: ${contact.name}`)
          return false
        }
        
        // Reject inferred/template emails
        if (contact.email?.includes('[') || 
            contact.email?.includes('example.') || 
            contact.email?.includes('domain.') ||
            contact.email?.includes('VISIT_WEBSITE')) {
          console.warn(`[HIRING_CONTACTS] Rejected template email: ${contact.email}`)
          return false
        }
        
        // Reject personal emails
        if (contact.email && personalDomains.some(d => contact.email!.toLowerCase().endsWith(d))) {
          console.warn(`[HIRING_CONTACTS] Rejected personal email: ${contact.email}`)
          return false
        }
        
        // Reject LinkedIn profiles without proper URL
        if (contact.linkedinUrl && !contact.linkedinUrl.includes('linkedin.com/')) {
          console.warn(`[HIRING_CONTACTS] Rejected invalid LinkedIn URL: ${contact.linkedinUrl}`)
          return false
        }
        
        return true
      })
      
      // Validation complete
      
      // Enhance each contact with metadata
      parsed = parsed.map(c => {
        const domain = `${companyName.toLowerCase().replace(/\s+/g,'').replace(/[^a-z0-9]/g,'')}.com`
        const inferred = c.name ? inferEmails(c.name, domain) : []
        
        return { 
          ...c, 
          confidence: Math.max(0, Math.min(1, c.confidence || 0.5)), 
          alternativeEmails: c.alternativeEmails || inferred, 
          emailType: (c.email ? c.emailType : 'pattern') as 'public'|'inferred'|'pattern',
          discoveryMethod: c.discoveryMethod || (c.email ? 'Direct lookup' : 'Pattern inference')
        }
      })
      
      // Final result prepared
      
      // CRITICAL FIX: Validate contacts before returning
      const validated = this.validateHiringContacts(parsed)
      
      // CRITICAL FIX: NO INFERRED EMAILS - return empty if none verified
      // User should visit company website or use LinkedIn instead of contacting fake emails
      const finalContacts = validated
      
      // Cache the result (even if empty)
      setCache(key, finalContacts)
      
      return { 
        success: validated.length > 0, 
        data: finalContacts, 
        metadata: { 
          requestId, 
          timestamp: started, 
          duration: Date.now() - started,
          contactsFound: finalContacts.length,
          withEmails: finalContacts.filter(c => c.email).length,
          error: validated.length === 0 
            ? `No verified hiring contacts found for ${companyName}. Visit company website or use LinkedIn InMail.` 
            : undefined
        }, 
        cached: false 
      }
    } catch (e) {
      console.error('[HIRING_CONTACTS] Error:', e)
      return { success: false, data: [], metadata: { requestId, timestamp: started, duration: Date.now() - started, error: (e as Error).message }, cached: false }
    }
  }

  // ... (rest of the code remains the same)

  // Extract normalized keywords and location from resume (STRICT JSON)
  static async extractResumeSignals(
    resumeText: string,
    maxKeywords: number = 50
  ): Promise<{ 
    keywords: string[]
    skillsWeighted?: {
      primarySkills: Array<{ skill: string; weight: number; years?: number | null; category: string }>
      secondarySkills: Array<{ skill: string; weight: number; years?: number | null; category: string }>
    }
    location?: string
    locations?: string[]
    personalInfo?: { name?: string; email?: string; phone?: string }
  }> {
    const key = makeKey('ppx:resume:signals:v3', { t: resumeText.slice(0, 3000), maxKeywords })
    const cached = getCache(key) as { keywords: string[]; location?: string; locations?: string[] } | undefined
    if (cached) return cached

    try {
      const client = createClient()
      
      const prompt = `ANALYZE THIS RESUME TEXT AND EXTRACT WEIGHTED SKILLS + LOCATION

CRITICAL: DO NOT search the web. ONLY read the resume text below.

RESUME TEXT:
${resumeText}

TASK 1 - EXTRACT WEIGHTED SKILLS:
Analyze the resume and extract skills with the following details:

PRIMARY SKILLS (Top 15 most important):
- Skills mentioned in recent jobs (last 3 years)
- Skills in job titles or prominently featured
- Skills with explicit years of experience mentioned
- Weight: 0.7 - 1.0 (based on recency, frequency, prominence)

SECONDARY SKILLS (Next 20 skills):
- Skills mentioned but less prominent
- Skills from older jobs or education
- Skills mentioned once or twice
- Weight: 0.3 - 0.69

For each skill, determine:
1. Skill name (e.g., "React", "Python", "Project Management")
2. Weight (0.0 - 1.0, where 1.0 = most important)
3. Years of experience (estimate from work history, or null if unclear)
4. Category: "technical", "soft", "language", or "tool"

WEIGHTING RULES:
- Recent job (last 2 years) + mentioned in title = 0.9-1.0
- Recent job + mentioned multiple times = 0.8-0.9
- Mentioned in multiple jobs = 0.7-0.8
- Mentioned once in recent job = 0.6-0.7
- Older job or education only = 0.3-0.5

TASK 2 - EXTRACT LOCATION FROM RESUME:
CRITICAL RULES:
- Read the resume text above carefully
- Look for location in: contact info section, address, current location
- Extract the SPECIFIC CITY and PROVINCE/STATE (e.g., "Edmonton, AB" or "Toronto, ON")
- DO NOT return just "Canada" - that's too broad
- DO NOT search the web for location - ONLY use what's in the resume text
- If you find "Edmonton, Alberta" write it as "Edmonton, AB"
- If you find "Toronto, Ontario" write it as "Toronto, ON"
- If no specific city found, return null

RETURN STRICT JSON (no markdown, no explanations):
{
  "keywords": ["Skill1", "Skill2", ..., "Skill35"],
  "skillsWeighted": {
    "primarySkills": [
      {
        "skill": "React",
        "weight": 0.95,
        "years": 5,
        "category": "technical"
      },
      {
        "skill": "Leadership",
        "weight": 0.85,
        "years": 3,
        "category": "soft"
      }
    ],
    "secondarySkills": [
      {
        "skill": "Python",
        "weight": 0.60,
        "years": 2,
        "category": "technical"
      }
    ]
  },
  "location": "City, PROVINCE" or null,
  "personalInfo": {
    "name": "Full Name",
    "email": "email@example.com",
    "phone": "555-1234"
  }
}

CRITICAL: 
- Return 15 primarySkills and 20 secondarySkills
- Keep "keywords" array for backward compatibility (top 35 skills)
- Extract location FROM THE RESUME TEXT ONLY (not from web search)
- Location must be "City, Province" format or null
- Weight must be between 0.0 and 1.0
- Years can be null if not determinable`

      console.log('[SIGNALS] Extracting resume signals...')

      const response = await client.makeRequest(
        'You are a resume parser. Extract data from the resume text provided. DO NOT search the web. Return only JSON.',
        prompt,
        { temperature: 0.1, maxTokens: 2500, model: 'sonar' }
      )

      if (process.env.PPX_DEBUG === 'true') {
        console.log('[SIGNALS] Raw response:', response.content?.slice(0, 400))
      }

      // ENTERPRISE FIX: Strip markdown code blocks that Perplexity sometimes adds
      let cleanedContent = response.content.trim()
      
      // Remove markdown code fences (```json ... ``` or ``` ... ```)
      cleanedContent = cleanedContent.replace(/^```(?:json)?\s*/gm, '').replace(/```\s*$/gm, '')
      
      // Extract JSON array/object if wrapped in explanatory text
      const jsonMatch = cleanedContent.match(/(\{[\s\S]*\}|\[[\s\S]*\])/);
      if (jsonMatch) {
        cleanedContent = jsonMatch[0]
      }

      const parsed = JSON.parse(cleanedContent) as { keywords: string[]; location?: string; locations?: string[]; personalInfo?: { name?: string; email?: string; phone?: string } }
      
      if (process.env.PPX_DEBUG === 'true') {
        console.log('[SIGNALS] Parsed:', {
          keywordCount: parsed.keywords?.length,
          location: parsed.location,
          hasLocations: !!parsed.locations,
          personalInfo: parsed.personalInfo
        })
      }

      setCache(key, parsed)
      return parsed
    } catch (error) {
      console.error('═══════════════════════════════════════════════════════')
      console.error('[EXTRACT SIGNALS] ❌ PERPLEXITY EXTRACTION FAILED')
      console.error('═══════════════════════════════════════════════════════')
      console.error('[EXTRACT SIGNALS] Error:', (error as Error).message)
      console.error('[EXTRACT SIGNALS] Resume text length:', resumeText.length, 'chars')
      console.error('[EXTRACT SIGNALS] First 300 chars of resume:')
      console.error(resumeText.substring(0, 300))
      console.error('═══════════════════════════════════════════════════════')
      
      // CRITICAL: Don't return fake data - throw error so upload route can handle it
      throw new Error(`Failed to extract resume signals: ${(error as Error).message}. Resume may be missing contact information or is corrupted.`)
    }
  }

  // ... (rest of the code remains the same)

  /**
   * ONE-SHOT COMPREHENSIVE RESEARCH
   * Replaces multiple API calls with a single comprehensive prompt
   * Returns: Job Analysis + Company Research + Hiring Contacts + News + Reviews
   * 
   * @param params - Job and resume details
   * @returns Complete research data for all Career Finder pages
   */
  static async comprehensiveJobResearch(params: {
    jobTitle: string
    company: string
    jobDescription: string
    location?: string
    resumeText: string
    resumeSkills?: string[]
  }): Promise<EnhancedResponse<ComprehensiveJobResearchData | null>> {
    const requestId = generateRequestId()
    const started = Date.now()

    try {
      const client = createClient()

      const prompt = `COMPREHENSIVE JOB APPLICATION RESEARCH

- Position: ${params.jobTitle}
- Company: ${params.company}
- Location: ${params.location || 'Not specified'}
- Description: ${params.jobDescription.slice(0, 1000)}

CANDIDATE SKILLS: ${params.resumeSkills ? params.resumeSkills.slice(0, 20).join(', ') : 'Extract from resume below'}

RESUME TEXT (First 2000 chars):
${params.resumeText.slice(0, 2000)}

---

YOUR MISSION: Conduct a comprehensive research report covering ALL of the following sections. This is a ONE-TIME research call, so be thorough and detailed. Include clickable URLs wherever possible.

OUTPUT FORMAT (Valid JSON ONLY):
\`\`\`json
{
  "jobAnalysis": {
    "matchScore": 85,
    "matchingSkills": ["skill1", "skill2"],
    "missingSkills": ["skill3", "skill4"],
    "skillsToHighlight": ["top skill to emphasize"],
    "recommendations": ["specific action 1", "specific action 2"],
    "estimatedFit": "Excellent|Good|Moderate|Poor"
  },
  "companyIntel": {
    "company": "${params.company}",
    "description": "detailed company overview (minimum 200 chars)",
    "size": "employee count or range",
    "revenue": "annual revenue if public",
    "industry": "primary industry",
    "founded": "year",
    "headquarters": "city, state/country",
    "website": "https://company.com",
    "marketPosition": "market leader|challenger|niche player",
    "generalEmail": "ONLY include if found on company website or LinkedIn - DO NOT GUESS. Leave empty if not found.",
    "careersPage": "https://company.com/careers"
  },
  "companyPsychology": {
    "culture": "detailed culture description based on reviews and public info",
    "values": ["value1", "value2", "value3"],
    "managementStyle": "hierarchical|flat|hybrid",
    "workEnvironment": "remote-friendly|hybrid|office-centric"
  },
  "hiringContacts": [
    {
      "name": "Real Person Name - ONLY if found on LinkedIn or company website",
      "title": "Talent Acquisition Manager",
      "department": "Human Resources",
      "email": "ONLY include if verified from LinkedIn or company website - DO NOT GUESS. Leave empty if not found.",
      "linkedinUrl": "https://linkedin.com/in/person - ONLY if found",
      "authority": "decision maker",
      "confidence": 0.9
    }
  ],
  "CRITICAL_INSTRUCTION": "DO NOT GUESS EMAILS. Only include emails that are explicitly found on the company website, LinkedIn profiles, or other verified sources. If no email is found, leave the field empty or set to null. NEVER construct emails like info@company.com or careers@company.com unless they are explicitly listed on official sources.",
  "marketIntelligence": {
    "competitivePosition": "how company compares to competitors",
    "industryTrends": "relevant industry trends affecting this role",
    "financialStability": "financial health assessment",
    "recentPerformance": "last 12 months highlights"
  },
  "news": [
    {
      "title": "Recent news headline",
      "summary": "Brief summary of the article",
      "url": "https://newsource.com/article",
      "date": "2024-01-15",
      "source": "TechCrunch",
      "impact": "positive|neutral|negative for employment"
    }
  ],
  "reviews": [
    {
      "platform": "Glassdoor",
      "rating": 4.2,
      "summary": "Overall employee sentiment summary",
      "url": "https://glassdoor.com/company-reviews",
      "pros": ["pro1", "pro2"],
      "cons": ["con1", "con2"]
    }
  ],
  "compensation": {
    "salaryRange": "$XX,000 - $YY,000 for ${params.jobTitle}",
    "benefits": "typical benefits package"
  },
  "strategicRecommendations": {
    "applicationStrategy": "specific advice on how to apply",
    "contactStrategy": "who to contact first and how",
    "interviewPrep": ["prepare for X", "research Y", "practice Z"]
  },
  "sources": ["https://source1.com", "https://source2.com", "https://source3.com"],
  "confidenceLevel": 0.85
}
\`\`\`

CRITICAL REQUIREMENTS:
1. Job Analysis: Compare resume skills to job requirements, calculate match score
2. Company Intel: Search company website, LinkedIn, Crunchbase, Wikipedia for REAL data
   - MUST find general company email (careers@, hr@, jobs@, info@, contact@)
   - Check company website contact page, footer, careers page
   - If no email found, generate likely addresses based on domain
3. Hiring Contacts: **CRITICAL - MUST FIND CONTACTS**
   - Search LinkedIn, Twitter, Facebook, Instagram, company website
   - Minimum 2-3 REAL hiring contacts if company has 10+ employees
   - Include verified LinkedIn URLs and emails where possible
   - DO NOT return fake/placeholder names
   - **MANDATORY FALLBACK**: If no hiring contacts found, extract company general inbox:
     * Check: careers@, hr@, jobs@, info@, hello@, contact@, support@
     * Return as: {"name":"General Inbox","title":"Company Contact","email":"found@company.com"}
   - NEVER return empty contacts array - app is useless without contact info
4. News: Find 2-5 recent news articles about the company with clickable URLs
5. Reviews: Search Glassdoor, Indeed, Comparably for employee reviews with clickable URLs
6. Market Intelligence: Research industry trends, competitive landscape
7. Strategic Recommendations: Provide actionable, company-specific advice

IMPORTANT:
- Return ONLY valid JSON (no markdown, no explanations)
- All URLs must be real and clickable
- If data not found after searching, use "Not available" but ALWAYS try multiple sources first
- Focus on actionable intelligence, not generic advice`

      const out = await withRetry(async () => {
        return client.makeRequest(
          'You are an elite corporate intelligence analyst providing comprehensive job application research. Return detailed JSON with all requested fields.',
          prompt,
          {
            temperature: 0.2,
            maxTokens: 8000,
            model: 'sonar-pro'
          }
        )
      })

      if (process.env.PPX_DEBUG === 'true') {
        console.log('[COMPREHENSIVE_RESEARCH] Raw response length:', out.content.length)
      }

      // Parse response
      let cleanedContent = out.content.trim()
      
      // Remove markdown code blocks
      cleanedContent = cleanedContent.replace(/^```(?:json)?\s*/gm, '').replace(/```\s*$/gm, '')
      
      // Extract JSON object
      const jsonMatch = cleanedContent.match(/\{[\s\S]*\}/)
      if (jsonMatch) {
        cleanedContent = jsonMatch[0]
      }

      const parsed = JSON.parse(cleanedContent) as Partial<ComprehensiveJobResearchData>

      // Construct with fallbacks
      const data: ComprehensiveJobResearchData = {
        jobAnalysis: {
          matchScore: parsed.jobAnalysis?.matchScore ?? 0,
          matchingSkills: parsed.jobAnalysis?.matchingSkills ?? [],
          missingSkills: parsed.jobAnalysis?.missingSkills ?? [],
          skillsToHighlight: parsed.jobAnalysis?.skillsToHighlight ?? [],
          recommendations: parsed.jobAnalysis?.recommendations ?? [],
          estimatedFit: parsed.jobAnalysis?.estimatedFit ?? 'Unknown'
        },
        companyIntel: {
          company: parsed.companyIntel?.company ?? params.company,
          description: parsed.companyIntel?.description ?? 'No description available',
          size: parsed.companyIntel?.size ?? 'Unknown',
          revenue: parsed.companyIntel?.revenue,
          industry: parsed.companyIntel?.industry ?? 'Unknown',
          founded: parsed.companyIntel?.founded,
          headquarters: parsed.companyIntel?.headquarters,
          website: parsed.companyIntel?.website,
          marketPosition: parsed.companyIntel?.marketPosition
        },
        companyPsychology: {
          culture: parsed.companyPsychology?.culture ?? 'No information available',
          values: parsed.companyPsychology?.values ?? [],
          managementStyle: parsed.companyPsychology?.managementStyle,
          workEnvironment: parsed.companyPsychology?.workEnvironment
        },
        hiringContacts: Array.isArray(parsed.hiringContacts)
          ? parsed.hiringContacts
              .map(contact => ({
                name: contact.name,
                title: contact.title,
                department: contact.department,
                email: contact.email,
                linkedinUrl: contact.linkedinUrl,
                authority: contact.authority ?? 'manager',
                confidence: contact.confidence ?? 0,
                contactMethod: contact.contactMethod
              }))
              .filter(contact => !!contact?.name && contact?.title)
          : [],
        marketIntelligence: {
          competitivePosition: parsed.marketIntelligence?.competitivePosition,
          industryTrends: parsed.marketIntelligence?.industryTrends,
          financialStability: parsed.marketIntelligence?.financialStability,
          recentPerformance: parsed.marketIntelligence?.recentPerformance
        },
        news: Array.isArray(parsed.news)
          ? parsed.news
              .map(item => (item?.title && item?.summary && item?.url
                ? {
                    title: item.title,
                    summary: item.summary,
                    url: item.url,
                    date: item.date,
                    source: item.source,
                    impact: item.impact
                  }
                : undefined))
              .filter((item): item is NonNullable<typeof item> => !!item)
          : [],
        reviews: Array.isArray(parsed.reviews)
          ? parsed.reviews
              .map(item => (item?.platform && item?.summary && item?.url
                ? {
                    platform: item.platform,
                    rating: item.rating,
                    summary: item.summary,
                    url: item.url,
                    pros: item.pros,
                    cons: item.cons
                  }
                : undefined))
              .filter((item): item is NonNullable<typeof item> => !!item)
          : [],
        compensation: parsed.compensation ?? {},
        strategicRecommendations: {
          applicationStrategy: parsed.strategicRecommendations?.applicationStrategy ?? 'Apply through company website',
          contactStrategy: parsed.strategicRecommendations?.contactStrategy ?? 'Reach out to HR via LinkedIn',
          interviewPrep: parsed.strategicRecommendations?.interviewPrep ?? []
        },
        sources: Array.isArray(parsed.sources)
          ? parsed.sources.filter((source): source is string => typeof source === 'string')
          : [],
        confidenceLevel: parsed.confidenceLevel ?? 0.5
      }

      if (process.env.PPX_DEBUG === 'true') {
        console.log('[COMPREHENSIVE_RESEARCH] Complete -', 
          'matchScore:', data.jobAnalysis.matchScore, 
          'contacts:', data.hiringContacts.length, 
          'news:', data.news.length, 
          'reviews:', data.reviews.length, 
          'confidence:', data.confidenceLevel
        )
      }

      return {
        success: true,
        data,
        metadata: { requestId, timestamp: started, duration: Date.now() - started },
        cached: false
      }
    } catch (error) {
      console.error('[COMPREHENSIVE_RESEARCH] Error:', error)
      return {
        success: false,
        data: null,
        metadata: { 
          requestId, 
          timestamp: started, 
          duration: Date.now() - started,
          error: (error as Error).message 
        },
        cached: false
      }
    }
  }

  // Resume Optimizer: Generate tailored resume variants
  static async generateResumeVariants(params: {
    resumeText: string
    jobTitle: string
    jobRequirements: string[]
    companyInsights: { culture: string; values: string[]; industry: string }
    template?: string
  }): Promise<EnhancedResponse<{
    variantA: string
    variantB: string
    recommendations: string[]
  }>> {
    const requestId = generateRequestId()
    const started = Date.now()
    const cacheKey = makeKey('resume-variants', params)
    
    const cached = getCache(cacheKey)
    if (cached) {
      return {
        success: true,
        data: cached as { variantA: string; variantB: string; recommendations: string[] },
        metadata: { requestId, timestamp: started, duration: 0 },
        cached: true
      }
    }

    try {
      const client = createClient()
      const systemPrompt = 'You are a professional resume optimization expert. Return only valid JSON with properly formatted resume text.'
      
      // Build template-specific instructions
      const templateInstructions = {
        modern: 'Use a contemporary style with visual hierarchy. Emphasize innovation and forward-thinking achievements.',
        professional: 'Use traditional, formal language. Focus on stability, reliability, and proven track record.',
        creative: 'Use dynamic language and unique phrasing. Highlight creativity, innovation, and out-of-the-box thinking.',
        tech: 'Use technical terminology and emphasize projects, technologies, and technical achievements.',
        minimal: 'Use simple, direct language. Focus on facts and quantifiable results. Maximum ATS compatibility.',
        executive: 'Use leadership language. Emphasize strategic impact, team leadership, and business results.'
      }
      
      const templateStyle = templateInstructions[params.template as keyof typeof templateInstructions] || templateInstructions.modern
      
      const userPrompt = `Analyze this resume and create TWO tailored variants for the target role using the ${params.template} template style.

**Resume:**
${params.resumeText}

**Target Role:** ${params.jobTitle}

**Key Requirements:**
${params.jobRequirements.map((req, i) => `${i + 1}. ${req}`).join('\n')}

**Company Culture:** ${params.companyInsights.culture}
**Company Values:** ${params.companyInsights.values.join(', ')}
**Industry:** ${params.companyInsights.industry}

**Template Style (${params.template}):** ${templateStyle}

Generate TWO resume variants:
1. **Variant A (Achievement-Focused):** Emphasize quantifiable achievements and metrics. ${templateStyle}
2. **Variant B (Skills-Focused):** Highlight technical skills and competencies. ${templateStyle}

CRITICAL FORMATTING REQUIREMENTS:
- Use proper line breaks (\\n\\n for sections, \\n for lines)
- DO NOT include name, email, phone, or address in the resume body
- Personal contact info will be added separately by the template
- Start directly with PROFESSIONAL SUMMARY or first section
- Use clear section headers (PROFESSIONAL SUMMARY, EXPERIENCE, EDUCATION, SKILLS)
- Format each job entry with: Title\\nCompany | Location | Dates\\n• Achievement 1\\n• Achievement 2
- Keep bullet points aligned with • symbol
- Ensure proper spacing between sections
- NO markdown formatting (no **, no #, no _)
- Plain text only with line breaks
- INCLUDE ALL job history from original resume

CRITICAL - PERSONAL INFO:
- DO NOT include the person's name anywhere in the resume body
- DO NOT include email address in the resume body
- DO NOT include phone number in the resume body
- DO NOT include physical address in the resume body
- These will be added by the template header automatically
- Start the resume body with the PROFESSIONAL SUMMARY section

For each variant, rewrite the resume to:
- Match keywords from job requirements
- Align with company culture and values
- Use industry-specific terminology appropriate for ${params.template} template
- Optimize for ATS (Applicant Tracking Systems)
- Keep formatting clean and professional
- Apply ${params.template} template style throughout
- NEVER duplicate personal contact information

Also provide 3-5 strategic recommendations for improving the resume.

Return ONLY valid JSON:
{
  "variantA": "Full resume text WITHOUT personal info (starts with PROFESSIONAL SUMMARY)...",
  "variantB": "Full resume text WITHOUT personal info (starts with PROFESSIONAL SUMMARY)...",
  "recommendations": ["Recommendation 1", "Recommendation 2", ...]
}`

      const response = await withRetry(
        () => client.makeRequest(systemPrompt, userPrompt, { temperature: 0.2, maxTokens: 4000, model: 'sonar-pro' }),
        MAX_RETRY_ATTEMPTS
      )

      const parsed = parseAIResponse<{
        variantA: string
        variantB: string
        recommendations: string[]
      }>(response.content)

      const data = {
        variantA: parsed.variantA || params.resumeText,
        variantB: parsed.variantB || params.resumeText,
        recommendations: Array.isArray(parsed.recommendations) ? parsed.recommendations : []
      }

      setCache(cacheKey, data)

      return {
        success: true,
        data,
        metadata: { requestId, timestamp: started, duration: Date.now() - started },
        cached: false
      }
    } catch (error) {
      console.error('[RESUME_VARIANTS] Error:', error)
      return {
        success: false,
        data: {
          variantA: params.resumeText,
          variantB: params.resumeText,
          recommendations: []
        },
        metadata: { 
          requestId, 
          timestamp: started, 
          duration: Date.now() - started,
          error: (error as Error).message 
        },
        cached: false
      }
    }
  }

  // Cover Letter Generator: Create personalized cover letters using templates
  static async generateCoverLetters(params: {
    jobTitle: string
    company: string
    jobRequirements: string[]
    resumeText: string
    companyInsights: {
      culture: string
      values: string[]
      recentNews: Array<{ title: string; summary: string }>
    }
    hiringManager?: { name: string; title: string }
    userName?: string
    templateId?: string
  }): Promise<EnhancedResponse<{
    variantA: string
    variantB: string
    personalization: string[]
  }>> {
    const requestId = generateRequestId()
    const started = Date.now()
    const cacheKey = makeKey('cover-letters', params)
    
    const cached = getCache(cacheKey)
    if (cached) {
      return {
        success: true,
        data: cached as { variantA: string; variantB: string; personalization: string[] },
        metadata: { requestId, timestamp: started, duration: 0 },
        cached: true
      }
    }

    try {
      // CRITICAL FIX: Calculate years of experience to prevent hallucinations
      const yearsExperience = calculateYearsFromResume(params.resumeText)
      if (process.env.PPX_DEBUG === 'true') {
        console.log('[COVER_LETTERS] Calculated experience:', yearsExperience, 'years')
      }

      // CRITICAL FIX: Get 2 cover letter templates that match the resume template
      const { getCoverLetterVariantsForResume } = await import('./cover-letter-templates')
      const [templateA, templateB] = getCoverLetterVariantsForResume(params.templateId || 'professional')
      
      if (process.env.PPX_DEBUG === 'true') {
        console.log('[COVER_LETTERS] Using templates:', templateA.name, '+', templateB.name, 'for resume template:', params.templateId)
      }

      const client = createClient()
      const systemPrompt = `You are an expert cover letter writer. Use the provided templates as structure guides and fill them with personalized content from the candidate's resume.

CRITICAL EXPERIENCE CONSTRAINT:
- Candidate has EXACTLY ${yearsExperience} years of total work experience
- DO NOT say "decades", "38 years", or any number higher than ${yearsExperience}
- If ${yearsExperience} < 10, say "several years" or "${yearsExperience} years"
- If ${yearsExperience} >= 10 && ${yearsExperience} < 20, say "${yearsExperience} years" or "over a decade"
- If ${yearsExperience} >= 20, say "${yearsExperience} years" or "two decades"
- NEVER invent or exaggerate experience duration
- Use ONLY the experience data provided in the resume

Return only valid JSON.`

    const userPrompt = `GENERATE TWO COVER LETTER VARIANTS

CRITICAL: The candidate is APPLYING TO ${params.company}. They do NOT currently work there.

**CANDIDATE INFO:**
- Name: ${params.userName || '[Your Name]'}
- Years of Experience: EXACTLY ${yearsExperience} years
- Applying For: ${params.jobTitle} at ${params.company}

**TEMPLATE A - ${templateA.name} (${templateA.tone} tone):**
Best for: ${templateA.bestFor.join(', ')}

STRUCTURE TO FOLLOW:
${templateA.template}

**TEMPLATE B - ${templateB.name} (${templateB.tone} tone):**
Best for: ${templateB.bestFor.join(', ')}

STRUCTURE TO FOLLOW:
${templateB.template}

**JOB DETAILS:**
- Position: ${params.jobTitle}
- Company: ${params.company}
- Hiring Manager: ${params.hiringManager?.name || 'Hiring Manager'}

**KEY REQUIREMENTS TO ADDRESS:**
${params.jobRequirements.map((req, i) => `${i + 1}. ${req}`).join('\n')}

**CANDIDATE'S RESUME (${yearsExperience} years total experience):**
${params.resumeText.slice(0, 1500)}

**COMPANY RESEARCH:**
- Culture: ${params.companyInsights.culture}
- Values: ${params.companyInsights.values.join(', ')}
- Recent News: ${params.companyInsights.recentNews.map(n => n.title).join(', ')}

**INSTRUCTIONS:**

For VARIANT A (${templateA.name}):
1. Follow Template A's structure EXACTLY
2. Replace ALL placeholders: [Your Name], [Job Title], [Company Name], [X years], [Skill 1], [Achievement], etc.
3. Use ${templateA.tone} tone
4. Fill with REAL data from the resume
5. Match the template's paragraph structure

For VARIANT B (${templateB.name}):
1. Follow Template B's structure EXACTLY
2. Replace ALL placeholders with real data
3. Use ${templateB.tone} tone
4. Keep the template's formatting (bullets, paragraphs, etc.)

**CRITICAL RULES:**
✓ Experience: Say EXACTLY "${yearsExperience} years" (no more, no less)
✓ Opening: "I am excited to apply for..." or "I am writing to express my interest in..."
✗ NEVER: "As a ${params.jobTitle} at ${params.company}..." (they don't work there yet!)
✗ NEVER: "proven track record" without specific metrics
✗ NEVER: Invent achievements not in resume
✓ Use ONLY real achievements from resume with actual numbers
✓ Match each template's tone and structure

Return ONLY valid JSON:
{
  "variantA": "Complete cover letter following Template A structure with all placeholders replaced...",
  "variantB": "Complete cover letter following Template B structure with all placeholders replaced...",
  "personalization": ["Tip 1: How to customize further", "Tip 2: What to emphasize", "Tip 3: Follow-up strategy"]
}`

      const response = await withRetry(
        () => client.makeRequest(systemPrompt, userPrompt, { temperature: 0.3, maxTokens: 4000, model: 'sonar-pro' }),
        MAX_RETRY_ATTEMPTS
      )

      const parsed = parseAIResponse<{
        variantA: string
        variantB: string
        personalization: string[]
      }>(response.content)

      const data = {
        variantA: parsed.variantA || 'Cover letter generation failed',
        variantB: parsed.variantB || 'Cover letter generation failed',
        personalization: Array.isArray(parsed.personalization) ? parsed.personalization : []
      }

      setCache(cacheKey, data)

      return {
        success: true,
        data,
        metadata: { requestId, timestamp: started, duration: Date.now() - started },
        cached: false
      }
    } catch (error) {
      console.error('[COVER_LETTERS] Error:', error)
      return {
        success: false,
        data: {
          variantA: 'Cover letter generation failed',
          variantB: 'Cover letter generation failed',
          personalization: []
        },
        metadata: { 
          requestId, 
          timestamp: started, 
          duration: Date.now() - started,
          error: (error as Error).message 
        },
        cached: false
      }
    }
  }

  // Email Outreach Generator: Create personalized email templates
  static async generateEmailOutreach(params: {
    hiringContact: { name: string; title: string; email?: string }
    jobTitle: string
    company: string
    resumeHighlights: string[]
  }): Promise<EnhancedResponse<{
    subjects: string[]
    templates: Array<{ type: 'formal' | 'conversational'; body: string }>
    mailtoLink: string
  }>> {
    const requestId = generateRequestId()
    const started = Date.now()
    const cacheKey = makeKey('email-outreach', params)
    
    const cached = getCache(cacheKey)
    if (cached) {
      return {
        success: true,
        data: cached as { subjects: string[]; templates: Array<{ type: 'formal' | 'conversational'; body: string }>; mailtoLink: string },
        metadata: { requestId, timestamp: started, duration: 0 },
        cached: true
      }
    }

    try {
      const client = createClient()
      const systemPrompt = 'You are an expert at professional networking and cold email outreach. Return only valid JSON.'
      const userPrompt = `Create personalized email outreach templates for contacting a hiring manager.

**Hiring Contact:** ${params.hiringContact.name}, ${params.hiringContact.title}
**Job Title:** ${params.jobTitle}
**Company:** ${params.company}

**Resume Highlights:**
${params.resumeHighlights.map((h, i) => `${i + 1}. ${h}`).join('\n')}

Generate:
1. **3 email subject lines** (varied approaches: direct, curious, value-focused)
2. **2 email templates:**
   - Formal: Professional, respectful tone
   - Conversational: Friendly, engaging tone

Each template should:
- Be concise (150-200 words)
- Reference the hiring manager by name
- Show genuine interest in the role/company
- Highlight 1-2 relevant achievements
- Include a clear call-to-action
- Be personalized, not generic

Return ONLY valid JSON:
{
  "subjects": ["Subject 1", "Subject 2", "Subject 3"],
  "templates": [
    { "type": "formal", "body": "Email body..." },
    { "type": "conversational", "body": "Email body..." }
  ]
}`

      const response = await withRetry(
        () => client.makeRequest(systemPrompt, userPrompt, { temperature: 0.4, maxTokens: 3000, model: 'sonar-pro' }),
        MAX_RETRY_ATTEMPTS
      )

      const parsed = parseAIResponse<{
        subjects: string[]
        templates: Array<{ type: 'formal' | 'conversational'; body: string }>
      }>(response.content)

      const mailtoLink = params.hiringContact.email 
        ? `mailto:${params.hiringContact.email}?subject=${encodeURIComponent(parsed.subjects?.[0] || 'Inquiry about ' + params.jobTitle)}`
        : ''

      const data = {
        subjects: Array.isArray(parsed.subjects) ? parsed.subjects : [],
        templates: Array.isArray(parsed.templates) ? parsed.templates : [],
        mailtoLink
      }

      setCache(cacheKey, data)

      return {
        success: true,
        data,
        metadata: { requestId, timestamp: started, duration: Date.now() - started },
        cached: false
      }
    } catch (error) {
      console.error('[EMAIL_OUTREACH] Error:', error)
      return {
        success: false,
        data: {
          subjects: [],
          templates: [],
          mailtoLink: ''
        },
        metadata: { 
          requestId, 
          timestamp: started, 
          duration: Date.now() - started,
          error: (error as Error).message 
        },
        cached: false
      }
    }
  }

  /**
   * AGENT-POWERED: Job search with autonomous decision-making
   * DEPRECATED: Agent orchestrator removed in cleanup
   * Use jobMarketAnalysisV2 instead
   */
  static async jobSearchWithAgent(
    jobTitle: string,
    location: string,
    options?: { maxResults?: number; workType?: string }
  ): Promise<EnhancedResponse<JobListing[]>> {
    console.log('🤖 [INTELLIGENCE] Agent method deprecated, using standard method...')
    
    // Fallback to standard method since agent-orchestrator was removed
    return await this.jobMarketAnalysisV2(location, '', {
      roleHint: jobTitle,
      maxResults: options?.maxResults,
      workType: options?.workType as 'remote' | 'hybrid' | 'onsite' | 'any' | undefined
    })
  }

  /**
   * AGENT-POWERED: Hiring contacts with 95%+ reliability
   * DEPRECATED: Agent orchestrator removed in cleanup
   * Use hiringContactsV2 instead
   */
  static async hiringContactsWithAgent(
    companyName: string,
    companyDomain?: string
  ): Promise<EnhancedResponse<HiringContact[]>> {
    console.log('🤖 [INTELLIGENCE] Agent method deprecated, using standard method...')
    
    // Fallback to standard method since agent-orchestrator was removed
    return await this.hiringContactsV2(companyName)
  }

  /**
   * Clear cache entries (admin utility)
   * @param prefix - Optional prefix to clear specific cache entries
   * @returns Number of entries cleared
   */
  static clearCache(prefix?: string): number {
    if (!prefix) {
      const size = cache.size
      cache.clear()
      return size
    }
    
    let cleared = 0
    for (const key of cache.keys()) {
      if (key.startsWith(prefix)) {
        cache.delete(key)
        cleared++
      }
    }
    return cleared
  }

  /**
   * Get cache statistics (admin utility)
   * @returns Cache stats including size, hit counts, and breakdown by prefix
   */
  static getCacheStats(): {
    totalEntries: number
    totalHits: number
    breakdown: Record<string, { count: number; hits: number }>
  } {
    const breakdown: Record<string, { count: number; hits: number }> = {}
    let totalHits = 0

    for (const [key, record] of cache.entries()) {
      const prefix = key.split(':')[0] || 'unknown'
      if (!breakdown[prefix]) {
        breakdown[prefix] = { count: 0, hits: 0 }
      }
      breakdown[prefix].count++
      breakdown[prefix].hits += record.metadata.hitCount
      totalHits += record.metadata.hitCount
    }

    return {
      totalEntries: cache.size,
      totalHits,
      breakdown
    }
  }

  /**
   * Custom query to Perplexity API (flexible utility)
   * @param options - Query options including prompts and parameters
   * @returns API response content
   */
  static async customQuery(options: {
    systemPrompt: string
    userPrompt: string
    temperature?: number
    maxTokens?: number
    model?: 'sonar' | 'sonar-pro'
  }): Promise<{ content: string }> {
    const client = createClient()
    const response = await client.makeRequest(
      options.systemPrompt,
      options.userPrompt,
      {
        temperature: options.temperature || 0.2,
        maxTokens: options.maxTokens || 4000,
        model: options.model || 'sonar-pro'
      }
    )
    return { content: response.content }
  }

  /**
   * Get recommended job boards based on location
   * @param location - User's location (e.g., "Toronto", "Canada", "USA")
   * @returns Array of recommended job board names
   */
  static getRecommendedBoards(location: string): string[] {
    const lowerLocation = location.toLowerCase()
    const isCanadian = lowerLocation.includes('canada') || 
                       lowerLocation.includes('toronto') || 
                       lowerLocation.includes('vancouver') || 
                       lowerLocation.includes('montreal') ||
                       lowerLocation.includes('calgary') ||
                       lowerLocation.includes('ottawa')

    if (isCanadian) {
      return [
        'Indeed Canada',
        'Workopolis',
        'Job Bank (Canada)',
        'LinkedIn',
        'Glassdoor',
        'Monster Canada',
        'CareerBuilder Canada',
        'Eluta.ca',
        'CharityVillage (Non-profit)',
        'TechTO (Tech jobs)'
      ]
    }

    // Default US/International boards
    return [
      'Indeed',
      'LinkedIn',
      'Glassdoor',
      'Monster',
      'CareerBuilder',
      'ZipRecruiter',
      'SimplyHired',
      'Dice (Tech)',
      'AngelList (Startups)',
      'RemoteOK (Remote)'
    ]
  }

  /**
   * Get list of available job boards
   * @returns Array of job board objects with name and URL
   */
  static getAvailableJobBoards(): Array<{ name: string; url: string; region: string }> {
    return [
      { name: 'Indeed', url: 'https://www.indeed.com', region: 'Global' },
      { name: 'LinkedIn', url: 'https://www.linkedin.com/jobs', region: 'Global' },
      { name: 'Glassdoor', url: 'https://www.glassdoor.com', region: 'Global' },
      { name: 'Monster', url: 'https://www.monster.com', region: 'Global' },
      { name: 'CareerBuilder', url: 'https://www.careerbuilder.com', region: 'US' },
      { name: 'ZipRecruiter', url: 'https://www.ziprecruiter.com', region: 'US' },
      { name: 'SimplyHired', url: 'https://www.simplyhired.com', region: 'US' },
      { name: 'Dice', url: 'https://www.dice.com', region: 'US (Tech)' },
      { name: 'Indeed Canada', url: 'https://ca.indeed.com', region: 'Canada' },
      { name: 'Workopolis', url: 'https://www.workopolis.com', region: 'Canada' },
      { name: 'Job Bank', url: 'https://www.jobbank.gc.ca', region: 'Canada' },
      { name: 'Eluta', url: 'https://www.eluta.ca', region: 'Canada' },
      { name: 'AngelList', url: 'https://angel.co/jobs', region: 'Startups' },
      { name: 'RemoteOK', url: 'https://remoteok.com', region: 'Remote' },
      { name: 'We Work Remotely', url: 'https://weworkremotely.com', region: 'Remote' }
    ]
  }

  /**
   * Extract career timeline from resume
   * @param resumeText - Resume text content
   * @returns Career timeline with industries and experience
   */
  static async extractCareerTimeline(resumeText: string): Promise<{
    industries: Array<{ name: string; percentage: number; years: number }>
    totalYears: number
    primaryIndustry: string
  }> {
    const client = createClient()
    const prompt = `Analyze this resume and extract the career timeline:

${resumeText.slice(0, 3000)}

Return ONLY valid JSON with this structure:
{
  "industries": [
    { "name": "Industry Name", "percentage": 40, "years": 5 },
    { "name": "Another Industry", "percentage": 30, "years": 3 }
  ],
  "totalYears": 8,
  "primaryIndustry": "Most relevant industry"
}

Rules:
- List all industries worked in
- Calculate percentage of time in each
- Count years of experience per industry
- Identify primary/dominant industry`

    const response = await client.makeRequest(
      'You are a career analyst. Extract career timeline data. Return ONLY valid JSON.',
      prompt,
      { temperature: 0.2, maxTokens: 1000, model: 'sonar-pro' }
    )

    try {
      const parsed = parseAIResponse<{
        industries: Array<{ name: string; percentage: number; years: number }>
        totalYears: number
        primaryIndustry: string
      }>(response.content)

      return {
        industries: parsed.industries || [],
        totalYears: parsed.totalYears || 0,
        primaryIndustry: parsed.primaryIndustry || (parsed.industries?.[0]?.name || 'Unknown')
      }
    } catch {
      // Fallback if parsing fails
      return {
        industries: [{ name: 'General', percentage: 100, years: 0 }],
        totalYears: 0,
        primaryIndustry: 'General'
      }
    }
  }

  /**
   * Enhanced company research with comprehensive data
   * @param params - Company name, job title, location
   * @returns Enhanced company research data
   */
  static async enhancedCompanyResearch(params: {
    companyName: string
    jobTitle?: string
    location?: string
  }): Promise<EnhancedResponse<IntelligenceResponse>> {
    // Use existing researchCompanyV2 as the base
    return await this.researchCompanyV2({
      company: params.companyName,
      role: params.jobTitle,
      geo: params.location
    })
  }
}
</file>

<file path="src/lib/rapidapi-client.ts">
/**
 * RapidAPI Multi-Source Job Aggregator
 * 
 * Queries multiple job APIs through RapidAPI with a single key
 * Supports: Active Jobs DB, JSearch, Indeed, Remote Jobs, LinkedIn, Upwork, Freelancer, etc.
 */

export interface Job {
  id: string
  title: string
  company: string
  location?: string
  description: string
  url: string
  source: string
  postedDate?: string
  salary?: string
  remote?: boolean
  jobType?: string[]
  skills?: string[]
  matchScore?: number
  matchedSkills?: string[]
  matchPercentage?: number
}

export interface JobSource {
  id: string
  name: string
  endpoint: string
  tier: 1 | 2 | 3
  cost: number // per request
  maxResults: number
  enabled: boolean
}

export interface SearchParams {
  keywords: string[]
  location?: string
  remote?: boolean
  jobType?: string[]
  limit?: number
  experienceLevel?: string
  salaryMin?: number
  page?: number
}

export interface QueryMetadata {
  sources: Record<string, {
    success: boolean
    count?: number
    cost?: number
    duration?: number
    error?: string
  }>
  totalJobs: number
  uniqueJobs?: number
  duration: number
  totalCost: number
}

// Job source configurations
export const JOB_SOURCES: Record<string, JobSource> = {
  // TIER 1: Primary Discovery (Always Use)
  'google-jobs': {
    id: 'google-jobs',
    name: 'Google Jobs API',
    endpoint: 'https://google-jobs-api.p.rapidapi.com/google-jobs/distance',
    tier: 1,
    cost: 0.001,
    maxResults: 50,
    enabled: true // 520ms - FASTEST! Show results first
  },
  'active-jobs-db': {
    id: 'active-jobs-db',
    name: 'Active Jobs DB',
    endpoint: 'https://active-jobs-db.p.rapidapi.com/active-ats-7d',
    tier: 1,
    cost: 0.001,
    maxResults: 100,
    enabled: true // 7-day active jobs from ATS platforms
  },
  'jsearch': {
    id: 'jsearch',
    name: 'JSearch',
    endpoint: 'https://jsearch.p.rapidapi.com/search',
    tier: 1,
    cost: 0.001,
    maxResults: 50,
    enabled: true // 3425ms - LinkedIn/Indeed/Glassdoor aggregator
  },
  'adzuna': {
    id: 'adzuna',
    name: 'Adzuna Jobs',
    endpoint: 'https://adzuna.p.rapidapi.com/search',
    tier: 1,
    cost: 0.001,
    maxResults: 50,
    enabled: true // NEW! Adzuna job board aggregator
  },
  
  // TIER 2: Specialized (Use Based on Search Type)
  'linkedin-jobs': {
    id: 'linkedin-jobs',
    name: 'LinkedIn Jobs',
    endpoint: 'https://linkedin-jobs-api.p.rapidapi.com/jobs',
    tier: 2,
    cost: 0.001,
    maxResults: 50,
    enabled: true // 1796ms - Professional roles, $60K+
  },
  'jobs-api': {
    id: 'jobs-api',
    name: 'Jobs API',
    endpoint: 'https://jobs-api14.p.rapidapi.com/v2/list',
    tier: 2,
    cost: 0.001,
    maxResults: 50,
    enabled: true // 1892ms - LinkedIn/Bing/Xing aggregator
  },
  'remote-jobs': {
    id: 'remote-jobs',
    name: 'Remote Jobs API',
    endpoint: 'https://remote-jobs-api.p.rapidapi.com/jobs',
    tier: 2,
    cost: 0.001,
    maxResults: 50,
    enabled: false // 4549ms - Only for remote: true
  },
  'indeed': {
    id: 'indeed',
    name: 'Indeed API',
    endpoint: 'https://indeed12.p.rapidapi.com/jobs/search',
    tier: 2,
    cost: 0.001,
    maxResults: 50,
    enabled: false // 8084ms - SLOW! Only as fallback
  },
  
  // TIER 3: Freelance/Specialized (Only When Needed)
  'freelancer': {
    id: 'freelancer',
    name: 'Freelancer API',
    endpoint: 'https://freelancer-api.p.rapidapi.com/jobs',
    tier: 3,
    cost: 0.001,
    maxResults: 50,
    enabled: false // 16148ms - VERY SLOW! Only for freelance
  }
}

export class RapidAPIClient {
  private apiKey: string
  
  constructor() {
    this.apiKey = process.env.RAPIDAPI_KEY || ''
    
    if (!this.apiKey) {
      console.warn('[RapidAPI] No API key found. Set RAPIDAPI_KEY environment variable.')
    }
  }
  
  /**
   * Query a single job source
   */
  async querySource(
    sourceId: string,
    params: SearchParams
  ): Promise<{ jobs: Job[], duration: number }> {
    const source = JOB_SOURCES[sourceId]
    if (!source) {
      throw new Error(`Unknown source: ${sourceId}`)
    }
    
    if (!source.enabled) {
      console.log(`[RapidAPI] Source ${sourceId} is disabled`)
      return { jobs: [], duration: 0 }
    }
    
    const startTime = Date.now()
    
    try {
      console.log(`[RapidAPI] Querying ${source.name}...`)
      
      // Build query parameters based on source
      const queryParams = this.buildQueryParams(sourceId, params)
      const url = new URL(source.endpoint)
      Object.entries(queryParams).forEach(([key, value]) => {
        if (value !== undefined && value !== null) {
          url.searchParams.append(key, String(value))
        }
      })
      
      const response = await fetch(url.toString(), {
        method: 'GET',
        headers: {
          'X-RapidAPI-Key': this.apiKey,
          'X-RapidAPI-Host': url.hostname
        }
      })
      
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }
      
      const data = await response.json()
      const jobs = this.normalizeJobs(data, sourceId)
      const duration = Date.now() - startTime
      
      console.log(`[RapidAPI] ${source.name}: ${jobs.length} jobs in ${duration}ms`)
      
      return { jobs, duration }
      
    } catch (error) {
      const duration = Date.now() - startTime
      console.error(`[RapidAPI] Error querying ${source.name}:`, error)
      throw error
    }
  }
  
  /**
   * Query a single source with pagination
   */
  async querySourceWithPagination(
    sourceId: string,
    params: SearchParams,
    maxPages: number = 3
  ): Promise<{ jobs: Job[], duration: number }> {
    const startTime = Date.now()
    const allJobs: Job[] = []
    
    console.log(`[RapidAPI] Querying ${sourceId} with pagination (${maxPages} pages)...`)
    
    for (let page = 1; page <= maxPages; page++) {
      try {
        const pageParams = { ...params, page }
        const { jobs } = await this.querySource(sourceId, pageParams)
        
        if (jobs.length === 0) {
          console.log(`[RapidAPI] ${sourceId}: No more results at page ${page}`)
          break
        }
        
        allJobs.push(...jobs)
        console.log(`[RapidAPI] ${sourceId}: Page ${page} - ${jobs.length} jobs`)
        
        // Small delay between pages to avoid rate limits
        if (page < maxPages) {
          await new Promise(resolve => setTimeout(resolve, 1000))
        }
      } catch (error) {
        console.error(`[RapidAPI] ${sourceId}: Error on page ${page}:`, error)
        break
      }
    }
    
    const duration = Date.now() - startTime
    return { jobs: allJobs, duration }
  }
  
  /**
   * Query multiple sources with pagination
   */
  async queryMultipleSourcesWithPagination(
    sourceIds: string[],
    params: SearchParams,
    maxPages: number = 3
  ): Promise<{ jobs: Job[], metadata: QueryMetadata }> {
    const startTime = Date.now()
    
    console.log(`[RapidAPI] Querying ${sourceIds.length} sources with pagination (${maxPages} pages each)...`)
    
    // Query all sources with pagination
    const results = await Promise.allSettled(
      sourceIds.map(id => this.querySourceWithPagination(id, params, maxPages))
    )
    
    // Collect results
    const allJobs: Job[] = []
    const metadata: QueryMetadata = {
      sources: {},
      totalJobs: 0,
      duration: 0,
      totalCost: 0
    }
    
    for (let i = 0; i < results.length; i++) {
      const result = results[i]
      const sourceId = sourceIds[i]
      const source = JOB_SOURCES[sourceId]
      
      if (result.status === 'fulfilled') {
        const { jobs, duration } = result.value
        allJobs.push(...jobs)
        
        metadata.sources[sourceId] = {
          success: true,
          count: jobs.length,
          cost: source.cost,
          duration
        }
        metadata.totalCost += source.cost * maxPages
      } else {
        metadata.sources[sourceId] = {
          success: false,
          count: 0,
          cost: 0,
          duration: 0,
          error: result.reason?.message || 'Unknown error'
        }
      }
    }
    
    metadata.totalJobs = allJobs.length
    metadata.duration = Date.now() - startTime
    
    return { jobs: allJobs, metadata }
  }
  
  /**
   * Query multiple sources in parallel
   */
  async queryMultipleSources(
    sourceIds: string[],
    params: SearchParams
  ): Promise<{ jobs: Job[], metadata: QueryMetadata }> {
    const startTime = Date.now()
    
    console.log(`[RapidAPI] Querying ${sourceIds.length} sources in parallel...`)
    
    // Query all sources in parallel
    const results = await Promise.allSettled(
      sourceIds.map(id => this.querySource(id, params))
    )
    
    // Collect results
    const allJobs: Job[] = []
    const metadata: QueryMetadata = {
      sources: {},
      totalJobs: 0,
      duration: 0,
      totalCost: 0
    }
    
    results.forEach((result, index) => {
      const sourceId = sourceIds[index]
      const source = JOB_SOURCES[sourceId]
      
      if (result.status === 'fulfilled') {
        const { jobs, duration } = result.value
        allJobs.push(...jobs)
        
        metadata.sources[sourceId] = {
          success: true,
          count: jobs.length,
          cost: source.cost,
          duration
        }
        metadata.totalCost += source.cost
      } else {
        metadata.sources[sourceId] = {
          success: false,
          error: result.reason.message
        }
      }
    })
    
    metadata.totalJobs = allJobs.length
    metadata.duration = Date.now() - startTime
    
    console.log(`[RapidAPI] Total: ${allJobs.length} jobs from ${sourceIds.length} sources in ${metadata.duration}ms`)
    console.log(`[RapidAPI] Cost: $${metadata.totalCost.toFixed(4)}`)
    
    return { jobs: allJobs, metadata }
  }
  
  /**
   * Build query parameters for specific source
   */
  private buildQueryParams(sourceId: string, params: SearchParams): Record<string, any> {
    const query = params.keywords.join(' ')
    
    switch (sourceId) {
      case 'active-jobs-db':
        return {
          title_filter: query || '',
          location_filter: 'Canada OR Alberta OR Edmonton',
          description_type: 'text',
          limit: 100, // Free tier max is 100 per request
          offset: ((params.page || 1) - 1) * 100
        }
      
      case 'jsearch':
        return {
          query: query ? `${query} in ${params.location}` : params.location,
          country: 'ca', // Canada only
          remote_jobs_only: params.remote,
          num_pages: 1,
          page: params.page || 1,
          date_posted: 'all' // All jobs, not just recent
        }
      
      case 'indeed':
        return {
          query,
          location: params.location,
          page_id: params.page || 1,
          locality: 'ca', // Canada
          fromage: 30, // Last 30 days
          radius: 150, // 150km radius
          sort: 'date' // Newest first
        }
      
      case 'remote-jobs':
        return {
          query,
          limit: params.limit || 50
        }
      
      case 'linkedin':
        return {
          keywords: query,
          location: params.location,
          remote: params.remote,
          limit: params.limit || 100
        }
      
      case 'upwork':
      case 'freelancer':
        return {
          query,
          limit: params.limit || 50
        }
      
      case 'startup-jobs':
        return {
          query,
          location: params.location,
          limit: params.limit || 100
        }
      
      case 'adzuna':
        return {
          what: query || '', // Empty = all jobs
          where: params.location,
          results_per_page: params.limit || 100,
          page: 1,
          sort_by: 'date'
        }
      
      case 'google-jobs':
        return {
          include: query || '',
          location: params.location,
          distance: 150 // 150km radius
        }
      
      case 'active-jobs-db':
        return {
          limit: params.limit || 1000,
          offset: 0,
          // Don't filter by title - get ALL jobs in location
          location_filter: `"${params.location}"`,
          description_type: 'text'
        }
      
      case 'jsearch':
        return {
          query: query || '',
          page: 1,
          num_pages: 10, // Get 10 pages = ~100 jobs per page = 1000 jobs
          country: 'ca', // Canada for Edmonton
          date_posted: 'all'
        }
      
      case 'linkedin-jobs':
        return {
          query: query || 'jobs',
          page: 1,
          num_pages: 1,
          country: 'us',
          date_posted: 'all'
        }
      
      default:
        return { query, location: params.location }
    }
  }
  
  /**
   * Normalize different API response formats to unified Job type
   */
  private normalizeJobs(data: any, sourceId: string): Job[] {
    try {
      switch (sourceId) {
        case 'active-jobs-db':
          return (Array.isArray(data) ? data : []).map((j: any) => ({
            id: j.id || `${sourceId}-${Date.now()}-${Math.random()}`,
            title: j.title || '',
            company: j.organization || '',
            location: (j.locations_derived || [])[0] || j.location || '',
            description: j.description || '',
            url: j.url || '',
            source: sourceId,
            postedDate: j.date_posted,
            salary: j.salary_raw,
            remote: j.remote_derived || false,
            jobType: j.employment_type || []
          }))
        
        case 'jsearch':
          return (data.data || []).map((j: any) => ({
            id: j.job_id || `${sourceId}-${Date.now()}-${Math.random()}`,
            title: j.job_title || '',
            company: j.employer_name || '',
            location: `${j.job_city || ''}, ${j.job_state || ''}`.trim(),
            description: j.job_description || '',
            url: j.job_apply_link || j.job_google_link || '',
            source: sourceId,
            postedDate: j.job_posted_at_datetime_utc,
            salary: j.job_salary || j.job_min_salary,
            remote: j.job_is_remote || false,
            jobType: j.job_employment_type ? [j.job_employment_type] : []
          }))
        
        case 'adzuna':
          return (data.results || []).map((j: any) => ({
            id: j.id || `${sourceId}-${Date.now()}-${Math.random()}`,
            title: j.title || '',
            company: j.company?.display_name || j.company || '',
            location: j.location?.display_name || j.location || '',
            description: j.description || '',
            url: j.redirect_url || j.url || '',
            source: sourceId,
            postedDate: j.created || j.date,
            salary: j.salary_min && j.salary_max 
              ? `$${j.salary_min}-$${j.salary_max}` 
              : j.salary_min 
                ? `$${j.salary_min}+`
                : undefined,
            remote: j.location?.display_name?.toLowerCase().includes('remote') || false,
            jobType: j.contract_time ? [j.contract_time] : []
          }))
        
        case 'google-jobs':
          return (data.jobs || []).map((j: any) => ({
            id: `${sourceId}-${Date.now()}-${Math.random()}`,
            title: j.title || '',
            company: j.company || '',
            location: j.location || data.filters?.appliedFilters?.location || '',
            description: j.snippet || j.description || '',
            url: j.link || j.url || '',
            source: sourceId,
            postedDate: j.postedDate,
            salary: j.salary,
            remote: j.remote || data.filters?.appliedFilters?.remote || false,
            jobType: j.jobType ? [j.jobType] : []
          }))
        
        case 'indeed':
          return (data.jobs || data.data || []).map((j: any) => ({
            id: j.id || j.job_id || `${sourceId}-${Date.now()}-${Math.random()}`,
            title: j.title || j.job_title || '',
            company: j.company || j.company_name || '',
            location: j.location || '',
            description: j.description || j.job_description || '',
            url: j.url || j.link || '',
            source: sourceId,
            postedDate: j.date || j.posted_date,
            salary: j.salary,
            remote: j.remote || false
          }))
        
        case 'remote-jobs':
        case 'linkedin':
        case 'upwork':
        case 'freelancer':
        case 'startup-jobs':
          // Generic normalization for other sources
          return (data.jobs || data.data || data.results || []).map((j: any) => ({
            id: j.id || j.job_id || `${sourceId}-${Date.now()}-${Math.random()}`,
            title: j.title || j.job_title || j.name || '',
            company: j.company || j.company_name || j.employer || '',
            location: j.location || j.job_location || '',
            description: j.description || j.job_description || j.details || '',
            url: j.url || j.link || j.apply_url || '',
            source: sourceId,
            postedDate: j.posted_date || j.date || j.created_at,
            salary: j.salary || j.budget,
            remote: j.remote || j.is_remote || false,
            jobType: j.job_type ? [j.job_type] : []
          }))
        
        default:
          console.warn(`[RapidAPI] Unknown source format: ${sourceId}`)
          return []
      }
    } catch (error) {
      console.error(`[RapidAPI] Error normalizing jobs from ${sourceId}:`, error)
      return []
    }
  }
}

// Export singleton instance
export const rapidAPIClient = new RapidAPIClient()
</file>

<file path="src/lib/supabase-bulk-download.ts">
/**
 * Supabase Bulk Download Integration
 * Orchestrates job downloads from RapidAPI and stores in Supabase
 */

import { supabaseAdmin } from './supabase'
import { RapidAPIClient } from './rapidapi-client'
import { AdzunaAPIClient } from './adzuna-api-client'
import type { Job } from '@/types/supabase'

const rapidAPI = new RapidAPIClient()
const adzunaAPI = new AdzunaAPIClient()

/**
 * Bulk download jobs for multiple locations
 */
export async function bulkDownloadJobs(locations: string[]) {
  console.log(`[BULK] Starting for ${locations.length} locations`)
  
  const startTime = Date.now()
  const allJobs: Partial<Job>[] = []
  let totalDownloaded = 0
  let totalInserted = 0
  let totalErrors = 0
  
  for (const location of locations) {
    console.log(`\n[BULK] Processing ${location}...`)
    
    try {
      // Query RapidAPI sources - targeting 500+ jobs
      // Strategy: Single broad search with high limits and multiple pages
      console.log(`  Searching for jobs in ${location}...`)
      
      // 1. Scrape Adzuna directly - MAXIMIZED with multiple strategies
      console.log(`  [ADZUNA] Scraping with multiple strategies...`)
      const adzunaJobs: any[] = []
      const seenJobIds = new Set<string>()
      
      // Strategy 1: Broad search (all jobs)
      console.log(`  [ADZUNA] Strategy 1: Broad search (all jobs)`)
      for (let page = 1; page <= 100; page++) {
        try {
          const result = await adzunaAPI.searchJobs({
            what: '',
            where: location,
            country: 'ca',
            resultsPerPage: 50,
            page,
            sortBy: 'date'
          })
          
          // CRITICAL: Validate data quality and deduplicate
          const validJobs = result.results.filter((j: any) => {
            // Skip duplicates
            if (seenJobIds.has(j.id)) {
              return false
            }
            
            const hasCompany = j.company?.display_name && j.company.display_name.trim().length > 0
            const hasDescription = j.description && j.description.trim().length > 0
            const hasTitle = j.title && j.title.trim().length > 0
            const hasUrl = j.redirect_url && j.redirect_url.trim().length > 0
            
            if (!hasCompany || !hasDescription || !hasTitle || !hasUrl) {
              return false
            }
            
            seenJobIds.add(j.id)
            return true
          })
          
          adzunaJobs.push(...validJobs.map((j: any) => ({
            id: j.id,
            title: j.title,
            company: j.company.display_name,
            location: j.location.display_name,
            description: j.description,
            url: j.redirect_url,
            source: 'adzuna',
            salary: {
              min: j.salary_min || null,
              max: j.salary_max || null
            },
            postedDate: j.created,
            jobType: j.contract_time ? [j.contract_time] : []
          })))
          
          console.log(`    Page ${page}: ${validJobs.length}/${result.results.length} valid jobs (${result.results.length - validJobs.length} rejected)`)
          
          // Stop if no more results
          if (result.results.length === 0) {
            console.log(`    No more results, stopping at page ${page}`)
            break
          }
          
          await sleep(500) // Rate limit (reduced from 1000ms for faster scraping)
          
        } catch (error: any) {
          console.error(`    Page ${page} error:`, error.message)
          break
        }
      }
      
      console.log(`  [ADZUNA] Total: ${adzunaJobs.length} valid jobs scraped`)
      console.log(`  [ADZUNA] Sample job:`, adzunaJobs[0] ? {
        title: adzunaJobs[0].title,
        company: adzunaJobs[0].company,
        hasSalary: !!(adzunaJobs[0].salary?.min || adzunaJobs[0].salary?.max),
        descriptionLength: adzunaJobs[0].description?.length || 0
      } : 'No jobs')
      
      // 2. Scrape RapidAPI sources
      const { jobs, metadata } = await rapidAPI.queryMultipleSourcesWithPagination(
        [
          'jsearch',          // JSearch API (working)
          'google-jobs'       // Google Jobs API (working)
        ],
        {
          keywords: [''], // Empty = all jobs
          location,
          limit: 100
        },
        20 // 20 pages per source
      )
      
      // Combine all jobs
      const allLocationJobs = [...adzunaJobs, ...jobs]
      
      // Transform to Supabase format
      const transformedJobs = allLocationJobs.map(job => transformJobForSupabase(job, location))
      allJobs.push(...transformedJobs)
      totalDownloaded += allLocationJobs.length
      
      console.log(`  Total downloaded for ${location}: ${allLocationJobs.length} jobs`)
      
      // Rate limit protection
      await sleep(3000)
      
    } catch (error: any) {
      console.error(`  Error processing ${location}:`, error.message)
      totalErrors++
    }
  }
  
  console.log(`\n[DEDUPE] Deduplicating ${allJobs.length} jobs...`)
  const uniqueJobs = deduplicateJobs(allJobs)
  console.log(`[DEDUPE] ${allJobs.length} → ${uniqueJobs.length} unique jobs`)
  
  // Batch insert to Supabase
  console.log(`\n[INSERT] Inserting ${uniqueJobs.length} jobs to Supabase...`)
  const insertResult = await batchInsertJobs(uniqueJobs)
  totalInserted = insertResult.inserted
  
  const duration = Math.round((Date.now() - startTime) / 1000)
  
  // Log to download history
  await logDownloadHistory({
    source: 'bulk-all',
    search_query: locations.join(', '),
    jobs_downloaded: totalDownloaded,
    unique_jobs: uniqueJobs.length,
    duplicates_found: allJobs.length - uniqueJobs.length,
    duration_seconds: duration,
    success: totalErrors === 0
  })
  
  console.log(`\n[BULK] Complete in ${duration}s`)
  console.log(`  - Downloaded: ${totalDownloaded}`)
  console.log(`  - Unique: ${uniqueJobs.length}`)
  console.log(`  - Inserted: ${totalInserted}`)
  console.log(`  - Errors: ${totalErrors}`)
  
  return {
    downloaded: totalDownloaded,
    unique: uniqueJobs.length,
    inserted: totalInserted,
    errors: totalErrors,
    duration
  }
}

/**
 * Transform RapidAPI job to Supabase format
 */
function transformJobForSupabase(job: any, location: string): Partial<Job> {
  const city = parseCity(job.location || location)
  const state = parseState(job.location || location)
  const postedDate = job.postedDate ? new Date(job.postedDate).toISOString() : undefined
  
  return {
    title: job.title || 'Unknown Title',
    company: job.company || 'Unknown Company',
    location: job.location || location,
    description: job.description || '',
    
    salary_min: job.salary?.min || undefined,
    salary_max: job.salary?.max || undefined,
    salary_type: 'yearly',
    salary_currency: 'CAD',
    
    job_type: job.jobType?.[0] || undefined,
    remote_type: job.remote ? 'remote' : 'on-site',
    
    url: job.url || '',
    external_id: job.id || `${job.source}-${Date.now()}-${Math.random()}`,
    source: job.source as any,
    apply_link: job.url || '',
    
    city: city || undefined,
    state: state || undefined,
    country: 'Canada',
    
    keywords: job.keywords || [],
    
    posted_date: postedDate,
    scraped_at: new Date().toISOString(),
    expires_at: new Date(Date.now() + 14 * 24 * 60 * 60 * 1000).toISOString(),
    
    raw_data: job
  }
}

/**
 * Batch insert jobs to Supabase with retry logic
 */
async function batchInsertJobs(jobs: Partial<Job>[], batchSize = 100) {
  console.log(`[BATCH INSERT] Starting for ${jobs.length} jobs (batch size: ${batchSize})`)
  
  if (jobs.length === 0) {
    console.log('[BATCH INSERT] No jobs to insert!')
    return { inserted: 0, errors: 0 }
  }
  
  // Log first job for debugging
  console.log('[BATCH INSERT] Sample job:', JSON.stringify(jobs[0], null, 2))
  
  let totalInserted = 0
  let totalErrors = 0
  
  for (let i = 0; i < jobs.length; i += batchSize) {
    const batch = jobs.slice(i, i + batchSize)
    const batchNum = Math.floor(i / batchSize) + 1
    
    console.log(`[BATCH ${batchNum}] Attempting to insert ${batch.length} jobs...`)
    
    let retries = 3
    let success = false
    
    while (retries > 0 && !success) {
      try {
        const { data, error } = await supabaseAdmin
          .from('jobs')
          .upsert(batch, {
            onConflict: 'company,title,location,source',
            ignoreDuplicates: false
          })
          .select('id')
        
        if (error) {
          console.error(`[BATCH ${batchNum}] Supabase Error:`)
          console.error(`  Code: ${error.code}`)
          console.error(`  Message: ${error.message}`)
          console.error(`  Details: ${JSON.stringify(error.details)}`)
          console.error(`  Hint: ${error.hint}`)
          
          if (retries > 1) {
            console.log(`  Retrying... (${retries - 1} attempts left)`)
            await sleep(2000)
            retries--
            continue
          }
          
          totalErrors += batch.length
          break
        }
        
        const insertedCount = data?.length || 0
        totalInserted += insertedCount
        success = true
        
        console.log(`[BATCH ${batchNum}] ✅ Inserted/Updated ${insertedCount} jobs`)
        
      } catch (error: any) {
        console.error(`[BATCH ${batchNum}] Exception:`, error.message)
        
        if (retries > 1) {
          console.log(`  Retrying... (${retries - 1} attempts left)`)
          await sleep(2000)
          retries--
        } else {
          console.error(`  Stack:`, error.stack)
          totalErrors += batch.length
          break
        }
      }
    }
    
    // Small delay between batches to avoid rate limiting
    await sleep(500)
  }
  
  console.log(`[BATCH INSERT] Complete: ${totalInserted} inserted, ${totalErrors} errors`)
  
  return {
    inserted: totalInserted,
    errors: totalErrors
  }
}

/**
 * Log download history
 */
async function logDownloadHistory(data: {
  source: string
  search_query: string
  jobs_downloaded: number
  unique_jobs: number
  duplicates_found: number
  duration_seconds: number
  success: boolean
}) {
  try {
    await supabaseAdmin
      .from('download_history')
      .insert({
        ...data,
        started_at: new Date().toISOString(),
        completed_at: new Date().toISOString()
      })
  } catch (error: any) {
    console.error('[HISTORY] Error logging:', error.message)
  }
}

/**
 * Deduplicate jobs by company + title + location
 */
function deduplicateJobs(jobs: Partial<Job>[]): Partial<Job>[] {
  const seen = new Set<string>()
  
  return jobs.filter(job => {
    const key = `${job.company}|${job.title}|${job.location}`.toLowerCase()
    if (seen.has(key)) return false
    seen.add(key)
    return true
  })
}

/**
 * Helper functions
 */
function sleep(ms: number) {
  return new Promise(resolve => setTimeout(resolve, ms))
}

function parseCity(location: string): string | null {
  return location?.split(',')[0]?.trim() || null
}

function parseState(location: string): string | null {
  return location?.split(',')[1]?.trim() || null
}
</file>

</files>
